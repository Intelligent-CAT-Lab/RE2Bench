You are given a Python function (between [PYTHON] and [/PYTHON]). For this function, I want you to take the provided output (between [OUTPUT] and [/OUTPUT]) and predict the input of the function. Both input and output are presented in a JSON format. The input structure is defined between [STRUCTURE] and [/STRUCTURE]. You only need to predict input variable values to fill out placeholders XXX in the structure, and print input between [INPUT] and [/INPUT]. You should maintain the structure when printing inputs. Do not change anything else.  ONLY print the input, DO NOT print any reasoning process.
[EXAMPLE]
[PYTHON]
class TempPathFactory(object):
    _given_basetemp = attr.ib(
        converter=attr.converters.optional(
            lambda p: Path(os.path.abspath(six.text_type(p)))
        )
    )
    _trace = attr.ib()
    _basetemp = attr.ib(default=None)

    def mktemp(self, basename, numbered=True):
        if not numbered:
            p = self.getbasetemp().joinpath(basename)
            p.mkdir()
        else:
            p = make_numbered_dir(root=self.getbasetemp(), prefix=basename)
            self._trace("mktemp", p)
        return p

    def getbasetemp(self):
        if self._given_basetemp is not None:
            basetemp = self._given_basetemp
            ensure_reset_dir(basetemp)
            basetemp = basetemp.resolve()
        else:
            from_env = os.environ.get("PYTEST_DEBUG_TEMPROOT")
            temproot = Path(from_env or tempfile.gettempdir()).resolve()
            user = get_user() or "unknown"
            rootdir = temproot.joinpath("pytest-of-{}".format(user))
            rootdir.mkdir(exist_ok=True)
            basetemp = make_numbered_dir_with_cleanup(
                prefix="pytest-", root=rootdir, keep=3, lock_timeout=LOCK_TIMEOUT
            )
        assert basetemp is not None, basetemp
        self._basetemp = t = basetemp
        self._trace("new basetemp", t)
        return t
[/PYTHON]

Functions called during the execution:
[PYTHON]
def parse_num(maybe_num):
    try:
        return int(maybe_num)
    except ValueError:
        return -1

def extract_suffixes(iter, prefix):
    p_len = len(prefix)
    for p in iter:
        yield p.name[p_len:]


def find_suffixes(root, prefix):
    return extract_suffixes(find_prefixed(root, prefix), prefix)

def make_numbered_dir(root, prefix):
    for i in range(10):
        # try up to 10 times to create the folder
        max_existing = _max(map(parse_num, find_suffixes(root, prefix)), default=-1)
        new_number = max_existing + 1
        new_path = root.joinpath("{}{}".format(prefix, new_number))
        try:
            new_path.mkdir()
        except Exception:
            pass
        else:
            _force_symlink(root, prefix + "current", new_path)
            return new_path
    else:
        raise EnvironmentError(
            "could not create numbered dir with prefix "
            "{prefix} in {root} after 10 tries".format(prefix=prefix, root=root)
        )
[/PYTHON]
What will be the input of `mktemp`, given the following output:
[OUTPUT]
```
{
    "strpath": "/tmp/pytest-of-root/pytest-0/test_mktemp0/world0"
}
```
[/OUTPUT]

[STRUCTURE]
```
{
    "self": {
        "_tmppath_factory": {
            "_given_basetemp": XXX,
            "_trace": XXX,
            "_basetemp": XXX
        }
    },
    "args": {
        "basename": XXX
    },
    "kwargs": XXX
}
```
[/STRUCTURE]

[INPUT]
```
{
    "self": {
        "_tmppath_factory": {
            "_given_basetemp": "/tmp/pytest-of-root/pytest-0/test_mktemp0",
            "_trace": {},
            "_basetemp": null
        }
    },
    "args": {
        "basename": "world"
    },
    "kwargs": {}
}
```
[/INPUT]
[/EXAMPLE]
[PYTHON]
from numbers import Integral, Real
import numpy as np
import sklearn.externals.array_api_extra as xpx
from sklearn.utils._array_api import _average, _convert_to_numpy, _find_matching_floating_dtype, _isin, _logsumexp, get_namespace, get_namespace_and_device, size
from sklearn.utils._param_validation import Interval
from sklearn.utils.multiclass import _check_partial_fit_first_call
from sklearn.utils.validation import _check_n_features, _check_sample_weight, check_is_fitted, check_non_negative, validate_data

class GaussianNB(_BaseNB):
    _parameter_constraints: dict = {'priors': ['array-like', None], 'var_smoothing': [Interval(Real, 0, None, closed='left')]}

    def __init__(self, *, priors=None, var_smoothing=1e-09):
        self.priors = priors
        self.var_smoothing = var_smoothing

    @staticmethod
    def _update_mean_variance(n_past, mu, var, X, sample_weight=None):
        xp, _ = get_namespace(X)
        if X.shape[0] == 0:
            return (mu, var)
        if sample_weight is not None:
            n_new = float(xp.sum(sample_weight))
            if np.isclose(n_new, 0.0):
                return (mu, var)
            new_mu = _average(X, axis=0, weights=sample_weight, xp=xp)
            new_var = _average((X - new_mu) ** 2, axis=0, weights=sample_weight, xp=xp)
        else:
            n_new = X.shape[0]
            new_var = xp.var(X, axis=0)
            new_mu = xp.mean(X, axis=0)
        if n_past == 0:
            return (new_mu, new_var)
        n_total = float(n_past + n_new)
        total_mu = (n_new * new_mu + n_past * mu) / n_total
        old_ssd = n_past * var
        new_ssd = n_new * new_var
        total_ssd = old_ssd + new_ssd + n_new * n_past / n_total * (mu - new_mu) ** 2
        total_var = total_ssd / n_total
        return (total_mu, total_var)

    def _partial_fit(self, X, y, classes=None, _refit=False, sample_weight=None):
        if _refit:
            self.classes_ = None
        first_call = _check_partial_fit_first_call(self, classes)
        X, y = validate_data(self, X, y, reset=first_call)
        xp, _, device_ = get_namespace_and_device(X)
        float_dtype = _find_matching_floating_dtype(X, xp=xp)
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X, dtype=float_dtype)
        xp_y, _ = get_namespace(y)
        self.epsilon_ = self.var_smoothing * xp.max(xp.var(X, axis=0))
        if first_call:
            n_features = X.shape[1]
            n_classes = self.classes_.shape[0]
            self.theta_ = xp.zeros((n_classes, n_features), dtype=float_dtype, device=device_)
            self.var_ = xp.zeros((n_classes, n_features), dtype=float_dtype, device=device_)
            self.class_count_ = xp.zeros(n_classes, dtype=float_dtype, device=device_)
            if self.priors is not None:
                priors = xp.asarray(self.priors, dtype=float_dtype, device=device_)
                if priors.shape[0] != n_classes:
                    raise ValueError('Number of priors must match number of classes.')
                if not xpx.isclose(xp.sum(priors), 1.0):
                    raise ValueError('The sum of the priors should be 1.')
                if xp.any(priors < 0):
                    raise ValueError('Priors must be non-negative.')
                self.class_prior_ = priors
            else:
                self.class_prior_ = xp.zeros(self.classes_.shape[0], dtype=float_dtype, device=device_)
        else:
            if X.shape[1] != self.theta_.shape[1]:
                msg = 'Number of features %d does not match previous data %d.'
                raise ValueError(msg % (X.shape[1], self.theta_.shape[1]))
            self.var_[:, :] -= self.epsilon_
        classes = self.classes_
        unique_y = xp_y.unique_values(y)
        unique_y_in_classes = _isin(unique_y, classes, xp=xp_y)
        if not xp_y.all(unique_y_in_classes):
            raise ValueError('The target label(s) %s in y do not exist in the initial classes %s' % (unique_y[~unique_y_in_classes], classes))
        for y_i in unique_y:
            i = int(xp_y.searchsorted(classes, y_i))
            y_i_mask = xp.asarray(y == y_i, device=device_)
            X_i = X[y_i_mask]
            if sample_weight is not None:
                sw_i = sample_weight[y_i_mask]
                N_i = xp.sum(sw_i)
            else:
                sw_i = None
                N_i = X_i.shape[0]
            new_theta, new_sigma = self._update_mean_variance(self.class_count_[i], self.theta_[i, :], self.var_[i, :], X_i, sw_i)
            self.theta_[i, :] = new_theta
            self.var_[i, :] = new_sigma
            self.class_count_[i] += N_i
        self.var_[:, :] += self.epsilon_
        if self.priors is None:
            self.class_prior_ = self.class_count_ / xp.sum(self.class_count_)
        return self
[/PYTHON]

Functions called during the execution:
[PYTHON]
scikit-learn.sklearn.externals.array_api_compat._internal.wrapped_f

@wraps(f)
def wrapped_f(*args: object, **kwargs: object) -> object:
    return f(*args, xp=xp, **kwargs)

scikit-learn.sklearn.externals.array_api_compat.numpy._aliases.asarray

def asarray(
    obj: Array | complex | NestedSequence[complex] | SupportsBufferProtocol,
    /,
    *,
    dtype: DType | None = None,
    device: Device | None = None,
    copy: _Copy | None = None,
    **kwargs: Any,
) -> Array:
    """
    Array API compatibility wrapper for asarray().

    See the corresponding documentation in the array library and/or the array API
    specification for more details.
    """
    _helpers._check_device(np, device)

    if copy is None:
        copy = np._CopyMode.IF_NEEDED
    elif copy is False:
        copy = np._CopyMode.NEVER
    elif copy is True:
        copy = np._CopyMode.ALWAYS

    return np.array(obj, copy=copy, dtype=dtype, **kwargs)  # pyright: ignore

scikit-learn.sklearn.externals.array_api_extra._delegation.isclose

def isclose(
    a: Array | complex,
    b: Array | complex,
    *,
    rtol: float = 1e-05,
    atol: float = 1e-08,
    equal_nan: bool = False,
    xp: ModuleType | None = None,
) -> Array:
    """
    Return a boolean array where two arrays are element-wise equal within a tolerance.

    The tolerance values are positive, typically very small numbers. The relative
    difference ``(rtol * abs(b))`` and the absolute difference `atol` are added together
    to compare against the absolute difference between `a` and `b`.

    NaNs are treated as equal if they are in the same place and if ``equal_nan=True``.
    Infs are treated as equal if they are in the same place and of the same sign in both
    arrays.

    Parameters
    ----------
    a, b : Array | int | float | complex | bool
        Input objects to compare. At least one must be an array.
    rtol : array_like, optional
        The relative tolerance parameter (see Notes).
    atol : array_like, optional
        The absolute tolerance parameter (see Notes).
    equal_nan : bool, optional
        Whether to compare NaN's as equal. If True, NaN's in `a` will be considered
        equal to NaN's in `b` in the output array.
    xp : array_namespace, optional
        The standard-compatible namespace for `a` and `b`. Default: infer.

    Returns
    -------
    Array
        A boolean array of shape broadcasted from `a` and `b`, containing ``True`` where
        `a` is close to `b`, and ``False`` otherwise.

    Warnings
    --------
    The default `atol` is not appropriate for comparing numbers with magnitudes much
    smaller than one (see notes).

    See Also
    --------
    math.isclose : Similar function in stdlib for Python scalars.

    Notes
    -----
    For finite values, `isclose` uses the following equation to test whether two
    floating point values are equivalent::

        absolute(a - b) <= (atol + rtol * absolute(b))

    Unlike the built-in `math.isclose`,
    the above equation is not symmetric in `a` and `b`,
    so that ``isclose(a, b)`` might be different from ``isclose(b, a)`` in some rare
    cases.

    The default value of `atol` is not appropriate when the reference value `b` has
    magnitude smaller than one. For example, it is unlikely that ``a = 1e-9`` and
    ``b = 2e-9`` should be considered "close", yet ``isclose(1e-9, 2e-9)`` is ``True``
    with default settings. Be sure to select `atol` for the use case at hand, especially
    for defining the threshold below which a non-zero value in `a` will be considered
    "close" to a very small or zero value in `b`.

    The comparison of `a` and `b` uses standard broadcasting, which means that `a` and
    `b` need not have the same shape in order for ``isclose(a, b)`` to evaluate to
    ``True``.

    `isclose` is not defined for non-numeric data types.
    ``bool`` is considered a numeric data-type for this purpose.
    """
    xp = array_namespace(a, b) if xp is None else xp

    if (
        is_numpy_namespace(xp)
        or is_cupy_namespace(xp)
        or is_dask_namespace(xp)
        or is_jax_namespace(xp)
    ):
        return xp.isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan)

    if is_torch_namespace(xp):
        a, b = asarrays(a, b, xp=xp)  # Array API 2024.12 support
        return xp.isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan)

    return _funcs.isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan, xp=xp)

scikit-learn.sklearn.naive_bayes._update_mean_variance

@staticmethod
def _update_mean_variance(n_past, mu, var, X, sample_weight=None):
    """Compute online update of Gaussian mean and variance.

    Given starting sample count, mean, and variance, a new set of
    points X, and optionally sample weights, return the updated mean and
    variance. (NB - each dimension (column) in X is treated as independent
    -- you get variance, not covariance).

    Can take scalar mean and variance, or vector mean and variance to
    simultaneously update a number of independent Gaussians.

    See Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:

    http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf

    Parameters
    ----------
    n_past : int
        Number of samples represented in old mean and variance. If sample
        weights were given, this should contain the sum of sample
        weights represented in old mean and variance.

    mu : array-like of shape (number of Gaussians,)
        Means for Gaussians in original set.

    var : array-like of shape (number of Gaussians,)
        Variances for Gaussians in original set.

    sample_weight : array-like of shape (n_samples,), default=None
        Weights applied to individual samples (1. for unweighted).

    Returns
    -------
    total_mu : array-like of shape (number of Gaussians,)
        Updated mean for each Gaussian over the combined set.

    total_var : array-like of shape (number of Gaussians,)
        Updated variance for each Gaussian over the combined set.
    """
    xp, _ = get_namespace(X)
    if X.shape[0] == 0:
        return mu, var

    # Compute (potentially weighted) mean and variance of new datapoints
    if sample_weight is not None:
        n_new = float(xp.sum(sample_weight))
        if np.isclose(n_new, 0.0):
            return mu, var
        new_mu = _average(X, axis=0, weights=sample_weight, xp=xp)
        new_var = _average((X - new_mu) ** 2, axis=0, weights=sample_weight, xp=xp)
    else:
        n_new = X.shape[0]
        new_var = xp.var(X, axis=0)
        new_mu = xp.mean(X, axis=0)

    if n_past == 0:
        return new_mu, new_var

    n_total = float(n_past + n_new)

    # Combine mean of old and new data, taking into consideration
    # (weighted) number of observations
    total_mu = (n_new * new_mu + n_past * mu) / n_total

    # Combine variance of old and new data, taking into consideration
    # (weighted) number of observations. This is achieved by combining
    # the sum-of-squared-differences (ssd)
    old_ssd = n_past * var
    new_ssd = n_new * new_var
    total_ssd = old_ssd + new_ssd + (n_new * n_past / n_total) * (mu - new_mu) ** 2
    total_var = total_ssd / n_total

    return total_mu, total_var

scikit-learn.sklearn.utils._array_api.get_namespace

def get_namespace(*arrays, remove_none=True, remove_types=(str,), xp=None):
    """Get namespace of arrays.

    Introspect `arrays` arguments and return their common Array API compatible
    namespace object, if any.

    Note that sparse arrays are filtered by default.

    See: https://numpy.org/neps/nep-0047-array-api-standard.html

    If `arrays` are regular numpy arrays, `array_api_compat.numpy` is returned instead.

    Namespace support is not enabled by default. To enabled it call:

      sklearn.set_config(array_api_dispatch=True)

    or:

      with sklearn.config_context(array_api_dispatch=True):
          # your code here

    Otherwise `array_api_compat.numpy` is
    always returned irrespective of the fact that arrays implement the
    `__array_namespace__` protocol or not.

    Note that if no arrays pass the set filters, ``_NUMPY_API_WRAPPER_INSTANCE, False``
    is returned.

    Parameters
    ----------
    *arrays : array objects
        Array objects.

    remove_none : bool, default=True
        Whether to ignore None objects passed in arrays.

    remove_types : tuple or list, default=(str,)
        Types to ignore in the arrays.

    xp : module, default=None
        Precomputed array namespace module. When passed, typically from a caller
        that has already performed inspection of its own inputs, skips array
        namespace inspection.

    Returns
    -------
    namespace : module
        Namespace shared by array objects. If any of the `arrays` are not arrays,
        the namespace defaults to the NumPy namespace.

    is_array_api_compliant : bool
        True if the arrays are containers that implement the array API spec (see
        https://data-apis.org/array-api/latest/index.html).
        Always False when array_api_dispatch=False.
    """
    array_api_dispatch = get_config()["array_api_dispatch"]
    if not array_api_dispatch:
        if xp is not None:
            return xp, False
        else:
            return np_compat, False

    if xp is not None:
        return xp, True

    arrays = _remove_non_arrays(
        *arrays,
        remove_none=remove_none,
        remove_types=remove_types,
    )

    if not arrays:
        return np_compat, False

    _check_array_api_dispatch(array_api_dispatch)

    namespace, is_array_api_compliant = array_api_compat.get_namespace(*arrays), True

    if namespace.__name__ == "array_api_strict" and hasattr(
        namespace, "set_array_api_strict_flags"
    ):
        namespace.set_array_api_strict_flags(api_version="2024.12")

    return namespace, is_array_api_compliant

scikit-learn.sklearn.utils._array_api.get_namespace_and_device

def get_namespace_and_device(
    *array_list, remove_none=True, remove_types=(str,), xp=None
):
    """Combination into one single function of `get_namespace` and `device`.

    Parameters
    ----------
    *array_list : array objects
        Array objects.
    remove_none : bool, default=True
        Whether to ignore None objects passed in arrays.
    remove_types : tuple or list, default=(str,)
        Types to ignore in the arrays.
    xp : module, default=None
        Precomputed array namespace module. When passed, typically from a caller
        that has already performed inspection of its own inputs, skips array
        namespace inspection.

    Returns
    -------
    namespace : module
        Namespace shared by array objects. If any of the `arrays` are not arrays,
        the namespace defaults to NumPy.
    is_array_api_compliant : bool
        True if the arrays are containers that implement the Array API spec.
        Always False when array_api_dispatch=False.
    device : device
        `device` object (see the "Device Support" section of the array API spec).
    """
    skip_remove_kwargs = dict(remove_none=False, remove_types=[])

    array_list = _remove_non_arrays(
        *array_list,
        remove_none=remove_none,
        remove_types=remove_types,
    )
    arrays_device = device(*array_list, **skip_remove_kwargs)

    if xp is None:
        xp, is_array_api = get_namespace(*array_list, **skip_remove_kwargs)
    else:
        xp, is_array_api = xp, True

    if is_array_api:
        return xp, is_array_api, arrays_device
    else:
        return xp, False, arrays_device

scikit-learn.sklearn.utils._array_api._find_matching_floating_dtype

def _find_matching_floating_dtype(*arrays, xp):
    """Find a suitable floating point dtype when computing with arrays.

    If any of the arrays are floating point, return the dtype with the highest
    precision by following official type promotion rules:

    https://data-apis.org/array-api/latest/API_specification/type_promotion.html

    If there are no floating point input arrays (all integral inputs for
    instance), return the default floating point dtype for the namespace.
    """
    dtyped_arrays = [xp.asarray(a) for a in arrays if hasattr(a, "dtype")]
    floating_dtypes = [
        a.dtype for a in dtyped_arrays if xp.isdtype(a.dtype, "real floating")
    ]
    if floating_dtypes:
        # Return the floating dtype with the highest precision:
        return xp.result_type(*floating_dtypes)

    # If none of the input arrays have a floating point dtype, they must be all
    # integer arrays or containers of Python scalars: return the default
    # floating point dtype for the namespace (implementation specific).
    return xp.asarray(0.0).dtype

scikit-learn.sklearn.utils._array_api._isin

def _isin(element, test_elements, xp, assume_unique=False, invert=False):
    """Calculates ``element in test_elements``, broadcasting over `element`
    only.

    Returns a boolean array of the same shape as `element` that is True
    where an element of `element` is in `test_elements` and False otherwise.
    """
    if _is_numpy_namespace(xp):
        return xp.asarray(
            numpy.isin(
                element=element,
                test_elements=test_elements,
                assume_unique=assume_unique,
                invert=invert,
            )
        )

    original_element_shape = element.shape
    element = xp.reshape(element, (-1,))
    test_elements = xp.reshape(test_elements, (-1,))
    return xp.reshape(
        _in1d(
            ar1=element,
            ar2=test_elements,
            xp=xp,
            assume_unique=assume_unique,
            invert=invert,
        ),
        original_element_shape,
    )

scikit-learn.sklearn.utils.multiclass._check_partial_fit_first_call

def _check_partial_fit_first_call(clf, classes=None):
    """Private helper function for factorizing common classes param logic.

    Estimators that implement the ``partial_fit`` API need to be provided with
    the list of possible classes at the first call to partial_fit.

    Subsequent calls to partial_fit should check that ``classes`` is still
    consistent with a previous value of ``clf.classes_`` when provided.

    This function returns True if it detects that this was the first call to
    ``partial_fit`` on ``clf``. In that case the ``classes_`` attribute is also
    set on ``clf``.

    """
    if getattr(clf, "classes_", None) is None and classes is None:
        raise ValueError("classes must be passed on the first call to partial_fit.")

    elif classes is not None:
        if getattr(clf, "classes_", None) is not None:
            if not np.array_equal(clf.classes_, unique_labels(classes)):
                raise ValueError(
                    "`classes=%r` is not the same as on last call "
                    "to partial_fit, was: %r" % (classes, clf.classes_)
                )

        else:
            # This is the first call to partial_fit
            clf.classes_ = unique_labels(classes)
            return True

    # classes is None and clf.classes_ has already previously been set:
    # nothing to do
    return False

scikit-learn.sklearn.utils.validation._check_sample_weight

def _check_sample_weight(
    sample_weight,
    X,
    *,
    dtype=None,
    force_float_dtype=True,
    ensure_non_negative=False,
    ensure_same_device=True,
    copy=False,
):
    """Validate sample weights.

    Note that passing sample_weight=None will output an array of ones.
    Therefore, in some cases, you may want to protect the call with:
    if sample_weight is not None:
        sample_weight = _check_sample_weight(...)

    Parameters
    ----------
    sample_weight : {ndarray, Number or None}, shape (n_samples,)
        Input sample weights.

    X : {ndarray, list, sparse matrix}
        Input data.

    dtype : dtype, default=None
        dtype of the validated `sample_weight`.
        If None, and `sample_weight` is an array:

            - If `sample_weight.dtype` is one of `{np.float64, np.float32}`,
              then the dtype is preserved.
            - Else the output has NumPy's default dtype: `np.float64`.

        If `dtype` is not `{np.float32, np.float64, None}`, then output will
        be `np.float64`.

    force_float_dtype : bool, default=True
        Whether `X` should be forced to be float dtype, when `dtype` is a non-float
        dtype or None.

    ensure_non_negative : bool, default=False,
        Whether or not the weights are expected to be non-negative.

        .. versionadded:: 1.0

    ensure_same_device : bool, default=True
        Whether `sample_weight` should be forced to be on the same device as `X`.

    copy : bool, default=False
        If True, a copy of sample_weight will be created.

    Returns
    -------
    sample_weight : ndarray of shape (n_samples,)
        Validated sample weight. It is guaranteed to be "C" contiguous.
    """
    xp, is_array_api, device = get_namespace_and_device(X, remove_types=(int, float))

    n_samples = _num_samples(X)

    max_float_type = _max_precision_float_dtype(xp, device)
    float_dtypes = (
        [xp.float32] if max_float_type == xp.float32 else [xp.float64, xp.float32]
    )
    if force_float_dtype and dtype is not None and dtype not in float_dtypes:
        dtype = max_float_type

    if sample_weight is None:
        sample_weight = xp.ones(n_samples, dtype=dtype, device=device)
    elif isinstance(sample_weight, numbers.Number):
        sample_weight = xp.full(n_samples, sample_weight, dtype=dtype, device=device)
    else:
        if force_float_dtype and dtype is None:
            dtype = float_dtypes
        if is_array_api and ensure_same_device:
            sample_weight = xp.asarray(sample_weight, device=device)
        sample_weight = check_array(
            sample_weight,
            accept_sparse=False,
            ensure_2d=False,
            dtype=dtype,
            order="C",
            copy=copy,
            input_name="sample_weight",
        )
        if sample_weight.ndim != 1:
            raise ValueError(
                f"Sample weights must be 1D array or scalar, got "
                f"{sample_weight.ndim}D array. Expected either a scalar value "
                f"or a 1D array of length {n_samples}."
            )

        if sample_weight.shape != (n_samples,):
            raise ValueError(
                "sample_weight.shape == {}, expected {}!".format(
                    sample_weight.shape, (n_samples,)
                )
            )

    if ensure_non_negative:
        check_non_negative(sample_weight, "`sample_weight`")

    return sample_weight

scikit-learn.sklearn.utils.validation.validate_data

def validate_data(
    _estimator,
    /,
    X="no_validation",
    y="no_validation",
    reset=True,
    validate_separately=False,
    skip_check_array=False,
    **check_params,
):
    """Validate input data and set or check feature names and counts of the input.

    This helper function should be used in an estimator that requires input
    validation. This mutates the estimator and sets the `n_features_in_` and
    `feature_names_in_` attributes if `reset=True`.

    .. versionadded:: 1.6

    Parameters
    ----------
    _estimator : estimator instance
        The estimator to validate the input for.

    X : {array-like, sparse matrix, dataframe} of shape \
            (n_samples, n_features), default='no validation'
        The input samples.
        If `'no_validation'`, no validation is performed on `X`. This is
        useful for meta-estimator which can delegate input validation to
        their underlying estimator(s). In that case `y` must be passed and
        the only accepted `check_params` are `multi_output` and
        `y_numeric`.

    y : array-like of shape (n_samples,), default='no_validation'
        The targets.

        - If `None`, :func:`~sklearn.utils.check_array` is called on `X`. If
          the estimator's `requires_y` tag is True, then an error will be raised.
        - If `'no_validation'`, :func:`~sklearn.utils.check_array` is called
          on `X` and the estimator's `requires_y` tag is ignored. This is a default
          placeholder and is never meant to be explicitly set. In that case `X` must be
          passed.
        - Otherwise, only `y` with `_check_y` or both `X` and `y` are checked with
          either :func:`~sklearn.utils.check_array` or
          :func:`~sklearn.utils.check_X_y` depending on `validate_separately`.

    reset : bool, default=True
        Whether to reset the `n_features_in_` attribute.
        If False, the input will be checked for consistency with data
        provided when reset was last True.

        .. note::

           It is recommended to call `reset=True` in `fit` and in the first
           call to `partial_fit`. All other methods that validate `X`
           should set `reset=False`.

    validate_separately : False or tuple of dicts, default=False
        Only used if `y` is not `None`.
        If `False`, call :func:`~sklearn.utils.check_X_y`. Else, it must be a tuple of
        kwargs to be used for calling :func:`~sklearn.utils.check_array` on `X` and `y`
        respectively.

        `estimator=self` is automatically added to these dicts to generate
        more informative error message in case of invalid input data.

    skip_check_array : bool, default=False
        If `True`, `X` and `y` are unchanged and only `feature_names_in_` and
        `n_features_in_` are checked. Otherwise, :func:`~sklearn.utils.check_array`
        is called on `X` and `y`.

    **check_params : kwargs
        Parameters passed to :func:`~sklearn.utils.check_array` or
        :func:`~sklearn.utils.check_X_y`. Ignored if validate_separately
        is not False.

        `estimator=self` is automatically added to these params to generate
        more informative error message in case of invalid input data.

    Returns
    -------
    out : {ndarray, sparse matrix} or tuple of these
        The validated input. A tuple is returned if both `X` and `y` are
        validated.
    """
    _check_feature_names(_estimator, X, reset=reset)
    tags = get_tags(_estimator)
    if y is None and tags.target_tags.required:
        raise ValueError(
            f"This {_estimator.__class__.__name__} estimator "
            "requires y to be passed, but the target y is None."
        )

    no_val_X = isinstance(X, str) and X == "no_validation"
    no_val_y = y is None or (isinstance(y, str) and y == "no_validation")

    if no_val_X and no_val_y:
        raise ValueError("Validation should be done on X, y or both.")

    default_check_params = {"estimator": _estimator}
    check_params = {**default_check_params, **check_params}

    if skip_check_array:
        if not no_val_X and no_val_y:
            out = X
        elif no_val_X and not no_val_y:
            out = y
        else:
            out = X, y
    elif not no_val_X and no_val_y:
        out = check_array(X, input_name="X", **check_params)
    elif no_val_X and not no_val_y:
        out = _check_y(y, **check_params)
    else:
        if validate_separately:
            # We need this because some estimators validate X and y
            # separately, and in general, separately calling check_array()
            # on X and y isn't equivalent to just calling check_X_y()
            # :(
            check_X_params, check_y_params = validate_separately
            if "estimator" not in check_X_params:
                check_X_params = {**default_check_params, **check_X_params}
            X = check_array(X, input_name="X", **check_X_params)
            if "estimator" not in check_y_params:
                check_y_params = {**default_check_params, **check_y_params}
            y = check_array(y, input_name="y", **check_y_params)
        else:
            X, y = check_X_y(X, y, **check_params)
        out = X, y

    if not no_val_X and check_params.get("ensure_2d", True):
        _check_n_features(_estimator, X, reset=reset)

    return out


[/PYTHON]
What will be the input of `_partial_fit`, given the following output:
[OUTPUT]
```
{
    "output": "GaussianNB()"
}
```
[/OUTPUT]

[STRUCTURE]
```
{
    "self": {
        "priors": XXX,
        "var_smoothing": XXX
    },
    "args": {
        "X": XXX,
        "y": XXX,
        "classes": XXX,
        "_refit": XXX,
        "sample_weight": XXX
    },
    "kwargs": {}
}
```
[/STRUCTURE]

[INPUT]
