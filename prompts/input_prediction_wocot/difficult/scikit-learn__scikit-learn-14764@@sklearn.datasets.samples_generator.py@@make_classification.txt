You are given a Python function (between [PYTHON] and [/PYTHON]). For this function, I want you to take the provided output (between [OUTPUT] and [/OUTPUT]) and predict the input of the function. Both input and output are presented in a JSON format. The input structure is defined between [STRUCTURE] and [/STRUCTURE]. You only need to predict input variable values to fill out placeholders XXX in the structure, and print input between [INPUT] and [/INPUT]. You should maintain the structure when printing inputs. Do not change anything else.  ONLY print the input, DO NOT print any reasoning process.
[EXAMPLE]
[PYTHON]
class TempPathFactory(object):
    _given_basetemp = attr.ib(
        converter=attr.converters.optional(
            lambda p: Path(os.path.abspath(six.text_type(p)))
        )
    )
    _trace = attr.ib()
    _basetemp = attr.ib(default=None)

    def mktemp(self, basename, numbered=True):
        if not numbered:
            p = self.getbasetemp().joinpath(basename)
            p.mkdir()
        else:
            p = make_numbered_dir(root=self.getbasetemp(), prefix=basename)
            self._trace("mktemp", p)
        return p

    def getbasetemp(self):
        if self._given_basetemp is not None:
            basetemp = self._given_basetemp
            ensure_reset_dir(basetemp)
            basetemp = basetemp.resolve()
        else:
            from_env = os.environ.get("PYTEST_DEBUG_TEMPROOT")
            temproot = Path(from_env or tempfile.gettempdir()).resolve()
            user = get_user() or "unknown"
            rootdir = temproot.joinpath("pytest-of-{}".format(user))
            rootdir.mkdir(exist_ok=True)
            basetemp = make_numbered_dir_with_cleanup(
                prefix="pytest-", root=rootdir, keep=3, lock_timeout=LOCK_TIMEOUT
            )
        assert basetemp is not None, basetemp
        self._basetemp = t = basetemp
        self._trace("new basetemp", t)
        return t
[/PYTHON]

Functions called during the execution:
[PYTHON]
def parse_num(maybe_num):
    try:
        return int(maybe_num)
    except ValueError:
        return -1

def extract_suffixes(iter, prefix):
    p_len = len(prefix)
    for p in iter:
        yield p.name[p_len:]


def find_suffixes(root, prefix):
    return extract_suffixes(find_prefixed(root, prefix), prefix)

def make_numbered_dir(root, prefix):
    for i in range(10):
        # try up to 10 times to create the folder
        max_existing = _max(map(parse_num, find_suffixes(root, prefix)), default=-1)
        new_number = max_existing + 1
        new_path = root.joinpath("{}{}".format(prefix, new_number))
        try:
            new_path.mkdir()
        except Exception:
            pass
        else:
            _force_symlink(root, prefix + "current", new_path)
            return new_path
    else:
        raise EnvironmentError(
            "could not create numbered dir with prefix "
            "{prefix} in {root} after 10 tries".format(prefix=prefix, root=root)
        )
[/PYTHON]
What will be the input of `mktemp`, given the following output:
[OUTPUT]
```
{
    "strpath": "/tmp/pytest-of-root/pytest-0/test_mktemp0/world0"
}
```
[/OUTPUT]

[STRUCTURE]
```
{
    "self": {
        "_tmppath_factory": {
            "_given_basetemp": XXX,
            "_trace": XXX,
            "_basetemp": XXX
        }
    },
    "args": {
        "basename": XXX
    },
    "kwargs": XXX
}
```
[/STRUCTURE]

[INPUT]
```
{
    "self": {
        "_tmppath_factory": {
            "_given_basetemp": "/tmp/pytest-of-root/pytest-0/test_mktemp0",
            "_trace": {},
            "_basetemp": null
        }
    },
    "args": {
        "basename": "world"
    },
    "kwargs": {}
}
```
[/INPUT]
[/EXAMPLE]
[PYTHON]
import numbers
import array
from collections.abc import Iterable
import numpy as np
from scipy import linalg
import scipy.sparse as sp
from ..preprocessing import MultiLabelBinarizer
from ..utils import check_array, check_random_state
from ..utils import shuffle as util_shuffle
from ..utils.random import sample_without_replacement

def make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None):
    generator = check_random_state(random_state)
    if n_informative + n_redundant + n_repeated > n_features:
        raise ValueError('Number of informative, redundant and repeated features must sum to less than the number of total features')
    if n_informative < np.log2(n_classes * n_clusters_per_class):
        msg = 'n_classes({}) * n_clusters_per_class({}) must be'
        msg += ' smaller or equal 2**n_informative({})={}'
        raise ValueError(msg.format(n_classes, n_clusters_per_class, n_informative, 2 ** n_informative))
    if weights is not None:
        if len(weights) not in [n_classes, n_classes - 1]:
            raise ValueError('Weights specified but incompatible with number of classes.')
        if len(weights) == n_classes - 1:
            if isinstance(weights, list):
                weights = weights + [1.0 - sum(weights)]
            else:
                weights = np.resize(weights, n_classes)
                weights[-1] = 1.0 - sum(weights[:-1])
    else:
        weights = [1.0 / n_classes] * n_classes
    n_useless = n_features - n_informative - n_redundant - n_repeated
    n_clusters = n_classes * n_clusters_per_class
    n_samples_per_cluster = [int(n_samples * weights[k % n_classes] / n_clusters_per_class) for k in range(n_clusters)]
    for i in range(n_samples - sum(n_samples_per_cluster)):
        n_samples_per_cluster[i % n_clusters] += 1
    X = np.zeros((n_samples, n_features))
    y = np.zeros(n_samples, dtype=np.int)
    centroids = _generate_hypercube(n_clusters, n_informative, generator).astype(float, copy=False)
    centroids *= 2 * class_sep
    centroids -= class_sep
    if not hypercube:
        centroids *= generator.rand(n_clusters, 1)
        centroids *= generator.rand(1, n_informative)
    X[:, :n_informative] = generator.randn(n_samples, n_informative)
    stop = 0
    for k, centroid in enumerate(centroids):
        start, stop = (stop, stop + n_samples_per_cluster[k])
        y[start:stop] = k % n_classes
        X_k = X[start:stop, :n_informative]
        A = 2 * generator.rand(n_informative, n_informative) - 1
        X_k[...] = np.dot(X_k, A)
        X_k += centroid
    if n_redundant > 0:
        B = 2 * generator.rand(n_informative, n_redundant) - 1
        X[:, n_informative:n_informative + n_redundant] = np.dot(X[:, :n_informative], B)
    if n_repeated > 0:
        n = n_informative + n_redundant
        indices = ((n - 1) * generator.rand(n_repeated) + 0.5).astype(np.intp)
        X[:, n:n + n_repeated] = X[:, indices]
    if n_useless > 0:
        X[:, -n_useless:] = generator.randn(n_samples, n_useless)
    if flip_y >= 0.0:
        flip_mask = generator.rand(n_samples) < flip_y
        y[flip_mask] = generator.randint(n_classes, size=flip_mask.sum())
    if shift is None:
        shift = (2 * generator.rand(n_features) - 1) * class_sep
    X += shift
    if scale is None:
        scale = 1 + 100 * generator.rand(n_features)
    X *= scale
    if shuffle:
        X, y = util_shuffle(X, y, random_state=generator)
        indices = np.arange(n_features)
        generator.shuffle(indices)
        X[:, :] = X[:, indices]
    return (X, y)
[/PYTHON]

Functions called during the execution:
[PYTHON]
.sklearn.utils.validation.check_random_state

def check_random_state(seed):
    if seed is None or seed is np.random:
        return np.random.mtrand._rand
    if isinstance(seed, numbers.Integral):
        return np.random.RandomState(seed)
    if isinstance(seed, np.random.RandomState):
        return seed
    raise ValueError('%r cannot be used to seed a numpy.random.RandomState instance' % seed)

.sklearn.datasets.samples_generator._generate_hypercube

def _generate_hypercube(samples, dimensions, rng):
    if dimensions > 30:
        return np.hstack([rng.randint(2, size=(samples, dimensions - 30)), _generate_hypercube(samples, 30, rng)])
    out = sample_without_replacement(2 ** dimensions, samples, random_state=rng).astype(dtype='>u4', copy=False)
    out = np.unpackbits(out.view('>u1')).reshape((-1, 32))[:, -dimensions:]
    return out

.sklearn.utils.__init__.shuffle

def shuffle(*arrays, **options):
    options['replace'] = False
    return resample(*arrays, **options)

.sklearn.utils.__init__.resample

def resample(*arrays, **options):
    random_state = check_random_state(options.pop('random_state', None))
    replace = options.pop('replace', True)
    max_n_samples = options.pop('n_samples', None)
    stratify = options.pop('stratify', None)
    if options:
        raise ValueError('Unexpected kw arguments: %r' % options.keys())
    if len(arrays) == 0:
        return None
    first = arrays[0]
    n_samples = first.shape[0] if hasattr(first, 'shape') else len(first)
    if max_n_samples is None:
        max_n_samples = n_samples
    elif max_n_samples > n_samples and (not replace):
        raise ValueError('Cannot sample %d out of arrays with dim %d when replace is False' % (max_n_samples, n_samples))
    check_consistent_length(*arrays)
    if stratify is None:
        if replace:
            indices = random_state.randint(0, n_samples, size=(max_n_samples,))
        else:
            indices = np.arange(n_samples)
            random_state.shuffle(indices)
            indices = indices[:max_n_samples]
    else:
        y = check_array(stratify, ensure_2d=False, dtype=None)
        if y.ndim == 2:
            y = np.array([' '.join(row.astype('str')) for row in y])
        classes, y_indices = np.unique(y, return_inverse=True)
        n_classes = classes.shape[0]
        class_counts = np.bincount(y_indices)
        class_indices = np.split(np.argsort(y_indices, kind='mergesort'), np.cumsum(class_counts)[:-1])
        n_i = _approximate_mode(class_counts, max_n_samples, random_state)
        indices = []
        for i in range(n_classes):
            indices_i = random_state.choice(class_indices[i], n_i[i], replace=replace)
            indices.extend(indices_i)
        indices = random_state.permutation(indices)
    arrays = [a.tocsr() if issparse(a) else a for a in arrays]
    resampled_arrays = [safe_indexing(a, indices) for a in arrays]
    if len(resampled_arrays) == 1:
        return resampled_arrays[0]
    else:
        return resampled_arrays

.sklearn.utils.validation.check_consistent_length

def check_consistent_length(*arrays):
    lengths = [_num_samples(X) for X in arrays if X is not None]
    uniques = np.unique(lengths)
    if len(uniques) > 1:
        raise ValueError('Found input variables with inconsistent numbers of samples: %r' % [int(l) for l in lengths])

.sklearn.utils.validation._num_samples

def _num_samples(x):
    message = 'Expected sequence or array-like, got %s' % type(x)
    if hasattr(x, 'fit') and callable(x.fit):
        raise TypeError(message)
    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):
        if hasattr(x, '__array__'):
            x = np.asarray(x)
        else:
            raise TypeError(message)
    if hasattr(x, 'shape') and x.shape is not None:
        if len(x.shape) == 0:
            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)
        if isinstance(x.shape[0], numbers.Integral):
            return x.shape[0]
    try:
        return len(x)
    except TypeError:
        raise TypeError(message)

.sklearn.utils.__init__.safe_indexing

def safe_indexing(X, indices, axis=0):
    if axis == 0:
        return _safe_indexing_row(X, indices)
    elif axis == 1:
        return _safe_indexing_column(X, indices)
    else:
        raise ValueError("'axis' should be either 0 (to index rows) or 1 (to index  column). Got {} instead.".format(axis))

.sklearn.utils.__init__._safe_indexing_row

def _safe_indexing_row(X, indices):
    if hasattr(X, 'iloc'):
        indices = np.asarray(indices)
        indices = indices if indices.flags.writeable else indices.copy()
        try:
            return X.iloc[indices]
        except ValueError:
            warnings.warn('Copying input dataframe for slicing.', DataConversionWarning)
            return X.copy().iloc[indices]
    elif hasattr(X, 'shape'):
        if hasattr(X, 'take') and (hasattr(indices, 'dtype') and indices.dtype.kind == 'i'):
            return X.take(indices, axis=0)
        else:
            return _array_indexing(X, indices, axis=0)
    else:
        return [X[idx] for idx in indices]


[/PYTHON]
What will be the input of `make_classification`, given the following output:
[OUTPUT]
```
{
    "output": [
        "[[ 328434.66586239 -255362.15570402]\n [ 328436.097118   -255364.97671134]\n [ 328435.73961145 -255365.69840323]\n [ 328435.50017082 -255363.87733367]\n [ 328435.02866934 -255362.97077127]\n [ 328435.43805283 -255363.14888243]\n [ 328435.27292872 -255362.55251631]\n [ 328435.36397986 -255363.41127608]\n [ 328435.51915116 -255362.27519871]\n [ 328434.97443501 -255362.87915339]\n [ 328435.53728103 -255363.26169276]\n [ 328435.32902232 -255364.51198749]\n [ 328435.68824272 -255364.09401828]\n [ 328434.84371127 -255362.11321622]\n [ 328436.23331335 -255365.49845064]\n [ 328435.84781324 -255363.91395485]\n [ 328435.14202478 -255362.3480803 ]\n [ 328435.85270297 -255363.47460024]\n [ 328436.60601476 -255365.17392455]\n [ 328435.69223154 -255363.84613826]\n [ 328435.6729316  -255363.5240945 ]\n [ 328436.11625797 -255364.3464764 ]\n [ 328436.235488   -255363.90990873]\n [ 328435.6810815  -255363.13555321]\n [ 328436.00116581 -255364.14514574]\n [ 328435.70699997 -255363.41898716]\n [ 328436.04693406 -255364.82056284]\n [ 328435.72856619 -255364.01516304]\n [ 328435.68434356 -255364.80927626]\n [ 328435.81879553 -255363.46423399]\n [ 328435.84566657 -255363.4943814 ]\n [ 328435.53600787 -255362.99056535]\n [ 328435.04800727 -255363.61935203]\n [ 328436.34816775 -255364.31438859]\n [ 328436.00202247 -255364.02680335]\n [ 328435.38919208 -255363.2421753 ]\n [ 328435.29729745 -255362.66888332]\n [ 328435.01215103 -255362.3181975 ]\n [ 328436.19604769 -255364.3263313 ]\n [ 328436.18289339 -255364.77671778]\n [ 328435.4138623  -255364.88136555]\n [ 328435.16968119 -255362.0862695 ]\n [ 328435.37217325 -255363.33574889]\n [ 328435.40203029 -255363.78850527]\n [ 328436.05253975 -255364.65748833]\n [ 328435.92748898 -255364.58027077]\n [ 328435.75185622 -255364.46696181]\n [ 328435.4879383  -255364.3534759 ]\n [ 328435.75528043 -255364.1796863 ]\n [ 328434.67319496 -255362.04517104]]",
        "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0]"
    ]
}
```
[/OUTPUT]

[STRUCTURE]
```
{
    "self": {},
    "args": {},
    "kwargs": {
        "class_sep": XXX,
        "n_redundant": XXX,
        "n_repeated": XXX,
        "flip_y": XXX,
        "shift": XXX,
        "scale": XXX,
        "shuffle": XXX,
        "n_samples": XXX,
        "n_classes": XXX,
        "weights": XXX,
        "n_features": XXX,
        "n_informative": XXX,
        "n_clusters_per_class": XXX,
        "hypercube": XXX,
        "random_state": XXX
    }
}
```
[/STRUCTURE]

[INPUT]
