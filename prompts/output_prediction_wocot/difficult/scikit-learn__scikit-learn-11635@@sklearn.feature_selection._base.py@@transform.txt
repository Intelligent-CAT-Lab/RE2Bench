You are given a Python function (between [PYTHON] and [/PYTHON]). For this function, I want you to take the provided inputs (between [INPUT] and [/INPUT]) and predict the output of the function. Both input and output are presented in a JSON format. The output structure is defined between [STRUCTURE] and [/STRUCTURE]. You only need to predict output variable values to fill out placeholders XXX in the structure, and print output between [OUTPUT] and [/OUTPUT]. You should maintain the structure when printing output. Do not change anything else. ONLY print the output, DO NOT print any reasoning process.
[EXAMPLE]
[PYTHON]
class TempPathFactory(object):
    _given_basetemp = attr.ib(
        converter=attr.converters.optional(
            lambda p: Path(os.path.abspath(six.text_type(p)))
        )
    )
    _trace = attr.ib()
    _basetemp = attr.ib(default=None)

    def mktemp(self, basename, numbered=True):
        if not numbered:
            p = self.getbasetemp().joinpath(basename)
            p.mkdir()
        else:
            p = make_numbered_dir(root=self.getbasetemp(), prefix=basename)
            self._trace("mktemp", p)
        return p

    def getbasetemp(self):
        if self._basetemp is not None:
            return self._basetemp

        if self._given_basetemp is not None:
            basetemp = self._given_basetemp
            ensure_reset_dir(basetemp)
            basetemp = basetemp.resolve()
        else:
            from_env = os.environ.get("PYTEST_DEBUG_TEMPROOT")
            temproot = Path(from_env or tempfile.gettempdir()).resolve()
            user = get_user() or "unknown"
            rootdir = temproot.joinpath("pytest-of-{}".format(user))
            rootdir.mkdir(exist_ok=True)
            basetemp = make_numbered_dir_with_cleanup(
                prefix="pytest-", root=rootdir, keep=3, lock_timeout=LOCK_TIMEOUT
            )
        assert basetemp is not None, basetemp
        self._basetemp = t = basetemp
        self._trace("new basetemp", t)
        return t
[/PYTHON]

Functions called during the execution:
[PYTHON]
def parse_num(maybe_num):
    try:
        return int(maybe_num)
    except ValueError:
        return -1

def extract_suffixes(iter, prefix):
    p_len = len(prefix)
    for p in iter:
        yield p.name[p_len:]


def find_suffixes(root, prefix):
    return extract_suffixes(find_prefixed(root, prefix), prefix)

def make_numbered_dir(root, prefix):
    for i in range(10):
        # try up to 10 times to create the folder
        max_existing = _max(map(parse_num, find_suffixes(root, prefix)), default=-1)
        new_number = max_existing + 1
        new_path = root.joinpath("{}{}".format(prefix, new_number))
        try:
            new_path.mkdir()
        except Exception:
            pass
        else:
            _force_symlink(root, prefix + "current", new_path)
            return new_path
    else:
        raise EnvironmentError(
            "could not create numbered dir with prefix "
            "{prefix} in {root} after 10 tries".format(prefix=prefix, root=root)
        )

[/PYTHON]

What will be the output of `mktemp`, given the following input:
[INPUT]
{
    "self": {
        "_tmppath_factory": {
            "_given_basetemp": "/tmp/pytest-of-root/pytest-0/test_mktemp0",
            "_trace": {},
            "_basetemp": null
        }
    },
    "args": {
        "basename": "world"
    },
    "kwargs": {}
}
[/INPUT]

[STRUCTURE]
```
{ 
    "strpath": XXX
}
```
[/STRUCTURE]

[OUTPUT]
```
{
    "strpath": "/tmp/pytest-of-root/pytest-0/test_mktemp0/world0"
}
```
[/OUTPUT]
[/EXAMPLE]
[PYTHON]
from abc import ABCMeta, abstractmethod
from warnings import warn
import numpy as np
from scipy.sparse import issparse, csc_matrix
from ..base import TransformerMixin
from ..utils import check_array, safe_mask

class SelectorMixin(TransformerMixin, metaclass=ABCMeta):

    def get_support(self, indices=False):
        mask = self._get_support_mask()
        return mask if not indices else np.where(mask)[0]

    @abstractmethod
    def _get_support_mask(self):

    def transform(self, X):
        tags = self._get_tags()
        X = check_array(X, dtype=None, accept_sparse='csr', force_all_finite=not tags.get('allow_nan', True))
        mask = self.get_support()
        if not mask.any():
            warn('No features were selected: either the data is too noisy or the selection test too strict.', UserWarning)
            return np.empty(0).reshape((X.shape[0], 0))
        if len(mask) != X.shape[1]:
            raise ValueError('X has a different shape than during fitting.')
        return X[:, safe_mask(X, mask)]

    def inverse_transform(self, X):
        if issparse(X):
            X = X.tocsc()
            it = self.inverse_transform(np.diff(X.indptr).reshape(1, -1))
            col_nonzeros = it.ravel()
            indptr = np.concatenate([[0], np.cumsum(col_nonzeros)])
            Xt = csc_matrix((X.data, X.indices, indptr), shape=(X.shape[0], len(indptr) - 1), dtype=X.dtype)
            return Xt
        support = self.get_support()
        X = check_array(X, dtype=None)
        if support.sum() != X.shape[1]:
            raise ValueError('X has a different shape than during fitting.')
        if X.ndim == 1:
            X = X[None, :]
        Xt = np.zeros((X.shape[0], support.size), dtype=X.dtype)
        Xt[:, support] = X
        return Xt
[/PYTHON]

Functions called during the execution:
[PYTHON]
.sklearn.base.BaseEstimator._get_tags

def _get_tags(self):
    collected_tags = {}
    for base_class in reversed(inspect.getmro(self.__class__)):
        if hasattr(base_class, '_more_tags'):
            more_tags = base_class._more_tags(self)
            collected_tags.update(more_tags)
    return collected_tags

.sklearn.base.BaseEstimator._more_tags

def _more_tags(self):
    return _DEFAULT_TAGS

.sklearn.feature_selection._from_model.SelectFromModel._more_tags

def _more_tags(self):
    estimator_tags = self.estimator._get_tags()
    return {'allow_nan': estimator_tags.get('allow_nan', True)}

.sklearn.base.MultiOutputMixin._more_tags

def _more_tags(self):
    return {'multioutput': True}

.sklearn.utils.validation.check_array

def check_array(array, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=None, estimator=None):
    if warn_on_dtype is not None:
        warnings.warn("'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.", FutureWarning, stacklevel=2)
    array_orig = array
    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'
    dtype_orig = getattr(array, 'dtype', None)
    if not hasattr(dtype_orig, 'kind'):
        dtype_orig = None
    dtypes_orig = None
    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):
        dtypes_orig = np.array(array.dtypes)
        if all((isinstance(dtype, np.dtype) for dtype in dtypes_orig)):
            dtype_orig = np.result_type(*array.dtypes)
    if dtype_numeric:
        if dtype_orig is not None and dtype_orig.kind == 'O':
            dtype = np.float64
        else:
            dtype = None
    if isinstance(dtype, (list, tuple)):
        if dtype_orig is not None and dtype_orig in dtype:
            dtype = None
        else:
            dtype = dtype[0]
    if force_all_finite not in (True, False, 'allow-nan'):
        raise ValueError('force_all_finite should be a bool or "allow-nan". Got {!r} instead'.format(force_all_finite))
    if estimator is not None:
        if isinstance(estimator, str):
            estimator_name = estimator
        else:
            estimator_name = estimator.__class__.__name__
    else:
        estimator_name = 'Estimator'
    context = ' by %s' % estimator_name if estimator is not None else ''
    if sp.issparse(array):
        _ensure_no_complex_data(array)
        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse)
    else:
        with warnings.catch_warnings():
            try:
                warnings.simplefilter('error', ComplexWarning)
                if dtype is not None and np.dtype(dtype).kind in 'iu':
                    array = np.asarray(array, order=order)
                    if array.dtype.kind == 'f':
                        _assert_all_finite(array, allow_nan=False, msg_dtype=dtype)
                    array = array.astype(dtype, casting='unsafe', copy=False)
                else:
                    array = np.asarray(array, order=order, dtype=dtype)
            except ComplexWarning:
                raise ValueError('Complex data not supported\n{}\n'.format(array))
        _ensure_no_complex_data(array)
        if ensure_2d:
            if array.ndim == 0:
                raise ValueError('Expected 2D array, got scalar array instead:\narray={}.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))
            if array.ndim == 1:
                raise ValueError('Expected 2D array, got 1D array instead:\narray={}.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))
        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):
            warnings.warn("Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).", FutureWarning, stacklevel=2)
        if dtype_numeric and array.dtype.kind == 'O':
            array = array.astype(np.float64)
        if not allow_nd and array.ndim >= 3:
            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))
        if force_all_finite:
            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')
    if ensure_min_samples > 0:
        n_samples = _num_samples(array)
        if n_samples < ensure_min_samples:
            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))
    if ensure_min_features > 0 and array.ndim == 2:
        n_features = array.shape[1]
        if n_features < ensure_min_features:
            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))
    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):
        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)
        warnings.warn(msg, DataConversionWarning, stacklevel=2)
    if copy and np.may_share_memory(array, array_orig):
        array = np.array(array, dtype=dtype, order=order)
    if warn_on_dtype and dtypes_orig is not None and ({array.dtype} != set(dtypes_orig)):
        msg = 'Data with input dtype %s were all converted to %s%s.' % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype, context)
        warnings.warn(msg, DataConversionWarning, stacklevel=3)
    return array

.sklearn.utils.validation._ensure_no_complex_data

def _ensure_no_complex_data(array):
    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):
        raise ValueError('Complex data not supported\n{}\n'.format(array))

.sklearn.utils.validation._assert_all_finite

def _assert_all_finite(X, allow_nan=False, msg_dtype=None):
    from .extmath import _safe_accumulator_op
    if _get_config()['assume_finite']:
        return
    X = np.asanyarray(X)
    is_float = X.dtype.kind in 'fc'
    if is_float and np.isfinite(_safe_accumulator_op(np.sum, X)):
        pass
    elif is_float:
        msg_err = 'Input contains {} or a value too large for {!r}.'
        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):
            type_err = 'infinity' if allow_nan else 'NaN, infinity'
            raise ValueError(msg_err.format(type_err, msg_dtype if msg_dtype is not None else X.dtype))
    elif X.dtype == np.dtype('object') and (not allow_nan):
        if _object_dtype_isnan(X).any():
            raise ValueError('Input contains NaN')

.sklearn._config.get_config

def get_config():
    return _global_config.copy()

.sklearn.utils.extmath._safe_accumulator_op

def _safe_accumulator_op(op, x, *args, **kwargs):
    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:
        result = op(x, *args, **kwargs, dtype=np.float64)
    else:
        result = op(x, *args, **kwargs)
    return result

.sklearn.utils.validation._num_samples

def _num_samples(x):
    message = 'Expected sequence or array-like, got %s' % type(x)
    if hasattr(x, 'fit') and callable(x.fit):
        raise TypeError(message)
    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):
        if hasattr(x, '__array__'):
            x = np.asarray(x)
        else:
            raise TypeError(message)
    if hasattr(x, 'shape') and x.shape is not None:
        if len(x.shape) == 0:
            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)
        if isinstance(x.shape[0], numbers.Integral):
            return x.shape[0]
    try:
        return len(x)
    except TypeError:
        raise TypeError(message)

.sklearn.feature_selection._from_model.SelectFromModel._get_support_mask

def _get_support_mask(self):
    if self.prefit:
        estimator = self.estimator
    elif hasattr(self, 'estimator_'):
        estimator = self.estimator_
    else:
        raise ValueError('Either fit the model before transform or set "prefit=True" while passing the fitted estimator to the constructor.')
    scores = _get_feature_importances(estimator, self.norm_order)
    threshold = _calculate_threshold(estimator, scores, self.threshold)
    if self.max_features is not None:
        mask = np.zeros_like(scores, dtype=bool)
        candidate_indices = np.argsort(-scores, kind='mergesort')[:self.max_features]
        mask[candidate_indices] = True
    else:
        mask = np.ones_like(scores, dtype=bool)
    mask[scores < threshold] = False
    return mask

.sklearn.feature_selection._from_model._get_feature_importances

def _get_feature_importances(estimator, norm_order=1):
    importances = getattr(estimator, 'feature_importances_', None)
    coef_ = getattr(estimator, 'coef_', None)
    if importances is None and coef_ is not None:
        if estimator.coef_.ndim == 1:
            importances = np.abs(coef_)
        else:
            importances = np.linalg.norm(coef_, axis=0, ord=norm_order)
    elif importances is None:
        raise ValueError('The underlying estimator %s has no `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to SelectFromModel or call fit before calling transform.' % estimator.__class__.__name__)
    return importances

.sklearn.ensemble._forest.BaseForest.feature_importances_

def feature_importances_(self):
    check_is_fitted(self)
    all_importances = Parallel(n_jobs=self.n_jobs, **_joblib_parallel_args(prefer='threads'))((delayed(getattr)(tree, 'feature_importances_') for tree in self.estimators_ if tree.tree_.node_count > 1))
    if not all_importances:
        return np.zeros(self.n_features_, dtype=np.float64)
    all_importances = np.mean(all_importances, axis=0, dtype=np.float64)
    return all_importances / np.sum(all_importances)

.sklearn.utils.validation.check_is_fitted

def check_is_fitted(estimator, attributes='deprecated', msg=None, all_or_any='deprecated'):
    if attributes != 'deprecated':
        warnings.warn('Passing attributes to check_is_fitted is deprecated and will be removed in 0.23. The attributes argument is ignored.', FutureWarning)
    if all_or_any != 'deprecated':
        warnings.warn('Passing all_or_any to check_is_fitted is deprecated and will be removed in 0.23. The any_or_all argument is ignored.', FutureWarning)
    if isclass(estimator):
        raise TypeError('{} is a class, not an instance.'.format(estimator))
    if msg is None:
        msg = "This %(name)s instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
    if not hasattr(estimator, 'fit'):
        raise TypeError('%s is not an estimator instance.' % estimator)
    attrs = [v for v in vars(estimator) if (v.endswith('_') or v.startswith('_')) and (not v.startswith('__'))]
    if not attrs:
        raise NotFittedError(msg % {'name': type(estimator).__name__})

.sklearn.utils.fixes._joblib_parallel_args

def _joblib_parallel_args(**kwargs):
    import joblib
    if joblib.__version__ >= LooseVersion('0.12'):
        return kwargs
    extra_args = set(kwargs.keys()).difference({'prefer', 'require'})
    if extra_args:
        raise NotImplementedError('unhandled arguments %s with joblib %s' % (list(extra_args), joblib.__version__))
    args = {}
    if 'prefer' in kwargs:
        prefer = kwargs['prefer']
        if prefer not in ['threads', 'processes', None]:
            raise ValueError('prefer=%s is not supported' % prefer)
        args['backend'] = {'threads': 'threading', 'processes': 'multiprocessing', None: None}[prefer]
    if 'require' in kwargs:
        require = kwargs['require']
        if require not in [None, 'sharedmem']:
            raise ValueError('require=%s is not supported' % require)
        if require == 'sharedmem':
            args['backend'] = 'threading'
    return args

.sklearn.tree._classes.BaseDecisionTree.feature_importances_

def feature_importances_(self):
    check_is_fitted(self)
    return self.tree_.compute_feature_importances()

.sklearn.feature_selection._from_model._calculate_threshold

def _calculate_threshold(estimator, importances, threshold):
    if threshold is None:
        est_name = estimator.__class__.__name__
        if hasattr(estimator, 'penalty') and estimator.penalty == 'l1' or 'Lasso' in est_name:
            threshold = 1e-05
        else:
            threshold = 'mean'
    if isinstance(threshold, str):
        if '*' in threshold:
            scale, reference = threshold.split('*')
            scale = float(scale.strip())
            reference = reference.strip()
            if reference == 'median':
                reference = np.median(importances)
            elif reference == 'mean':
                reference = np.mean(importances)
            else:
                raise ValueError('Unknown reference: ' + reference)
            threshold = scale * reference
        elif threshold == 'median':
            threshold = np.median(importances)
        elif threshold == 'mean':
            threshold = np.mean(importances)
        else:
            raise ValueError("Expected threshold='mean' or threshold='median' got %s" % threshold)
    else:
        threshold = float(threshold)
    return threshold

.sklearn.feature_selection._rfe.RFE._more_tags

def _more_tags(self):
    estimator_tags = self.estimator._get_tags()
    return {'poor_score': True, 'allow_nan': estimator_tags.get('allow_nan', True)}

.sklearn.feature_selection._rfe.RFE._get_support_mask

def _get_support_mask(self):
    check_is_fitted(self)
    return self.support_

.sklearn.utils.validation._ensure_sparse_format

def _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse):
    if dtype is None:
        dtype = spmatrix.dtype
    changed_format = False
    if isinstance(accept_sparse, str):
        accept_sparse = [accept_sparse]
    _check_large_sparse(spmatrix, accept_large_sparse)
    if accept_sparse is False:
        raise TypeError('A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.')
    elif isinstance(accept_sparse, (list, tuple)):
        if len(accept_sparse) == 0:
            raise ValueError("When providing 'accept_sparse' as a tuple or list, it must contain at least one string value.")
        if spmatrix.format not in accept_sparse:
            spmatrix = spmatrix.asformat(accept_sparse[0])
            changed_format = True
    elif accept_sparse is not True:
        raise ValueError("Parameter 'accept_sparse' should be a string, boolean or list of strings. You provided 'accept_sparse={}'.".format(accept_sparse))
    if dtype != spmatrix.dtype:
        spmatrix = spmatrix.astype(dtype)
    elif copy and (not changed_format):
        spmatrix = spmatrix.copy()
    if force_all_finite:
        if not hasattr(spmatrix, 'data'):
            warnings.warn("Can't check %s sparse matrix for nan or inf." % spmatrix.format, stacklevel=2)
        else:
            _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == 'allow-nan')
    return spmatrix

.sklearn.utils.validation._check_large_sparse

def _check_large_sparse(X, accept_large_sparse=False):
    if not accept_large_sparse:
        supported_indices = ['int32']
        if X.getformat() == 'coo':
            index_keys = ['col', 'row']
        elif X.getformat() in ['csr', 'csc', 'bsr']:
            index_keys = ['indices', 'indptr']
        else:
            return
        for key in index_keys:
            indices_datatype = getattr(X, key).dtype
            if indices_datatype not in supported_indices:
                raise ValueError('Only sparse matrices with 32-bit integer indices are accepted. Got %s indices.' % indices_datatype)

.sklearn.feature_selection._variance_threshold.VarianceThreshold._more_tags

def _more_tags(self):
    return {'allow_nan': True}

.sklearn.feature_selection._variance_threshold.VarianceThreshold._get_support_mask

def _get_support_mask(self):
    check_is_fitted(self)
    return self.variances_ > self.threshold


[/PYTHON]
What will be the output of `transform`, given the following input:
[INPUT]
```
{
    "self": {
        "estimator": {
            "decision_function_shape": "ovr",
            "break_ties": false,
            "kernel": "linear",
            "degree": 3,
            "gamma": "scale",
            "coef0": 0.0,
            "tol": 0.001,
            "C": 1.0,
            "nu": 0.0,
            "epsilon": 0.0,
            "shrinking": true,
            "probability": false,
            "cache_size": 200,
            "class_weight": null,
            "verbose": false,
            "max_iter": -1,
            "random_state": null
        },
        "n_features_to_select": null,
        "step": 1,
        "verbose": 0,
        "estimator_": {
            "decision_function_shape": "ovr",
            "break_ties": false,
            "kernel": "linear",
            "degree": 3,
            "gamma": "scale",
            "coef0": 0.0,
            "tol": 0.001,
            "C": 1.0,
            "nu": 0.0,
            "epsilon": 0.0,
            "shrinking": true,
            "probability": false,
            "cache_size": 200,
            "class_weight": null,
            "verbose": false,
            "max_iter": -1,
            "random_state": null,
            "_sparse": false,
            "class_weight_": "[1. 1. 1.]",
            "classes_": "[0 1 2]",
            "_gamma": 0.14353552621394797,
            "support_": "[ 34  40  42  44  46  50  52  56  57  63  65  66  78  86  91  93  96  97\n 103 104 108 112 116 119]",
            "support_vectors_": "[[1.9 0.4]\n [4.7 1.4]\n [4.9 1.5]\n [4.6 1.5]\n [4.7 1.6]\n [4.8 1.8]\n [4.9 1.5]\n [4.8 1.4]\n [5.  1.7]\n [5.1 1.6]\n [4.5 1.6]\n [4.7 1.5]\n [3.  1.1]\n [4.5 1.7]\n [4.9 2. ]\n [4.9 1.8]\n [4.8 1.8]\n [4.9 1.8]\n [5.1 1.5]\n [5.6 1.4]\n [4.8 1.8]\n [5.1 1.9]\n [5.  1.9]\n [5.1 1.8]]",
            "_n_support": "[ 1 12 11]",
            "dual_coef_": "[[ 1.         -0.         -0.         -0.         -0.         -0.\n  -0.         -0.         -0.         -0.         -0.         -0.\n  -1.         -0.23668638 -0.         -0.         -0.         -0.\n  -0.         -0.         -0.         -0.         -0.         -0.        ]\n [ 0.23668638  1.          1.          1.          1.          1.\n   1.          1.          1.          1.          1.          1.\n   0.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.        ]]",
            "intercept_": "[ 3.27999997  2.2923075  13.57000542]",
            "probA_": "[]",
            "probB_": "[]",
            "fit_status_": 0,
            "shape_fit_": [
                120,
                2
            ],
            "_intercept_": "[ 3.27999997  2.2923075  13.57000542]",
            "_dual_coef_": "[[ 1.         -0.         -0.         -0.         -0.         -0.\n  -0.         -0.         -0.         -0.         -0.         -0.\n  -1.         -0.23668638 -0.         -0.         -0.         -0.\n  -0.         -0.         -0.         -0.         -0.         -0.        ]\n [ 0.23668638  1.          1.          1.          1.          1.\n   1.          1.          1.          1.          1.          1.\n   0.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.        ]]"
        },
        "n_features_": "2",
        "support_": "[False False  True  True]",
        "ranking_": "[3 2 1 1]"
    },
    "args": {
        "X": "[[5.4 3.7 1.5 0.2]\n [4.8 3.4 1.6 0.2]\n [4.8 3.  1.4 0.1]\n [4.3 3.  1.1 0.1]\n [5.8 4.  1.2 0.2]\n [5.7 4.4 1.5 0.4]\n [5.4 3.9 1.3 0.4]\n [5.1 3.5 1.4 0.3]\n [5.7 3.8 1.7 0.3]\n [5.1 3.8 1.5 0.3]\n [5.  2.  3.5 1. ]\n [5.9 3.  4.2 1.5]\n [6.  2.2 4.  1. ]\n [6.1 2.9 4.7 1.4]\n [5.6 2.9 3.6 1.3]\n [6.7 3.1 4.4 1.4]\n [5.6 3.  4.5 1.5]\n [5.8 2.7 4.1 1. ]\n [6.2 2.2 4.5 1.5]\n [5.6 2.5 3.9 1.1]\n [6.5 3.2 5.1 2. ]\n [6.4 2.7 5.3 1.9]\n [6.8 3.  5.5 2.1]\n [5.7 2.5 5.  2. ]\n [5.8 2.8 5.1 2.4]\n [6.4 3.2 5.3 2.3]\n [6.5 3.  5.5 1.8]\n [7.7 3.8 6.7 2.2]\n [7.7 2.6 6.9 2.3]\n [6.  2.2 5.  1.5]]"
    },
    "kwargs": {}
}
```
[/INPUT]

[STRUCTURE]
```
{
    "output": XXX        
}
```
[/STRUCTURE]

[OUTPUT]
