You are given a Python function (between [PYTHON] and [/PYTHON]). For this function, I want you to take the provided inputs (between [INPUT] and [/INPUT]) and predict the output of the function. Both input and output are presented in a JSON format. The output structure is defined between [STRUCTURE] and [/STRUCTURE]. You only need to predict output variable values to fill out placeholders XXX in the structure, and print output between [OUTPUT] and [/OUTPUT]. You should maintain the structure when printing output. Do not change anything else. ONLY print the output, DO NOT print any reasoning process.
[EXAMPLE]
[PYTHON]
class TempPathFactory(object):
    _given_basetemp = attr.ib(
        converter=attr.converters.optional(
            lambda p: Path(os.path.abspath(six.text_type(p)))
        )
    )
    _trace = attr.ib()
    _basetemp = attr.ib(default=None)

    def mktemp(self, basename, numbered=True):
        if not numbered:
            p = self.getbasetemp().joinpath(basename)
            p.mkdir()
        else:
            p = make_numbered_dir(root=self.getbasetemp(), prefix=basename)
            self._trace("mktemp", p)
        return p

    def getbasetemp(self):
        if self._basetemp is not None:
            return self._basetemp

        if self._given_basetemp is not None:
            basetemp = self._given_basetemp
            ensure_reset_dir(basetemp)
            basetemp = basetemp.resolve()
        else:
            from_env = os.environ.get("PYTEST_DEBUG_TEMPROOT")
            temproot = Path(from_env or tempfile.gettempdir()).resolve()
            user = get_user() or "unknown"
            rootdir = temproot.joinpath("pytest-of-{}".format(user))
            rootdir.mkdir(exist_ok=True)
            basetemp = make_numbered_dir_with_cleanup(
                prefix="pytest-", root=rootdir, keep=3, lock_timeout=LOCK_TIMEOUT
            )
        assert basetemp is not None, basetemp
        self._basetemp = t = basetemp
        self._trace("new basetemp", t)
        return t
[/PYTHON]

Functions called during the execution:
[PYTHON]
def parse_num(maybe_num):
    try:
        return int(maybe_num)
    except ValueError:
        return -1

def extract_suffixes(iter, prefix):
    p_len = len(prefix)
    for p in iter:
        yield p.name[p_len:]


def find_suffixes(root, prefix):
    return extract_suffixes(find_prefixed(root, prefix), prefix)

def make_numbered_dir(root, prefix):
    for i in range(10):
        # try up to 10 times to create the folder
        max_existing = _max(map(parse_num, find_suffixes(root, prefix)), default=-1)
        new_number = max_existing + 1
        new_path = root.joinpath("{}{}".format(prefix, new_number))
        try:
            new_path.mkdir()
        except Exception:
            pass
        else:
            _force_symlink(root, prefix + "current", new_path)
            return new_path
    else:
        raise EnvironmentError(
            "could not create numbered dir with prefix "
            "{prefix} in {root} after 10 tries".format(prefix=prefix, root=root)
        )

[/PYTHON]

What will be the output of `mktemp`, given the following input:
[INPUT]
{
    "self": {
        "_tmppath_factory": {
            "_given_basetemp": "/tmp/pytest-of-root/pytest-0/test_mktemp0",
            "_trace": {},
            "_basetemp": null
        }
    },
    "args": {
        "basename": "world"
    },
    "kwargs": {}
}
[/INPUT]

[STRUCTURE]
```
{ 
    "strpath": XXX
}
```
[/STRUCTURE]

[OUTPUT]
```
{
    "strpath": "/tmp/pytest-of-root/pytest-0/test_mktemp0/world0"
}
```
[/OUTPUT]
[/EXAMPLE]
[PYTHON]
import warnings
from math import sqrt
import numpy as np
from scipy import linalg
from scipy.linalg.lapack import get_lapack_funcs
from .base import LinearModel, _pre_fit
from ..base import RegressorMixin
from ..utils import as_float_array, check_array, check_X_y
from ..model_selection import check_cv
from ..externals.joblib import Parallel, delayed
premature = ' Orthogonal matching pursuit ended prematurely due to linear\ndependence in the dictionary. The requested precision might not have been met.\n'

def orthogonal_mp_gram(Gram, Xy, n_nonzero_coefs=None, tol=None, norms_squared=None, copy_Gram=True, copy_Xy=True, return_path=False, return_n_iter=False):
    Gram = check_array(Gram, order='F', copy=copy_Gram)
    Xy = np.asarray(Xy)
    if Xy.ndim > 1 and Xy.shape[1] > 1:
        copy_Gram = True
    if Xy.ndim == 1:
        Xy = Xy[:, np.newaxis]
        if tol is not None:
            norms_squared = [norms_squared]
    if copy_Xy or not Xy.flags.writeable:
        Xy = Xy.copy()
    if n_nonzero_coefs is None and tol is None:
        n_nonzero_coefs = int(0.1 * len(Gram))
    if tol is not None and norms_squared is None:
        raise ValueError('Gram OMP needs the precomputed norms in order to evaluate the error sum of squares.')
    if tol is not None and tol < 0:
        raise ValueError('Epsilon cannot be negative')
    if tol is None and n_nonzero_coefs <= 0:
        raise ValueError('The number of atoms must be positive')
    if tol is None and n_nonzero_coefs > len(Gram):
        raise ValueError('The number of atoms cannot be more than the number of features')
    if return_path:
        coef = np.zeros((len(Gram), Xy.shape[1], len(Gram)))
    else:
        coef = np.zeros((len(Gram), Xy.shape[1]))
    n_iters = []
    for k in range(Xy.shape[1]):
        out = _gram_omp(Gram, Xy[:, k], n_nonzero_coefs, norms_squared[k] if tol is not None else None, tol, copy_Gram=copy_Gram, copy_Xy=False, return_path=return_path)
        if return_path:
            _, idx, coefs, n_iter = out
            coef = coef[:, :, :len(idx)]
            for n_active, x in enumerate(coefs.T):
                coef[idx[:n_active + 1], k, n_active] = x[:n_active + 1]
        else:
            x, idx, n_iter = out
            coef[idx, k] = x
        n_iters.append(n_iter)
    if Xy.shape[1] == 1:
        n_iters = n_iters[0]
    if return_n_iter:
        return (np.squeeze(coef), n_iters)
    else:
        return np.squeeze(coef)
[/PYTHON]

Functions called during the execution:
[PYTHON]
.sklearn.utils.validation.check_array

def check_array(array, accept_sparse=False, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None):
    if accept_sparse is None:
        warnings.warn("Passing 'None' to parameter 'accept_sparse' in methods check_array and check_X_y is deprecated in version 0.19 and will be removed in 0.21. Use 'accept_sparse=False'  instead.", DeprecationWarning)
        accept_sparse = False
    array_orig = array
    dtype_numeric = isinstance(dtype, six.string_types) and dtype == 'numeric'
    dtype_orig = getattr(array, 'dtype', None)
    if not hasattr(dtype_orig, 'kind'):
        dtype_orig = None
    if dtype_numeric:
        if dtype_orig is not None and dtype_orig.kind == 'O':
            dtype = np.float64
        else:
            dtype = None
    if isinstance(dtype, (list, tuple)):
        if dtype_orig is not None and dtype_orig in dtype:
            dtype = None
        else:
            dtype = dtype[0]
    if force_all_finite not in (True, False, 'allow-nan'):
        raise ValueError('force_all_finite should be a bool or "allow-nan". Got {!r} instead'.format(force_all_finite))
    if estimator is not None:
        if isinstance(estimator, six.string_types):
            estimator_name = estimator
        else:
            estimator_name = estimator.__class__.__name__
    else:
        estimator_name = 'Estimator'
    context = ' by %s' % estimator_name if estimator is not None else ''
    if sp.issparse(array):
        _ensure_no_complex_data(array)
        array = _ensure_sparse_format(array, accept_sparse, dtype, copy, force_all_finite)
    else:
        with warnings.catch_warnings():
            try:
                warnings.simplefilter('error', ComplexWarning)
                array = np.asarray(array, dtype=dtype, order=order)
            except ComplexWarning:
                raise ValueError('Complex data not supported\n{}\n'.format(array))
        _ensure_no_complex_data(array)
        if ensure_2d:
            if array.ndim == 0:
                raise ValueError('Expected 2D array, got scalar array instead:\narray={}.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))
            if array.ndim == 1:
                raise ValueError('Expected 2D array, got 1D array instead:\narray={}.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))
        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):
            warnings.warn("Beginning in version 0.22, arrays of strings will be interpreted as decimal numbers if parameter 'dtype' is 'numeric'. It is recommended that you convert the array to type np.float64 before passing it to check_array.", FutureWarning)
        if dtype_numeric and array.dtype.kind == 'O':
            array = array.astype(np.float64)
        if not allow_nd and array.ndim >= 3:
            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))
        if force_all_finite:
            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')
    shape_repr = _shape_repr(array.shape)
    if ensure_min_samples > 0:
        n_samples = _num_samples(array)
        if n_samples < ensure_min_samples:
            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, shape_repr, ensure_min_samples, context))
    if ensure_min_features > 0 and array.ndim == 2:
        n_features = array.shape[1]
        if n_features < ensure_min_features:
            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, shape_repr, ensure_min_features, context))
    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):
        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)
        warnings.warn(msg, DataConversionWarning)
    if copy and np.may_share_memory(array, array_orig):
        array = np.array(array, dtype=dtype, order=order)
    return array

.sklearn.utils.validation._ensure_no_complex_data

def _ensure_no_complex_data(array):
    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):
        raise ValueError('Complex data not supported\n{}\n'.format(array))

.sklearn.utils.validation._assert_all_finite

def _assert_all_finite(X, allow_nan=False):
    if _get_config()['assume_finite']:
        return
    X = np.asanyarray(X)
    is_float = X.dtype.kind in 'fc'
    if is_float and np.isfinite(X.sum()):
        pass
    elif is_float:
        msg_err = 'Input contains {} or a value too large for {!r}.'
        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):
            type_err = 'infinity' if allow_nan else 'NaN, infinity'
            raise ValueError(msg_err.format(type_err, X.dtype))

.sklearn._config.get_config

def get_config():
    return _global_config.copy()

.sklearn.utils.validation._shape_repr

def _shape_repr(shape):
    if len(shape) == 0:
        return '()'
    joined = ', '.join(('%d' % e for e in shape))
    if len(shape) == 1:
        joined += ','
    return '(%s)' % joined

.sklearn.utils.validation._num_samples

def _num_samples(x):
    if hasattr(x, 'fit') and callable(x.fit):
        raise TypeError('Expected sequence or array-like, got estimator %s' % x)
    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):
        if hasattr(x, '__array__'):
            x = np.asarray(x)
        else:
            raise TypeError('Expected sequence or array-like, got %s' % type(x))
    if hasattr(x, 'shape'):
        if len(x.shape) == 0:
            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)
        return x.shape[0]
    else:
        return len(x)

.sklearn.linear_model.omp._gram_omp

def _gram_omp(Gram, Xy, n_nonzero_coefs, tol_0=None, tol=None, copy_Gram=True, copy_Xy=True, return_path=False):
    Gram = Gram.copy('F') if copy_Gram else np.asfortranarray(Gram)
    if copy_Xy or not Xy.flags.writeable:
        Xy = Xy.copy()
    min_float = np.finfo(Gram.dtype).eps
    nrm2, swap = linalg.get_blas_funcs(('nrm2', 'swap'), (Gram,))
    potrs, = get_lapack_funcs(('potrs',), (Gram,))
    indices = np.arange(len(Gram))
    alpha = Xy
    tol_curr = tol_0
    delta = 0
    gamma = np.empty(0)
    n_active = 0
    max_features = len(Gram) if tol is not None else n_nonzero_coefs
    L = np.empty((max_features, max_features), dtype=Gram.dtype)
    L[0, 0] = 1.0
    if return_path:
        coefs = np.empty_like(L)
    while True:
        lam = np.argmax(np.abs(alpha))
        if lam < n_active or alpha[lam] ** 2 < min_float:
            warnings.warn(premature, RuntimeWarning, stacklevel=3)
            break
        if n_active > 0:
            L[n_active, :n_active] = Gram[lam, :n_active]
            linalg.solve_triangular(L[:n_active, :n_active], L[n_active, :n_active], trans=0, lower=1, overwrite_b=True, check_finite=False)
            v = nrm2(L[n_active, :n_active]) ** 2
            Lkk = Gram[lam, lam] - v
            if Lkk <= min_float:
                warnings.warn(premature, RuntimeWarning, stacklevel=3)
                break
            L[n_active, n_active] = sqrt(Lkk)
        else:
            L[0, 0] = sqrt(Gram[lam, lam])
        Gram[n_active], Gram[lam] = swap(Gram[n_active], Gram[lam])
        Gram.T[n_active], Gram.T[lam] = swap(Gram.T[n_active], Gram.T[lam])
        indices[n_active], indices[lam] = (indices[lam], indices[n_active])
        Xy[n_active], Xy[lam] = (Xy[lam], Xy[n_active])
        n_active += 1
        gamma, _ = potrs(L[:n_active, :n_active], Xy[:n_active], lower=True, overwrite_b=False)
        if return_path:
            coefs[:n_active, n_active - 1] = gamma
        beta = np.dot(Gram[:, :n_active], gamma)
        alpha = Xy - beta
        if tol is not None:
            tol_curr += delta
            delta = np.inner(gamma, beta[:n_active])
            tol_curr -= delta
            if abs(tol_curr) <= tol:
                break
        elif n_active == max_features:
            break
    if return_path:
        return (gamma, indices[:n_active], coefs[:, :n_active], n_active)
    else:
        return (gamma, indices[:n_active], n_active)

.sklearn.externals.joblib.pool.ArrayMemmapReducer.__call__

def __call__(self, a):
    m = _get_backing_memmap(a)
    if m is not None:
        return _reduce_memmap_backed(a, m)
    if not a.dtype.hasobject and self._max_nbytes is not None and (a.nbytes > self._max_nbytes):
        try:
            os.makedirs(self._temp_folder)
            os.chmod(self._temp_folder, FOLDER_PERMISSIONS)
        except OSError as e:
            if e.errno != errno.EEXIST:
                raise e
        basename = '%d-%d-%s.pkl' % (os.getpid(), id(threading.current_thread()), hash(a))
        filename = os.path.join(self._temp_folder, basename)
        if not os.path.exists(filename):
            if self.verbose > 0:
                print('Memmaping (shape=%r, dtype=%s) to new file %s' % (a.shape, a.dtype, filename))
            for dumped_filename in dump(a, filename):
                os.chmod(dumped_filename, FILE_PERMISSIONS)
            if self._prewarm:
                load(filename, mmap_mode=self._mmap_mode).max()
        elif self.verbose > 1:
            print('Memmaping (shape=%s, dtype=%s) to old file %s' % (a.shape, a.dtype, filename))
        return (load, (filename, self._mmap_mode))
    else:
        if self.verbose > 1:
            print('Pickling array (shape=%r, dtype=%s).' % (a.shape, a.dtype))
        return (loads, (dumps(a, protocol=HIGHEST_PROTOCOL),))

.sklearn.externals.joblib.pool._get_backing_memmap

def _get_backing_memmap(a):
    b = getattr(a, 'base', None)
    if b is None:
        return None
    elif isinstance(b, mmap):
        return a
    else:
        return _get_backing_memmap(b)

.sklearn.externals.joblib.pool.CustomizablePicklingQueue.send

def send(obj):
    buffer = BytesIO()
    CustomizablePickler(buffer, self._reducers).dump(obj)
    self._writer.send_bytes(buffer.getvalue())

.sklearn.externals.joblib.pool.CustomizablePickler.__init__

def __init__(self, writer, reducers=None, protocol=HIGHEST_PROTOCOL):
    Pickler.__init__(self, writer, protocol=protocol)
    if reducers is None:
        reducers = {}
    if hasattr(Pickler, 'dispatch'):
        self.dispatch = Pickler.dispatch.copy()
    else:
        self.dispatch_table = copyreg.dispatch_table.copy()
    for type, reduce_func in reducers.items():
        self.register(type, reduce_func)

.sklearn.externals.joblib.pool.CustomizablePickler.register

def register(self, type, reduce_func):
    if hasattr(Pickler, 'dispatch'):

        def dispatcher(self, obj):
            reduced = reduce_func(obj)
            self.save_reduce(*reduced, obj=obj)
        self.dispatch[type] = dispatcher
    else:
        self.dispatch_table[type] = reduce_func

.sklearn.externals.joblib._parallel_backends.SafeFunction.__call__

def __call__(self, *args, **kwargs):
    try:
        return self.func(*args, **kwargs)
    except KeyboardInterrupt:
        raise WorkerInterrupt()
    except:
        e_type, e_value, e_tb = sys.exc_info()
        text = format_exc(e_type, e_value, e_tb, context=10, tb_offset=1)
        raise TransportableException(text, e_type)

.sklearn.externals.joblib.parallel.BatchedCalls.__call__

def __call__(self):
    return [func(*args, **kwargs) for func, args, kwargs in self.items]

.sklearn.decomposition.dict_learning._sparse_encode

def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars', regularization=None, copy_cov=True, init=None, max_iter=1000, check_input=True, verbose=0, positive=False):
    if X.ndim == 1:
        X = X[:, np.newaxis]
    n_samples, n_features = X.shape
    n_components = dictionary.shape[0]
    if dictionary.shape[1] != X.shape[1]:
        raise ValueError('Dictionary and X have different numbers of features:dictionary.shape: {} X.shape{}'.format(dictionary.shape, X.shape))
    if cov is None and algorithm != 'lasso_cd':
        copy_cov = False
        cov = np.dot(dictionary, X.T)
    if algorithm == 'lasso_lars':
        alpha = float(regularization) / n_features
        try:
            err_mgt = np.seterr(all='ignore')
            lasso_lars = LassoLars(alpha=alpha, fit_intercept=False, verbose=verbose, normalize=False, precompute=gram, fit_path=False, positive=positive)
            lasso_lars.fit(dictionary.T, X.T, Xy=cov)
            new_code = lasso_lars.coef_
        finally:
            np.seterr(**err_mgt)
    elif algorithm == 'lasso_cd':
        alpha = float(regularization) / n_features
        clf = Lasso(alpha=alpha, fit_intercept=False, normalize=False, precompute=gram, max_iter=max_iter, warm_start=True, positive=positive)
        if init is not None:
            clf.coef_ = init
        clf.fit(dictionary.T, X.T, check_input=check_input)
        new_code = clf.coef_
    elif algorithm == 'lars':
        try:
            err_mgt = np.seterr(all='ignore')
            lars = Lars(fit_intercept=False, verbose=verbose, normalize=False, precompute=gram, n_nonzero_coefs=int(regularization), fit_path=False, positive=positive)
            lars.fit(dictionary.T, X.T, Xy=cov)
            new_code = lars.coef_
        finally:
            np.seterr(**err_mgt)
    elif algorithm == 'threshold':
        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T
        if positive:
            new_code[new_code < 0] = 0
    elif algorithm == 'omp':
        if positive:
            raise ValueError('Positive constraint not supported for "omp" coding method.')
        new_code = orthogonal_mp_gram(Gram=gram, Xy=cov, n_nonzero_coefs=int(regularization), tol=None, norms_squared=row_norms(X, squared=True), copy_Xy=copy_cov).T
    else:
        raise ValueError('Sparse coding method must be "lasso_lars" "lasso_cd",  "lasso", "threshold" or "omp", got %s.' % algorithm)
    if new_code.ndim != 2:
        return new_code.reshape(n_samples, n_components)
    return new_code

.sklearn.utils.extmath.row_norms

def row_norms(X, squared=False):
    if sparse.issparse(X):
        if not isinstance(X, sparse.csr_matrix):
            X = sparse.csr_matrix(X)
        norms = csr_row_norms(X)
    else:
        norms = np.einsum('ij,ij->i', X, X)
    if not squared:
        np.sqrt(norms, norms)
    return norms

.sklearn.externals.joblib.pool.CustomizablePicklingQueue.put

def put(obj):
    wlock_acquire()
    try:
        return send(obj)
    finally:
        wlock_release()

.sklearn.externals.joblib.parallel.BatchCompletionCallBack.__call__

def __call__(self, out):
    self.parallel.n_completed_tasks += self.batch_size
    this_batch_duration = time.time() - self.dispatch_timestamp
    self.parallel._backend.batch_completed(self.batch_size, this_batch_duration)
    self.parallel.print_progress()
    if self.parallel._original_iterator is not None:
        self.parallel.dispatch_next()

.sklearn.externals.joblib._parallel_backends.AutoBatchingMixin.batch_completed

def batch_completed(self, batch_size, duration):
    if batch_size == self._effective_batch_size:
        old_duration = self._smoothed_batch_duration
        if old_duration == 0:
            new_duration = duration
        else:
            new_duration = 0.8 * old_duration + 0.2 * duration
        self._smoothed_batch_duration = new_duration

.sklearn.externals.joblib.parallel.Parallel.print_progress

def print_progress(self):
    if not self.verbose:
        return
    elapsed_time = time.time() - self._start_time
    if self._original_iterator is not None:
        if _verbosity_filter(self.n_dispatched_batches, self.verbose):
            return
        self._print('Done %3i tasks      | elapsed: %s', (self.n_completed_tasks, short_format_time(elapsed_time)))
    else:
        index = self.n_completed_tasks
        total_tasks = self.n_dispatched_tasks
        if not index == 0:
            cursor = total_tasks - index + 1 - self._pre_dispatch_amount
            frequency = total_tasks // self.verbose + 1
            is_last_item = index + 1 == total_tasks
            if is_last_item or cursor % frequency:
                return
        remaining_time = elapsed_time / index * (self.n_dispatched_tasks - index * 1.0)
        self._print('Done %3i out of %3i | elapsed: %s remaining: %s', (index, total_tasks, short_format_time(elapsed_time), short_format_time(remaining_time)))

.sklearn.externals.joblib.pool.CustomizablePicklingQueue.get

def get():
    racquire()
    try:
        return recv()
    finally:
        rrelease()


[/PYTHON]
What will be the output of `orthogonal_mp_gram`, given the following input:
[INPUT]
```
{
    "self": {},
    "args": {},
    "kwargs": {
        "Gram": "[[ 1.00000000e+00  3.81506944e-01  1.14715887e-01 -4.68607043e-01\n   5.84719658e-02 -1.00601972e-01 -1.83510043e-01 -1.65667971e-02\n   3.27876822e-01 -4.48139501e-02 -1.62726917e-02  4.96932416e-01]\n [ 3.81506944e-01  1.00000000e+00  5.98382782e-02  9.25732329e-02\n  -2.06897998e-01 -8.91322682e-02 -3.17686864e-01  2.65142869e-02\n  -3.54909210e-01 -7.21809509e-01  9.00883273e-03  6.58684170e-01]\n [ 1.14715887e-01  5.98382782e-02  1.00000000e+00 -1.49677007e-01\n   1.35940238e-01 -4.37486996e-01 -3.09998023e-01 -5.04662967e-01\n   2.97667021e-01 -3.39423421e-01 -1.46652624e-01  6.71427599e-03]\n [-4.68607043e-01  9.25732329e-02 -1.49677007e-01  1.00000000e+00\n  -3.90373744e-01  2.40310978e-01  4.07849862e-01  3.92318210e-01\n  -7.21266184e-01 -1.34785291e-01 -2.76100508e-01 -2.79362559e-04]\n [ 5.84719658e-02 -2.06897998e-01  1.35940238e-01 -3.90373744e-01\n   1.00000000e+00 -7.46501844e-01 -1.20107166e-01 -6.24618127e-01\n   5.78463889e-01  3.10437036e-01 -7.78310684e-02 -4.36706827e-02]\n [-1.00601972e-01 -8.91322682e-02 -4.37486996e-01  2.40310978e-01\n  -7.46501844e-01  1.00000000e+00  2.67422100e-02  3.54147450e-01\n  -3.43679149e-01  2.78389735e-02  3.22626373e-02 -3.91819703e-01]\n [-1.83510043e-01 -3.17686864e-01 -3.09998023e-01  4.07849862e-01\n  -1.20107166e-01  2.67422100e-02  1.00000000e+00  7.37160571e-01\n  -2.73861456e-01  3.44858307e-01 -1.42671753e-01 -1.14706234e-01]\n [-1.65667971e-02  2.65142869e-02 -5.04662967e-01  3.92318210e-01\n  -6.24618127e-01  3.54147450e-01  7.37160571e-01  1.00000000e+00\n  -5.23514292e-01  7.13866157e-02  5.21231760e-02  2.17745098e-01]\n [ 3.27876822e-01 -3.54909210e-01  2.97667021e-01 -7.21266184e-01\n   5.78463889e-01 -3.43679149e-01 -2.73861456e-01 -5.23514292e-01\n   1.00000000e+00  6.55590151e-02 -3.79448801e-01 -3.46617185e-01]\n [-4.48139501e-02 -7.21809509e-01 -3.39423421e-01 -1.34785291e-01\n   3.10437036e-01  2.78389735e-02  3.44858307e-01  7.13866157e-02\n   6.55590151e-02  1.00000000e+00  4.37528732e-01 -1.70499778e-01]\n [-1.62726917e-02  9.00883273e-03 -1.46652624e-01 -2.76100508e-01\n  -7.78310684e-02  3.22626373e-02 -1.42671753e-01  5.21231760e-02\n  -3.79448801e-01  4.37528732e-01  1.00000000e+00  3.73148348e-01]\n [ 4.96932416e-01  6.58684170e-01  6.71427599e-03 -2.79362559e-04\n  -4.36706827e-02 -3.91819703e-01 -1.14706234e-01  2.17745098e-01\n  -3.46617185e-01 -1.70499778e-01  3.73148348e-01  1.00000000e+00]]",
        "Xy": "[[ 0.88986487]\n [ 2.33249979]\n [ 0.13957277]\n [ 0.21592705]\n [-0.48258954]\n [-0.207901  ]\n [-0.74100454]\n [ 0.06184457]\n [-0.82782566]\n [-1.68362053]\n [ 0.0210131 ]\n [ 1.53638069]]",
        "n_nonzero_coefs": 1,
        "tol": null,
        "norms_squared": "[5.44055527]",
        "copy_Xy": false
    }
}
```
[/INPUT]

[STRUCTURE]
```
{
    "output": XXX        
}
```
[/STRUCTURE]

[OUTPUT]
