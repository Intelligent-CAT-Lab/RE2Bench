You are given a Python function (between [PYTHON] and [/PYTHON]). For this function, I want you to take the provided inputs (between [INPUT] and [/INPUT]) and predict the output of the function. Both input and output are presented in a JSON format. The output structure is defined between [STRUCTURE] and [/STRUCTURE]. You only need to predict output variable values to fill out placeholders XXX in the structure, and print output between [OUTPUT] and [/OUTPUT]. You should maintain the structure when printing output. Do not change anything else. ONLY print the output, DO NOT print any reasoning process.
[EXAMPLE]
[PYTHON]
class TempPathFactory(object):
    _given_basetemp = attr.ib(
        converter=attr.converters.optional(
            lambda p: Path(os.path.abspath(six.text_type(p)))
        )
    )
    _trace = attr.ib()
    _basetemp = attr.ib(default=None)

    def mktemp(self, basename, numbered=True):
        if not numbered:
            p = self.getbasetemp().joinpath(basename)
            p.mkdir()
        else:
            p = make_numbered_dir(root=self.getbasetemp(), prefix=basename)
            self._trace("mktemp", p)
        return p

    def getbasetemp(self):
        if self._basetemp is not None:
            return self._basetemp

        if self._given_basetemp is not None:
            basetemp = self._given_basetemp
            ensure_reset_dir(basetemp)
            basetemp = basetemp.resolve()
        else:
            from_env = os.environ.get("PYTEST_DEBUG_TEMPROOT")
            temproot = Path(from_env or tempfile.gettempdir()).resolve()
            user = get_user() or "unknown"
            rootdir = temproot.joinpath("pytest-of-{}".format(user))
            rootdir.mkdir(exist_ok=True)
            basetemp = make_numbered_dir_with_cleanup(
                prefix="pytest-", root=rootdir, keep=3, lock_timeout=LOCK_TIMEOUT
            )
        assert basetemp is not None, basetemp
        self._basetemp = t = basetemp
        self._trace("new basetemp", t)
        return t
[/PYTHON]

Functions called during the execution:
[PYTHON]
def parse_num(maybe_num):
    try:
        return int(maybe_num)
    except ValueError:
        return -1

def extract_suffixes(iter, prefix):
    p_len = len(prefix)
    for p in iter:
        yield p.name[p_len:]


def find_suffixes(root, prefix):
    return extract_suffixes(find_prefixed(root, prefix), prefix)

def make_numbered_dir(root, prefix):
    for i in range(10):
        # try up to 10 times to create the folder
        max_existing = _max(map(parse_num, find_suffixes(root, prefix)), default=-1)
        new_number = max_existing + 1
        new_path = root.joinpath("{}{}".format(prefix, new_number))
        try:
            new_path.mkdir()
        except Exception:
            pass
        else:
            _force_symlink(root, prefix + "current", new_path)
            return new_path
    else:
        raise EnvironmentError(
            "could not create numbered dir with prefix "
            "{prefix} in {root} after 10 tries".format(prefix=prefix, root=root)
        )

[/PYTHON]

What will be the output of `mktemp`, given the following input:
[INPUT]
{
    "self": {
        "_tmppath_factory": {
            "_given_basetemp": "/tmp/pytest-of-root/pytest-0/test_mktemp0",
            "_trace": {},
            "_basetemp": null
        }
    },
    "args": {
        "basename": "world"
    },
    "kwargs": {}
}
[/INPUT]

[STRUCTURE]
```
{ 
    "strpath": XXX
}
```
[/STRUCTURE]

[OUTPUT]
```
{
    "strpath": "/tmp/pytest-of-root/pytest-0/test_mktemp0/world0"
}
```
[/OUTPUT]
[/EXAMPLE]
[PYTHON]
import warnings
import numpy as np
from scipy.sparse import coo_matrix
from scipy.sparse import csr_matrix
from ..preprocessing import LabelBinarizer, label_binarize
from ..preprocessing import LabelEncoder
from ..utils import assert_all_finite
from ..utils import check_array
from ..utils import check_consistent_length
from ..utils import column_or_1d
from ..utils.multiclass import unique_labels
from ..utils.multiclass import type_of_target
from ..utils.validation import _num_samples
from ..utils.sparsefuncs import count_nonzero
from ..exceptions import UndefinedMetricWarning

def precision_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None):
    p, _, _, _ = precision_recall_fscore_support(y_true, y_pred, labels=labels, pos_label=pos_label, average=average, warn_for=('precision',), sample_weight=sample_weight)
    return p
[/PYTHON]

Functions called during the execution:
[PYTHON]
.sklearn.metrics.classification.precision_recall_fscore_support

def precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None):
    average_options = (None, 'micro', 'macro', 'weighted', 'samples')
    if average not in average_options and average != 'binary':
        raise ValueError('average has to be one of ' + str(average_options))
    if beta <= 0:
        raise ValueError('beta should be >0 in the F-beta score')
    y_type, y_true, y_pred = _check_targets(y_true, y_pred)
    check_consistent_length(y_true, y_pred, sample_weight)
    present_labels = unique_labels(y_true, y_pred)
    if average == 'binary':
        if y_type == 'binary':
            if pos_label not in present_labels and len(present_labels) >= 2:
                raise ValueError('pos_label=%r is not a valid label: %r' % (pos_label, present_labels))
            labels = [pos_label]
        else:
            raise ValueError("Target is %s but average='binary'. Please choose another average setting." % y_type)
    elif pos_label not in (None, 1):
        warnings.warn("Note that pos_label (set to %r) is ignored when average != 'binary' (got %r). You may use labels=[pos_label] to specify a single positive class." % (pos_label, average), UserWarning)
    samplewise = average == 'samples'
    MCM = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight, labels=labels, samplewise=samplewise)
    tp_sum = MCM[:, 1, 1]
    pred_sum = tp_sum + MCM[:, 0, 1]
    true_sum = tp_sum + MCM[:, 1, 0]
    if average == 'micro':
        tp_sum = np.array([tp_sum.sum()])
        pred_sum = np.array([pred_sum.sum()])
        true_sum = np.array([true_sum.sum()])
    beta2 = beta ** 2
    with np.errstate(divide='ignore', invalid='ignore'):
        precision = _prf_divide(tp_sum, pred_sum, 'precision', 'predicted', average, warn_for)
        recall = _prf_divide(tp_sum, true_sum, 'recall', 'true', average, warn_for)
        f_score = (1 + beta2) * precision * recall / (beta2 * precision + recall)
        f_score[tp_sum == 0] = 0.0
    if average == 'weighted':
        weights = true_sum
        if weights.sum() == 0:
            return (0, 0, 0, None)
    elif average == 'samples':
        weights = sample_weight
    else:
        weights = None
    if average is not None:
        assert average != 'binary' or len(precision) == 1
        precision = np.average(precision, weights=weights)
        recall = np.average(recall, weights=weights)
        f_score = np.average(f_score, weights=weights)
        true_sum = None
    return (precision, recall, f_score, true_sum)

.sklearn.metrics.classification._check_targets

def _check_targets(y_true, y_pred):
    check_consistent_length(y_true, y_pred)
    type_true = type_of_target(y_true)
    type_pred = type_of_target(y_pred)
    y_type = {type_true, type_pred}
    if y_type == {'binary', 'multiclass'}:
        y_type = {'multiclass'}
    if len(y_type) > 1:
        raise ValueError("Classification metrics can't handle a mix of {0} and {1} targets".format(type_true, type_pred))
    y_type = y_type.pop()
    if y_type not in ['binary', 'multiclass', 'multilabel-indicator']:
        raise ValueError('{0} is not supported'.format(y_type))
    if y_type in ['binary', 'multiclass']:
        y_true = column_or_1d(y_true)
        y_pred = column_or_1d(y_pred)
        if y_type == 'binary':
            unique_values = np.union1d(y_true, y_pred)
            if len(unique_values) > 2:
                y_type = 'multiclass'
    if y_type.startswith('multilabel'):
        y_true = csr_matrix(y_true)
        y_pred = csr_matrix(y_pred)
        y_type = 'multilabel-indicator'
    return (y_type, y_true, y_pred)

.sklearn.utils.validation.check_consistent_length

def check_consistent_length(*arrays):
    lengths = [_num_samples(X) for X in arrays if X is not None]
    uniques = np.unique(lengths)
    if len(uniques) > 1:
        raise ValueError('Found input variables with inconsistent numbers of samples: %r' % [int(l) for l in lengths])

.sklearn.utils.validation._num_samples

def _num_samples(x):
    if hasattr(x, 'fit') and callable(x.fit):
        raise TypeError('Expected sequence or array-like, got estimator %s' % x)
    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):
        if hasattr(x, '__array__'):
            x = np.asarray(x)
        else:
            raise TypeError('Expected sequence or array-like, got %s' % type(x))
    if hasattr(x, 'shape'):
        if len(x.shape) == 0:
            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)
        if isinstance(x.shape[0], numbers.Integral):
            return x.shape[0]
        else:
            return len(x)
    else:
        return len(x)

.sklearn.utils.multiclass.type_of_target

def type_of_target(y):
    valid = (isinstance(y, (Sequence, spmatrix)) or hasattr(y, '__array__')) and (not isinstance(y, str))
    if not valid:
        raise ValueError('Expected array-like (array or non-string sequence), got %r' % y)
    sparseseries = y.__class__.__name__ == 'SparseSeries'
    if sparseseries:
        raise ValueError("y cannot be class 'SparseSeries'.")
    if is_multilabel(y):
        return 'multilabel-indicator'
    try:
        y = np.asarray(y)
    except ValueError:
        return 'unknown'
    try:
        if not hasattr(y[0], '__array__') and isinstance(y[0], Sequence) and (not isinstance(y[0], str)):
            raise ValueError('You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead.')
    except IndexError:
        pass
    if y.ndim > 2 or (y.dtype == object and len(y) and (not isinstance(y.flat[0], str))):
        return 'unknown'
    if y.ndim == 2 and y.shape[1] == 0:
        return 'unknown'
    if y.ndim == 2 and y.shape[1] > 1:
        suffix = '-multioutput'
    else:
        suffix = ''
    if y.dtype.kind == 'f' and np.any(y != y.astype(int)):
        return 'continuous' + suffix
    if len(np.unique(y)) > 2 or (y.ndim >= 2 and len(y[0]) > 1):
        return 'multiclass' + suffix
    else:
        return 'binary'

.sklearn.utils.multiclass.is_multilabel

def is_multilabel(y):
    if hasattr(y, '__array__'):
        y = np.asarray(y)
    if not (hasattr(y, 'shape') and y.ndim == 2 and (y.shape[1] > 1)):
        return False
    if issparse(y):
        if isinstance(y, (dok_matrix, lil_matrix)):
            y = y.tocsr()
        return len(y.data) == 0 or (np.unique(y.data).size == 1 and (y.dtype.kind in 'biu' or _is_integral_float(np.unique(y.data))))
    else:
        labels = np.unique(y)
        return len(labels) < 3 and (y.dtype.kind in 'biu' or _is_integral_float(labels))

.sklearn.utils.validation.column_or_1d

def column_or_1d(y, warn=False):
    shape = np.shape(y)
    if len(shape) == 1:
        return np.ravel(y)
    if len(shape) == 2 and shape[1] == 1:
        if warn:
            warnings.warn('A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().', DataConversionWarning, stacklevel=2)
        return np.ravel(y)
    raise ValueError('bad input shape {0}'.format(shape))

.sklearn.utils.multiclass.unique_labels

def unique_labels(*ys):
    if not ys:
        raise ValueError('No argument has been passed.')
    ys_types = set((type_of_target(x) for x in ys))
    if ys_types == {'binary', 'multiclass'}:
        ys_types = {'multiclass'}
    if len(ys_types) > 1:
        raise ValueError('Mix type of y not allowed, got types %s' % ys_types)
    label_type = ys_types.pop()
    if label_type == 'multilabel-indicator' and len(set((check_array(y, ['csr', 'csc', 'coo']).shape[1] for y in ys))) > 1:
        raise ValueError('Multi-label binary indicator input with different numbers of labels')
    _unique_labels = _FN_UNIQUE_LABELS.get(label_type, None)
    if not _unique_labels:
        raise ValueError('Unknown label type: %s' % repr(ys))
    ys_labels = set(chain.from_iterable((_unique_labels(y) for y in ys)))
    if len(set((isinstance(label, str) for label in ys_labels))) > 1:
        raise ValueError('Mix of label input types (string and number)')
    return np.array(sorted(ys_labels))

.sklearn.utils.multiclass._unique_multiclass

def _unique_multiclass(y):
    if hasattr(y, '__array__'):
        return np.unique(np.asarray(y))
    else:
        return set(y)

.sklearn.metrics.classification.multilabel_confusion_matrix

def multilabel_confusion_matrix(y_true, y_pred, sample_weight=None, labels=None, samplewise=False):
    y_type, y_true, y_pred = _check_targets(y_true, y_pred)
    if sample_weight is not None:
        sample_weight = column_or_1d(sample_weight)
    check_consistent_length(y_true, y_pred, sample_weight)
    if y_type not in ('binary', 'multiclass', 'multilabel-indicator'):
        raise ValueError('%s is not supported' % y_type)
    present_labels = unique_labels(y_true, y_pred)
    if labels is None:
        labels = present_labels
        n_labels = None
    else:
        n_labels = len(labels)
        labels = np.hstack([labels, np.setdiff1d(present_labels, labels, assume_unique=True)])
    if y_true.ndim == 1:
        if samplewise:
            raise ValueError('Samplewise metrics are not available outside of multilabel classification.')
        le = LabelEncoder()
        le.fit(labels)
        y_true = le.transform(y_true)
        y_pred = le.transform(y_pred)
        sorted_labels = le.classes_
        tp = y_true == y_pred
        tp_bins = y_true[tp]
        if sample_weight is not None:
            tp_bins_weights = np.asarray(sample_weight)[tp]
        else:
            tp_bins_weights = None
        if len(tp_bins):
            tp_sum = np.bincount(tp_bins, weights=tp_bins_weights, minlength=len(labels))
        else:
            true_sum = pred_sum = tp_sum = np.zeros(len(labels))
        if len(y_pred):
            pred_sum = np.bincount(y_pred, weights=sample_weight, minlength=len(labels))
        if len(y_true):
            true_sum = np.bincount(y_true, weights=sample_weight, minlength=len(labels))
        indices = np.searchsorted(sorted_labels, labels[:n_labels])
        tp_sum = tp_sum[indices]
        true_sum = true_sum[indices]
        pred_sum = pred_sum[indices]
    else:
        sum_axis = 1 if samplewise else 0
        if not np.array_equal(labels, present_labels):
            if np.max(labels) > np.max(present_labels):
                raise ValueError('All labels must be in [0, n labels) for multilabel targets. Got %d > %d' % (np.max(labels), np.max(present_labels)))
            if np.min(labels) < 0:
                raise ValueError('All labels must be in [0, n labels) for multilabel targets. Got %d < 0' % np.min(labels))
        if n_labels is not None:
            y_true = y_true[:, labels[:n_labels]]
            y_pred = y_pred[:, labels[:n_labels]]
        true_and_pred = y_true.multiply(y_pred)
        tp_sum = count_nonzero(true_and_pred, axis=sum_axis, sample_weight=sample_weight)
        pred_sum = count_nonzero(y_pred, axis=sum_axis, sample_weight=sample_weight)
        true_sum = count_nonzero(y_true, axis=sum_axis, sample_weight=sample_weight)
    fp = pred_sum - tp_sum
    fn = true_sum - tp_sum
    tp = tp_sum
    if sample_weight is not None and samplewise:
        sample_weight = np.array(sample_weight)
        tp = np.array(tp)
        fp = np.array(fp)
        fn = np.array(fn)
        tn = sample_weight * y_true.shape[1] - tp - fp - fn
    elif sample_weight is not None:
        tn = sum(sample_weight) - tp - fp - fn
    elif samplewise:
        tn = y_true.shape[1] - tp - fp - fn
    else:
        tn = y_true.shape[0] - tp - fp - fn
    return np.array([tn, fp, fn, tp]).T.reshape(-1, 2, 2)

.sklearn.preprocessing.label.LabelEncoder.fit

def fit(self, y):
    y = column_or_1d(y, warn=True)
    self.classes_ = _encode(y)
    return self

.sklearn.preprocessing.label._encode

def _encode(values, uniques=None, encode=False):
    if values.dtype == object:
        return _encode_python(values, uniques, encode)
    else:
        return _encode_numpy(values, uniques, encode)

.sklearn.preprocessing.label._encode_numpy

def _encode_numpy(values, uniques=None, encode=False):
    if uniques is None:
        if encode:
            uniques, encoded = np.unique(values, return_inverse=True)
            return (uniques, encoded)
        else:
            return np.unique(values)
    if encode:
        diff = _encode_check_unknown(values, uniques)
        if diff:
            raise ValueError('y contains previously unseen labels: %s' % str(diff))
        encoded = np.searchsorted(uniques, values)
        return (uniques, encoded)
    else:
        return uniques

.sklearn.preprocessing.label.LabelEncoder.transform

def transform(self, y):
    check_is_fitted(self, 'classes_')
    y = column_or_1d(y, warn=True)
    if _num_samples(y) == 0:
        return np.array([])
    _, y = _encode(y, uniques=self.classes_, encode=True)
    return y

.sklearn.utils.validation.check_is_fitted

def check_is_fitted(estimator, attributes, msg=None, all_or_any=all):
    if msg is None:
        msg = "This %(name)s instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
    if not hasattr(estimator, 'fit'):
        raise TypeError('%s is not an estimator instance.' % estimator)
    if not isinstance(attributes, (list, tuple)):
        attributes = [attributes]
    if not all_or_any([hasattr(estimator, attr) for attr in attributes]):
        raise NotFittedError(msg % {'name': type(estimator).__name__})

.sklearn.preprocessing.label._encode_check_unknown

def _encode_check_unknown(values, uniques, return_mask=False):
    if values.dtype == object:
        uniques_set = set(uniques)
        diff = list(set(values) - uniques_set)
        if return_mask:
            if diff:
                valid_mask = np.array([val in uniques_set for val in values])
            else:
                valid_mask = np.ones(len(values), dtype=bool)
            return (diff, valid_mask)
        else:
            return diff
    else:
        unique_values = np.unique(values)
        diff = list(np.setdiff1d(unique_values, uniques, assume_unique=True))
        if return_mask:
            if diff:
                valid_mask = np.in1d(values, uniques)
            else:
                valid_mask = np.ones(len(values), dtype=bool)
            return (diff, valid_mask)
        else:
            return diff

.sklearn.metrics.classification._prf_divide

def _prf_divide(numerator, denominator, metric, modifier, average, warn_for):
    result = numerator / denominator
    mask = denominator == 0.0
    if not np.any(mask):
        return result
    result[mask] = 0.0
    axis0 = 'sample'
    axis1 = 'label'
    if average == 'samples':
        axis0, axis1 = (axis1, axis0)
    if metric in warn_for and 'f-score' in warn_for:
        msg_start = '{0} and F-score are'.format(metric.title())
    elif metric in warn_for:
        msg_start = '{0} is'.format(metric.title())
    elif 'f-score' in warn_for:
        msg_start = 'F-score is'
    else:
        return result
    msg = '{0} ill-defined and being set to 0.0 {{0}} no {1} {2}s.'.format(msg_start, modifier, axis0)
    if len(mask) == 1:
        msg = msg.format('due to')
    else:
        msg = msg.format('in {0}s with'.format(axis1))
    warnings.warn(msg, UndefinedMetricWarning, stacklevel=2)
    return result

.sklearn.utils.validation.check_array

def check_array(array, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None):
    if accept_sparse is None:
        warnings.warn("Passing 'None' to parameter 'accept_sparse' in methods check_array and check_X_y is deprecated in version 0.19 and will be removed in 0.21. Use 'accept_sparse=False'  instead.", DeprecationWarning)
        accept_sparse = False
    array_orig = array
    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'
    dtype_orig = getattr(array, 'dtype', None)
    if not hasattr(dtype_orig, 'kind'):
        dtype_orig = None
    dtypes_orig = None
    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):
        dtypes_orig = np.array(array.dtypes)
    if dtype_numeric:
        if dtype_orig is not None and dtype_orig.kind == 'O':
            dtype = np.float64
        else:
            dtype = None
    if isinstance(dtype, (list, tuple)):
        if dtype_orig is not None and dtype_orig in dtype:
            dtype = None
        else:
            dtype = dtype[0]
    if force_all_finite not in (True, False, 'allow-nan'):
        raise ValueError('force_all_finite should be a bool or "allow-nan". Got {!r} instead'.format(force_all_finite))
    if estimator is not None:
        if isinstance(estimator, str):
            estimator_name = estimator
        else:
            estimator_name = estimator.__class__.__name__
    else:
        estimator_name = 'Estimator'
    context = ' by %s' % estimator_name if estimator is not None else ''
    if sp.issparse(array):
        _ensure_no_complex_data(array)
        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse)
    else:
        with warnings.catch_warnings():
            try:
                warnings.simplefilter('error', ComplexWarning)
                array = np.asarray(array, dtype=dtype, order=order)
            except ComplexWarning:
                raise ValueError('Complex data not supported\n{}\n'.format(array))
        _ensure_no_complex_data(array)
        if ensure_2d:
            if array.ndim == 0:
                raise ValueError('Expected 2D array, got scalar array instead:\narray={}.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))
            if array.ndim == 1:
                raise ValueError('Expected 2D array, got 1D array instead:\narray={}.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))
        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):
            warnings.warn("Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).", FutureWarning)
        if dtype_numeric and array.dtype.kind == 'O':
            array = array.astype(np.float64)
        if not allow_nd and array.ndim >= 3:
            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))
        if force_all_finite:
            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')
    if ensure_min_samples > 0:
        n_samples = _num_samples(array)
        if n_samples < ensure_min_samples:
            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))
    if ensure_min_features > 0 and array.ndim == 2:
        n_features = array.shape[1]
        if n_features < ensure_min_features:
            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))
    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):
        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)
        warnings.warn(msg, DataConversionWarning)
    if copy and np.may_share_memory(array, array_orig):
        array = np.array(array, dtype=dtype, order=order)
    if warn_on_dtype and dtypes_orig is not None and ({array.dtype} != set(dtypes_orig)):
        msg = 'Data with input dtype %s were all converted to %s%s.' % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype, context)
        warnings.warn(msg, DataConversionWarning, stacklevel=3)
    return array

.sklearn.utils.validation._ensure_no_complex_data

def _ensure_no_complex_data(array):
    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):
        raise ValueError('Complex data not supported\n{}\n'.format(array))

.sklearn.utils.validation._ensure_sparse_format

def _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse):
    if dtype is None:
        dtype = spmatrix.dtype
    changed_format = False
    if isinstance(accept_sparse, str):
        accept_sparse = [accept_sparse]
    _check_large_sparse(spmatrix, accept_large_sparse)
    if accept_sparse is False:
        raise TypeError('A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.')
    elif isinstance(accept_sparse, (list, tuple)):
        if len(accept_sparse) == 0:
            raise ValueError("When providing 'accept_sparse' as a tuple or list, it must contain at least one string value.")
        if spmatrix.format not in accept_sparse:
            spmatrix = spmatrix.asformat(accept_sparse[0])
            changed_format = True
    elif accept_sparse is not True:
        raise ValueError("Parameter 'accept_sparse' should be a string, boolean or list of strings. You provided 'accept_sparse={}'.".format(accept_sparse))
    if dtype != spmatrix.dtype:
        spmatrix = spmatrix.astype(dtype)
    elif copy and (not changed_format):
        spmatrix = spmatrix.copy()
    if force_all_finite:
        if not hasattr(spmatrix, 'data'):
            warnings.warn("Can't check %s sparse matrix for nan or inf." % spmatrix.format)
        else:
            _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == 'allow-nan')
    return spmatrix

.sklearn.utils.validation._check_large_sparse

def _check_large_sparse(X, accept_large_sparse=False):
    if not accept_large_sparse:
        supported_indices = ['int32']
        if X.getformat() == 'coo':
            index_keys = ['col', 'row']
        elif X.getformat() in ['csr', 'csc', 'bsr']:
            index_keys = ['indices', 'indptr']
        else:
            return
        for key in index_keys:
            indices_datatype = getattr(X, key).dtype
            if indices_datatype not in supported_indices:
                raise ValueError('Only sparse matrices with 32-bit integer indices are accepted. Got %s indices.' % indices_datatype)

.sklearn.utils.validation._assert_all_finite

def _assert_all_finite(X, allow_nan=False):
    from .extmath import _safe_accumulator_op
    if _get_config()['assume_finite']:
        return
    X = np.asanyarray(X)
    is_float = X.dtype.kind in 'fc'
    if is_float and np.isfinite(_safe_accumulator_op(np.sum, X)):
        pass
    elif is_float:
        msg_err = 'Input contains {} or a value too large for {!r}.'
        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):
            type_err = 'infinity' if allow_nan else 'NaN, infinity'
            raise ValueError(msg_err.format(type_err, X.dtype))

.sklearn._config.get_config

def get_config():
    return _global_config.copy()

.sklearn.utils.multiclass._unique_indicator

def _unique_indicator(y):
    return np.arange(check_array(y, ['csr', 'csc', 'coo']).shape[1])

.sklearn.utils.sparsefuncs.count_nonzero

def count_nonzero(X, axis=None, sample_weight=None):
    if axis == -1:
        axis = 1
    elif axis == -2:
        axis = 0
    elif X.format != 'csr':
        raise TypeError('Expected CSR sparse format, got {0}'.format(X.format))
    if axis is None:
        if sample_weight is None:
            return X.nnz
        else:
            return np.dot(np.diff(X.indptr), sample_weight)
    elif axis == 1:
        out = np.diff(X.indptr)
        if sample_weight is None:
            return out.astype('intp')
        return out * sample_weight
    elif axis == 0:
        if sample_weight is None:
            return np.bincount(X.indices, minlength=X.shape[1])
        else:
            weights = np.repeat(sample_weight, np.diff(X.indptr))
            return np.bincount(X.indices, minlength=X.shape[1], weights=weights)
    else:
        raise ValueError('Unsupported axis: {0}'.format(axis))


[/PYTHON]
What will be the output of `precision_score`, given the following input:
[INPUT]
```
{
    "self": {},
    "args": {
        "y_true": null,
        "y_pred": null
    },
    "kwargs": {}
}
```
[/INPUT]

[STRUCTURE]
```
{
    "output": XXX        
}
```
[/STRUCTURE]

[OUTPUT]
