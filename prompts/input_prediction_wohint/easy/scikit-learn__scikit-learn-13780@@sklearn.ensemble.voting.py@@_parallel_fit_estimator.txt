You are given a Python function (between [PYTHON] and [/PYTHON]). For this function, I want you to take the provided output (between [OUTPUT] and [/OUTPUT]) and predict the input of the function. Both input and output are presented in a JSON format. You only need to predict input variable values, and print input between [INPUT] and [/INPUT]. For prediction, simulate the execution of the program step by step and print your reasoning process before arriving at an answer between [THOUGHT] and [/THOUGHT]. 
[EXAMPLE]
[PYTHON]
class TempPathFactory(object):
    _given_basetemp = attr.ib(
        converter=attr.converters.optional(
            lambda p: Path(os.path.abspath(six.text_type(p)))
        )
    )
    _trace = attr.ib()
    _basetemp = attr.ib(default=None)

    def mktemp(self, basename, numbered=True):
        if not numbered:
            p = self.getbasetemp().joinpath(basename)
            p.mkdir()
        else:
            p = make_numbered_dir(root=self.getbasetemp(), prefix=basename)
            self._trace("mktemp", p)
        return p

    def getbasetemp(self):
        if self._given_basetemp is not None:
            basetemp = self._given_basetemp
            ensure_reset_dir(basetemp)
            basetemp = basetemp.resolve()
        else:
            from_env = os.environ.get("PYTEST_DEBUG_TEMPROOT")
            temproot = Path(from_env or tempfile.gettempdir()).resolve()
            user = get_user() or "unknown"
            rootdir = temproot.joinpath("pytest-of-{}".format(user))
            rootdir.mkdir(exist_ok=True)
            basetemp = make_numbered_dir_with_cleanup(
                prefix="pytest-", root=rootdir, keep=3, lock_timeout=LOCK_TIMEOUT
            )
        assert basetemp is not None, basetemp
        self._basetemp = t = basetemp
        self._trace("new basetemp", t)
        return t
[/PYTHON]

Functions called during the execution:
[PYTHON]
def parse_num(maybe_num):
    try:
        return int(maybe_num)
    except ValueError:
        return -1

def extract_suffixes(iter, prefix):
    p_len = len(prefix)
    for p in iter:
        yield p.name[p_len:]


def find_suffixes(root, prefix):
    return extract_suffixes(find_prefixed(root, prefix), prefix)

def make_numbered_dir(root, prefix):
    for i in range(10):
        # try up to 10 times to create the folder
        max_existing = _max(map(parse_num, find_suffixes(root, prefix)), default=-1)
        new_number = max_existing + 1
        new_path = root.joinpath("{}{}".format(prefix, new_number))
        try:
            new_path.mkdir()
        except Exception:
            pass
        else:
            _force_symlink(root, prefix + "current", new_path)
            return new_path
    else:
        raise EnvironmentError(
            "could not create numbered dir with prefix "
            "{prefix} in {root} after 10 tries".format(prefix=prefix, root=root)
        )
[/PYTHON]
What will be the input of `mktemp`, given the following output:
[OUTPUT]
```
{
    "strpath": "/tmp/pytest-of-root/pytest-0/test_mktemp0/world0"
}
```
[/OUTPUT]

[THOUGHT]
1. To get ".../world0", we need:
basename == "world", and self.getbasetemp() to be exactly "/tmp/pytest-of-root/pytest-0/test_mktemp0".
2. Make `getbasetemp()` return that exact base directory.`getbasetemp()` returns `_given_basetemp` if it is not None. So I set `_given_basetemp` to "/tmp/pytest-of-root/pytest-0/test_mktemp0" and _basetemp to None.
3. Ensure the numbered suffix is 0.
`make_numbered_dir` scans existing children with the given prefix and picks max_existing + 1. If none exist, it creates <prefix>0. To align with world0, we assume thereâ€™s no existing world* folder under the base; thus the first created is world0.
[/THOUGHT]

[INPUT]
```
{
    "self": {
        "_tmppath_factory": {
            "_given_basetemp": "/tmp/pytest-of-root/pytest-0/test_mktemp0",
            "_trace": {},
            "_basetemp": null
        }
    },
    "args": {
        "basename": "world"
    },
    "kwargs": {}
}
```
[/INPUT]
[/EXAMPLE]
[PYTHON]
import numpy as np
from abc import abstractmethod
from ..base import ClassifierMixin
from ..base import RegressorMixin
from ..base import TransformerMixin
from ..base import clone
from ..preprocessing import LabelEncoder
from ..utils._joblib import Parallel, delayed
from ..utils.validation import has_fit_parameter, check_is_fitted
from ..utils.metaestimators import _BaseComposition
from ..utils import Bunch

def _parallel_fit_estimator(estimator, X, y, sample_weight=None):
    if sample_weight is not None:
        try:
            estimator.fit(X, y, sample_weight=sample_weight)
        except TypeError as exc:
            if "unexpected keyword argument 'sample_weight'" in str(exc):
                raise ValueError('Underlying estimator {} does not support sample weights.'.format(estimator.__class__.__name__)) from exc
            raise
    else:
        estimator.fit(X, y)
    return estimator
[/PYTHON]

Functions called during the execution:
[PYTHON]
.sklearn.linear_model.logistic.LogisticRegression.fit

def fit(self, X, y, sample_weight=None):
    solver = _check_solver(self.solver, self.penalty, self.dual)
    if not isinstance(self.C, numbers.Number) or self.C < 0:
        raise ValueError('Penalty term must be positive; got (C=%r)' % self.C)
    if self.penalty == 'elasticnet':
        if not isinstance(self.l1_ratio, numbers.Number) or self.l1_ratio < 0 or self.l1_ratio > 1:
            raise ValueError('l1_ratio must be between 0 and 1; got (l1_ratio=%r)' % self.l1_ratio)
    elif self.l1_ratio is not None:
        warnings.warn("l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty={})".format(self.penalty))
    if self.penalty == 'none':
        if self.C != 1.0:
            warnings.warn("Setting penalty='none' will ignore the C and l1_ratio parameters")
        C_ = np.inf
        penalty = 'l2'
    else:
        C_ = self.C
        penalty = self.penalty
    if not isinstance(self.max_iter, numbers.Number) or self.max_iter < 0:
        raise ValueError('Maximum number of iteration must be positive; got (max_iter=%r)' % self.max_iter)
    if not isinstance(self.tol, numbers.Number) or self.tol < 0:
        raise ValueError('Tolerance for stopping criteria must be positive; got (tol=%r)' % self.tol)
    if solver in ['lbfgs', 'liblinear']:
        _dtype = np.float64
    else:
        _dtype = [np.float64, np.float32]
    X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order='C', accept_large_sparse=solver != 'liblinear')
    check_classification_targets(y)
    self.classes_ = np.unique(y)
    n_samples, n_features = X.shape
    multi_class = _check_multi_class(self.multi_class, solver, len(self.classes_))
    if solver == 'liblinear':
        if effective_n_jobs(self.n_jobs) != 1:
            warnings.warn("'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = {}.".format(effective_n_jobs(self.n_jobs)))
        self.coef_, self.intercept_, n_iter_ = _fit_liblinear(X, y, self.C, self.fit_intercept, self.intercept_scaling, self.class_weight, self.penalty, self.dual, self.verbose, self.max_iter, self.tol, self.random_state, sample_weight=sample_weight)
        self.n_iter_ = np.array([n_iter_])
        return self
    if solver in ['sag', 'saga']:
        max_squared_sum = row_norms(X, squared=True).max()
    else:
        max_squared_sum = None
    n_classes = len(self.classes_)
    classes_ = self.classes_
    if n_classes < 2:
        raise ValueError('This solver needs samples of at least 2 classes in the data, but the data contains only one class: %r' % classes_[0])
    if len(self.classes_) == 2:
        n_classes = 1
        classes_ = classes_[1:]
    if self.warm_start:
        warm_start_coef = getattr(self, 'coef_', None)
    else:
        warm_start_coef = None
    if warm_start_coef is not None and self.fit_intercept:
        warm_start_coef = np.append(warm_start_coef, self.intercept_[:, np.newaxis], axis=1)
    self.coef_ = list()
    self.intercept_ = np.zeros(n_classes)
    if multi_class == 'multinomial':
        classes_ = [None]
        warm_start_coef = [warm_start_coef]
    if warm_start_coef is None:
        warm_start_coef = [None] * n_classes
    path_func = delayed(_logistic_regression_path)
    if solver in ['sag', 'saga']:
        prefer = 'threads'
    else:
        prefer = 'processes'
    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, **_joblib_parallel_args(prefer=prefer))((path_func(X, y, pos_class=class_, Cs=[C_], l1_ratio=self.l1_ratio, fit_intercept=self.fit_intercept, tol=self.tol, verbose=self.verbose, solver=solver, multi_class=multi_class, max_iter=self.max_iter, class_weight=self.class_weight, check_input=False, random_state=self.random_state, coef=warm_start_coef_, penalty=penalty, max_squared_sum=max_squared_sum, sample_weight=sample_weight) for class_, warm_start_coef_ in zip(classes_, warm_start_coef)))
    fold_coefs_, _, n_iter_ = zip(*fold_coefs_)
    self.n_iter_ = np.asarray(n_iter_, dtype=np.int32)[:, 0]
    if multi_class == 'multinomial':
        self.coef_ = fold_coefs_[0][0]
    else:
        self.coef_ = np.asarray(fold_coefs_)
        self.coef_ = self.coef_.reshape(n_classes, n_features + int(self.fit_intercept))
    if self.fit_intercept:
        self.intercept_ = self.coef_[:, -1]
        self.coef_ = self.coef_[:, :-1]
    return self

.sklearn.linear_model.logistic._check_solver

def _check_solver(solver, penalty, dual):
    if solver == 'warn':
        solver = 'liblinear'
        warnings.warn("Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.", FutureWarning)
    all_solvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']
    if solver not in all_solvers:
        raise ValueError('Logistic Regression supports only solvers in %s, got %s.' % (all_solvers, solver))
    all_penalties = ['l1', 'l2', 'elasticnet', 'none']
    if penalty not in all_penalties:
        raise ValueError('Logistic Regression supports only penalties in %s, got %s.' % (all_penalties, penalty))
    if solver not in ['liblinear', 'saga'] and penalty not in ('l2', 'none'):
        raise ValueError("Solver %s supports only 'l2' or 'none' penalties, got %s penalty." % (solver, penalty))
    if solver != 'liblinear' and dual:
        raise ValueError('Solver %s supports only dual=False, got dual=%s' % (solver, dual))
    if penalty == 'elasticnet' and solver != 'saga':
        raise ValueError("Only 'saga' solver supports elasticnet penalty, got solver={}.".format(solver))
    if solver == 'liblinear' and penalty == 'none':
        raise ValueError("penalty='none' is not supported for the liblinear solver")
    return solver

.sklearn.utils.validation.check_X_y

def check_X_y(X, y, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, multi_output=False, ensure_min_samples=1, ensure_min_features=1, y_numeric=False, warn_on_dtype=None, estimator=None):
    if y is None:
        raise ValueError('y cannot be None')
    X = check_array(X, accept_sparse=accept_sparse, accept_large_sparse=accept_large_sparse, dtype=dtype, order=order, copy=copy, force_all_finite=force_all_finite, ensure_2d=ensure_2d, allow_nd=allow_nd, ensure_min_samples=ensure_min_samples, ensure_min_features=ensure_min_features, warn_on_dtype=warn_on_dtype, estimator=estimator)
    if multi_output:
        y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False, dtype=None)
    else:
        y = column_or_1d(y, warn=True)
        _assert_all_finite(y)
    if y_numeric and y.dtype.kind == 'O':
        y = y.astype(np.float64)
    check_consistent_length(X, y)
    return (X, y)

.sklearn.utils.validation.check_array

def check_array(array, accept_sparse=False, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=None, estimator=None):
    if warn_on_dtype is not None:
        warnings.warn("'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.", DeprecationWarning)
    array_orig = array
    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'
    dtype_orig = getattr(array, 'dtype', None)
    if not hasattr(dtype_orig, 'kind'):
        dtype_orig = None
    dtypes_orig = None
    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):
        dtypes_orig = np.array(array.dtypes)
    if dtype_numeric:
        if dtype_orig is not None and dtype_orig.kind == 'O':
            dtype = np.float64
        else:
            dtype = None
    if isinstance(dtype, (list, tuple)):
        if dtype_orig is not None and dtype_orig in dtype:
            dtype = None
        else:
            dtype = dtype[0]
    if force_all_finite not in (True, False, 'allow-nan'):
        raise ValueError('force_all_finite should be a bool or "allow-nan". Got {!r} instead'.format(force_all_finite))
    if estimator is not None:
        if isinstance(estimator, str):
            estimator_name = estimator
        else:
            estimator_name = estimator.__class__.__name__
    else:
        estimator_name = 'Estimator'
    context = ' by %s' % estimator_name if estimator is not None else ''
    if sp.issparse(array):
        _ensure_no_complex_data(array)
        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse)
    else:
        with warnings.catch_warnings():
            try:
                warnings.simplefilter('error', ComplexWarning)
                array = np.asarray(array, dtype=dtype, order=order)
            except ComplexWarning:
                raise ValueError('Complex data not supported\n{}\n'.format(array))
        _ensure_no_complex_data(array)
        if ensure_2d:
            if array.ndim == 0:
                raise ValueError('Expected 2D array, got scalar array instead:\narray={}.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))
            if array.ndim == 1:
                raise ValueError('Expected 2D array, got 1D array instead:\narray={}.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))
        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):
            warnings.warn("Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).", FutureWarning)
        if dtype_numeric and array.dtype.kind == 'O':
            array = array.astype(np.float64)
        if not allow_nd and array.ndim >= 3:
            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))
        if force_all_finite:
            _assert_all_finite(array, allow_nan=force_all_finite == 'allow-nan')
    if ensure_min_samples > 0:
        n_samples = _num_samples(array)
        if n_samples < ensure_min_samples:
            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))
    if ensure_min_features > 0 and array.ndim == 2:
        n_features = array.shape[1]
        if n_features < ensure_min_features:
            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))
    if warn_on_dtype and dtype_orig is not None and (array.dtype != dtype_orig):
        msg = 'Data with input dtype %s was converted to %s%s.' % (dtype_orig, array.dtype, context)
        warnings.warn(msg, DataConversionWarning)
    if copy and np.may_share_memory(array, array_orig):
        array = np.array(array, dtype=dtype, order=order)
    if warn_on_dtype and dtypes_orig is not None and ({array.dtype} != set(dtypes_orig)):
        msg = 'Data with input dtype %s were all converted to %s%s.' % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype, context)
        warnings.warn(msg, DataConversionWarning, stacklevel=3)
    return array

.sklearn.utils.validation._ensure_no_complex_data

def _ensure_no_complex_data(array):
    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):
        raise ValueError('Complex data not supported\n{}\n'.format(array))

.sklearn.utils.validation._assert_all_finite

def _assert_all_finite(X, allow_nan=False):
    from .extmath import _safe_accumulator_op
    if _get_config()['assume_finite']:
        return
    X = np.asanyarray(X)
    is_float = X.dtype.kind in 'fc'
    if is_float and np.isfinite(_safe_accumulator_op(np.sum, X)):
        pass
    elif is_float:
        msg_err = 'Input contains {} or a value too large for {!r}.'
        if allow_nan and np.isinf(X).any() or (not allow_nan and (not np.isfinite(X).all())):
            type_err = 'infinity' if allow_nan else 'NaN, infinity'
            raise ValueError(msg_err.format(type_err, X.dtype))
    elif X.dtype == np.dtype('object') and (not allow_nan):
        if _object_dtype_isnan(X).any():
            raise ValueError('Input contains NaN')

.sklearn._config.get_config

def get_config():
    return _global_config.copy()

.sklearn.utils.extmath._safe_accumulator_op

def _safe_accumulator_op(op, x, *args, **kwargs):
    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:
        result = op(x, *args, **kwargs, dtype=np.float64)
    else:
        result = op(x, *args, **kwargs)
    return result

.sklearn.utils.validation._num_samples

def _num_samples(x):
    if hasattr(x, 'fit') and callable(x.fit):
        raise TypeError('Expected sequence or array-like, got estimator %s' % x)
    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):
        if hasattr(x, '__array__'):
            x = np.asarray(x)
        else:
            raise TypeError('Expected sequence or array-like, got %s' % type(x))
    if hasattr(x, 'shape'):
        if len(x.shape) == 0:
            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)
        if isinstance(x.shape[0], numbers.Integral):
            return x.shape[0]
        else:
            return len(x)
    else:
        return len(x)

.sklearn.utils.validation.column_or_1d

def column_or_1d(y, warn=False):
    shape = np.shape(y)
    if len(shape) == 1:
        return np.ravel(y)
    if len(shape) == 2 and shape[1] == 1:
        if warn:
            warnings.warn('A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().', DataConversionWarning, stacklevel=2)
        return np.ravel(y)
    raise ValueError('bad input shape {0}'.format(shape))

.sklearn.utils.validation.check_consistent_length

def check_consistent_length(*arrays):
    lengths = [_num_samples(X) for X in arrays if X is not None]
    uniques = np.unique(lengths)
    if len(uniques) > 1:
        raise ValueError('Found input variables with inconsistent numbers of samples: %r' % [int(l) for l in lengths])

.sklearn.utils.multiclass.check_classification_targets

def check_classification_targets(y):
    y_type = type_of_target(y)
    if y_type not in ['binary', 'multiclass', 'multiclass-multioutput', 'multilabel-indicator', 'multilabel-sequences']:
        raise ValueError('Unknown label type: %r' % y_type)

.sklearn.utils.multiclass.type_of_target

def type_of_target(y):
    valid = (isinstance(y, (Sequence, spmatrix)) or hasattr(y, '__array__')) and (not isinstance(y, str))
    if not valid:
        raise ValueError('Expected array-like (array or non-string sequence), got %r' % y)
    sparseseries = y.__class__.__name__ == 'SparseSeries'
    if sparseseries:
        raise ValueError("y cannot be class 'SparseSeries'.")
    if is_multilabel(y):
        return 'multilabel-indicator'
    try:
        y = np.asarray(y)
    except ValueError:
        return 'unknown'
    try:
        if not hasattr(y[0], '__array__') and isinstance(y[0], Sequence) and (not isinstance(y[0], str)):
            raise ValueError('You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format.')
    except IndexError:
        pass
    if y.ndim > 2 or (y.dtype == object and len(y) and (not isinstance(y.flat[0], str))):
        return 'unknown'
    if y.ndim == 2 and y.shape[1] == 0:
        return 'unknown'
    if y.ndim == 2 and y.shape[1] > 1:
        suffix = '-multioutput'
    else:
        suffix = ''
    if y.dtype.kind == 'f' and np.any(y != y.astype(int)):
        _assert_all_finite(y)
        return 'continuous' + suffix
    if len(np.unique(y)) > 2 or (y.ndim >= 2 and len(y[0]) > 1):
        return 'multiclass' + suffix
    else:
        return 'binary'

.sklearn.utils.multiclass.is_multilabel

def is_multilabel(y):
    if hasattr(y, '__array__'):
        y = np.asarray(y)
    if not (hasattr(y, 'shape') and y.ndim == 2 and (y.shape[1] > 1)):
        return False
    if issparse(y):
        if isinstance(y, (dok_matrix, lil_matrix)):
            y = y.tocsr()
        return len(y.data) == 0 or (np.unique(y.data).size == 1 and (y.dtype.kind in 'biu' or _is_integral_float(np.unique(y.data))))
    else:
        labels = np.unique(y)
        return len(labels) < 3 and (y.dtype.kind in 'biu' or _is_integral_float(labels))

.sklearn.linear_model.logistic._check_multi_class

def _check_multi_class(multi_class, solver, n_classes):
    if multi_class == 'warn':
        multi_class = 'ovr'
        if n_classes > 2:
            warnings.warn("Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.", FutureWarning)
    if multi_class == 'auto':
        if solver == 'liblinear':
            multi_class = 'ovr'
        elif n_classes > 2:
            multi_class = 'multinomial'
        else:
            multi_class = 'ovr'
    if multi_class not in ('multinomial', 'ovr'):
        raise ValueError("multi_class should be 'multinomial', 'ovr' or 'auto'. Got %s." % multi_class)
    if multi_class == 'multinomial' and solver == 'liblinear':
        raise ValueError('Solver %s does not support a multinomial backend.' % solver)
    return multi_class

.sklearn.svm.base._fit_liblinear

def _fit_liblinear(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state=None, multi_class='ovr', loss='logistic_regression', epsilon=0.1, sample_weight=None):
    if loss not in ['epsilon_insensitive', 'squared_epsilon_insensitive']:
        enc = LabelEncoder()
        y_ind = enc.fit_transform(y)
        classes_ = enc.classes_
        if len(classes_) < 2:
            raise ValueError('This solver needs samples of at least 2 classes in the data, but the data contains only one class: %r' % classes_[0])
        class_weight_ = compute_class_weight(class_weight, classes_, y)
    else:
        class_weight_ = np.empty(0, dtype=np.float64)
        y_ind = y
    liblinear.set_verbosity_wrap(verbose)
    rnd = check_random_state(random_state)
    if verbose:
        print('[LibLinear]', end='')
    bias = -1.0
    if fit_intercept:
        if intercept_scaling <= 0:
            raise ValueError('Intercept scaling is %r but needs to be greater than 0. To disable fitting an intercept, set fit_intercept=False.' % intercept_scaling)
        else:
            bias = intercept_scaling
    libsvm.set_verbosity_wrap(verbose)
    libsvm_sparse.set_verbosity_wrap(verbose)
    liblinear.set_verbosity_wrap(verbose)
    if sp.issparse(X):
        _check_large_sparse(X)
    y_ind = np.asarray(y_ind, dtype=np.float64).ravel()
    y_ind = np.require(y_ind, requirements='W')
    if sample_weight is None:
        sample_weight = np.ones(X.shape[0])
    else:
        sample_weight = np.array(sample_weight, dtype=np.float64, order='C')
        check_consistent_length(sample_weight, X)
    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)
    raw_coef_, n_iter_ = liblinear.train_wrap(X, y_ind, sp.isspmatrix(X), solver_type, tol, bias, C, class_weight_, max_iter, rnd.randint(np.iinfo('i').max), epsilon, sample_weight)
    n_iter_ = max(n_iter_)
    if n_iter_ >= max_iter:
        warnings.warn('Liblinear failed to converge, increase the number of iterations.', ConvergenceWarning)
    if fit_intercept:
        coef_ = raw_coef_[:, :-1]
        intercept_ = intercept_scaling * raw_coef_[:, -1]
    else:
        coef_ = raw_coef_
        intercept_ = 0.0
    return (coef_, intercept_, n_iter_)

.sklearn.preprocessing.label.LabelEncoder.fit_transform

def fit_transform(self, y):
    y = column_or_1d(y, warn=True)
    self.classes_, y = _encode(y, encode=True)
    return y

.sklearn.preprocessing.label._encode

def _encode(values, uniques=None, encode=False):
    if values.dtype == object:
        try:
            res = _encode_python(values, uniques, encode)
        except TypeError:
            raise TypeError('argument must be a string or number')
        return res
    else:
        return _encode_numpy(values, uniques, encode)

.sklearn.preprocessing.label._encode_numpy

def _encode_numpy(values, uniques=None, encode=False):
    if uniques is None:
        if encode:
            uniques, encoded = np.unique(values, return_inverse=True)
            return (uniques, encoded)
        else:
            return np.unique(values)
    if encode:
        diff = _encode_check_unknown(values, uniques)
        if diff:
            raise ValueError('y contains previously unseen labels: %s' % str(diff))
        encoded = np.searchsorted(uniques, values)
        return (uniques, encoded)
    else:
        return uniques

.sklearn.utils.class_weight.compute_class_weight

def compute_class_weight(class_weight, classes, y):
    from ..preprocessing import LabelEncoder
    if set(y) - set(classes):
        raise ValueError('classes should include all valid labels that can be in y')
    if class_weight is None or len(class_weight) == 0:
        weight = np.ones(classes.shape[0], dtype=np.float64, order='C')
    elif class_weight == 'balanced':
        le = LabelEncoder()
        y_ind = le.fit_transform(y)
        if not all(np.in1d(classes, le.classes_)):
            raise ValueError('classes should have valid labels that are in y')
        recip_freq = len(y) / (len(le.classes_) * np.bincount(y_ind).astype(np.float64))
        weight = recip_freq[le.transform(classes)]
    else:
        weight = np.ones(classes.shape[0], dtype=np.float64, order='C')
        if not isinstance(class_weight, dict):
            raise ValueError("class_weight must be dict, 'balanced', or None, got: %r" % class_weight)
        for c in class_weight:
            i = np.searchsorted(classes, c)
            if i >= len(classes) or classes[i] != c:
                raise ValueError('Class label {} not present.'.format(c))
            else:
                weight[i] = class_weight[c]
    return weight

.sklearn.utils.validation.check_random_state

def check_random_state(seed):
    if seed is None or seed is np.random:
        return np.random.mtrand._rand
    if isinstance(seed, (numbers.Integral, np.integer)):
        return np.random.RandomState(seed)
    if isinstance(seed, np.random.RandomState):
        return seed
    raise ValueError('%r cannot be used to seed a numpy.random.RandomState instance' % seed)

.sklearn.svm.base._get_liblinear_solver_type

def _get_liblinear_solver_type(multi_class, penalty, loss, dual):
    _solver_type_dict = {'logistic_regression': {'l1': {False: 6}, 'l2': {False: 0, True: 7}}, 'hinge': {'l2': {True: 3}}, 'squared_hinge': {'l1': {False: 5}, 'l2': {False: 2, True: 1}}, 'epsilon_insensitive': {'l2': {True: 13}}, 'squared_epsilon_insensitive': {'l2': {False: 11, True: 12}}, 'crammer_singer': 4}
    if multi_class == 'crammer_singer':
        return _solver_type_dict[multi_class]
    elif multi_class != 'ovr':
        raise ValueError('`multi_class` must be one of `ovr`, `crammer_singer`, got %r' % multi_class)
    _solver_pen = _solver_type_dict.get(loss, None)
    if _solver_pen is None:
        error_string = "loss='%s' is not supported" % loss
    else:
        _solver_dual = _solver_pen.get(penalty, None)
        if _solver_dual is None:
            error_string = "The combination of penalty='%s' and loss='%s' is not supported" % (penalty, loss)
        else:
            solver_num = _solver_dual.get(dual, None)
            if solver_num is None:
                error_string = "The combination of penalty='%s' and loss='%s' are not supported when dual=%s" % (penalty, loss, dual)
            else:
                return solver_num
    raise ValueError('Unsupported set of arguments: %s, Parameters: penalty=%r, loss=%r, dual=%r' % (error_string, penalty, loss, dual))

.sklearn.ensemble.forest.BaseForest.fit

def fit(self, X, y, sample_weight=None):
    if self.n_estimators == 'warn':
        warn('The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.', FutureWarning)
        self.n_estimators = 10
    X = check_array(X, accept_sparse='csc', dtype=DTYPE)
    y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
    if sample_weight is not None:
        sample_weight = check_array(sample_weight, ensure_2d=False)
    if issparse(X):
        X.sort_indices()
    self.n_features_ = X.shape[1]
    y = np.atleast_1d(y)
    if y.ndim == 2 and y.shape[1] == 1:
        warn('A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().', DataConversionWarning, stacklevel=2)
    if y.ndim == 1:
        y = np.reshape(y, (-1, 1))
    self.n_outputs_ = y.shape[1]
    y, expanded_class_weight = self._validate_y_class_weight(y)
    if getattr(y, 'dtype', None) != DOUBLE or not y.flags.contiguous:
        y = np.ascontiguousarray(y, dtype=DOUBLE)
    if expanded_class_weight is not None:
        if sample_weight is not None:
            sample_weight = sample_weight * expanded_class_weight
        else:
            sample_weight = expanded_class_weight
    self._validate_estimator()
    if not self.bootstrap and self.oob_score:
        raise ValueError('Out of bag estimation only available if bootstrap=True')
    random_state = check_random_state(self.random_state)
    if not self.warm_start or not hasattr(self, 'estimators_'):
        self.estimators_ = []
    n_more_estimators = self.n_estimators - len(self.estimators_)
    if n_more_estimators < 0:
        raise ValueError('n_estimators=%d must be larger or equal to len(estimators_)=%d when warm_start==True' % (self.n_estimators, len(self.estimators_)))
    elif n_more_estimators == 0:
        warn('Warm-start fitting without increasing n_estimators does not fit new trees.')
    else:
        if self.warm_start and len(self.estimators_) > 0:
            random_state.randint(MAX_INT, size=len(self.estimators_))
        trees = [self._make_estimator(append=False, random_state=random_state) for i in range(n_more_estimators)]
        trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, **_joblib_parallel_args(prefer='threads'))((delayed(_parallel_build_trees)(t, self, X, y, sample_weight, i, len(trees), verbose=self.verbose, class_weight=self.class_weight) for i, t in enumerate(trees)))
        self.estimators_.extend(trees)
    if self.oob_score:
        self._set_oob_score(X, y)
    if hasattr(self, 'classes_') and self.n_outputs_ == 1:
        self.n_classes_ = self.n_classes_[0]
        self.classes_ = self.classes_[0]
    return self

.sklearn.ensemble.forest.ForestClassifier._validate_y_class_weight

def _validate_y_class_weight(self, y):
    check_classification_targets(y)
    y = np.copy(y)
    expanded_class_weight = None
    if self.class_weight is not None:
        y_original = np.copy(y)
    self.classes_ = []
    self.n_classes_ = []
    y_store_unique_indices = np.zeros(y.shape, dtype=np.int)
    for k in range(self.n_outputs_):
        classes_k, y_store_unique_indices[:, k] = np.unique(y[:, k], return_inverse=True)
        self.classes_.append(classes_k)
        self.n_classes_.append(classes_k.shape[0])
    y = y_store_unique_indices
    if self.class_weight is not None:
        valid_presets = ('balanced', 'balanced_subsample')
        if isinstance(self.class_weight, str):
            if self.class_weight not in valid_presets:
                raise ValueError('Valid presets for class_weight include "balanced" and "balanced_subsample". Given "%s".' % self.class_weight)
            if self.warm_start:
                warn('class_weight presets "balanced" or "balanced_subsample" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use "balanced" weights, use compute_class_weight("balanced", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.')
        if self.class_weight != 'balanced_subsample' or not self.bootstrap:
            if self.class_weight == 'balanced_subsample':
                class_weight = 'balanced'
            else:
                class_weight = self.class_weight
            expanded_class_weight = compute_sample_weight(class_weight, y_original)
    return (y, expanded_class_weight)

.sklearn.ensemble.base.BaseEnsemble._validate_estimator

def _validate_estimator(self, default=None):
    if not isinstance(self.n_estimators, (numbers.Integral, np.integer)):
        raise ValueError('n_estimators must be an integer, got {0}.'.format(type(self.n_estimators)))
    if self.n_estimators <= 0:
        raise ValueError('n_estimators must be greater than zero, got {0}.'.format(self.n_estimators))
    if self.base_estimator is not None:
        self.base_estimator_ = self.base_estimator
    else:
        self.base_estimator_ = default
    if self.base_estimator_ is None:
        raise ValueError('base_estimator cannot be None')

.sklearn.ensemble.base.BaseEnsemble._make_estimator

def _make_estimator(self, append=True, random_state=None):
    estimator = clone(self.base_estimator_)
    estimator.set_params(**{p: getattr(self, p) for p in self.estimator_params})
    if random_state is not None:
        _set_random_states(estimator, random_state)
    if append:
        self.estimators_.append(estimator)
    return estimator

.sklearn.base.clone

def clone(estimator, safe=True):
    estimator_type = type(estimator)
    if estimator_type in (list, tuple, set, frozenset):
        return estimator_type([clone(e, safe=safe) for e in estimator])
    elif not hasattr(estimator, 'get_params') or isinstance(estimator, type):
        if not safe:
            return copy.deepcopy(estimator)
        else:
            raise TypeError("Cannot clone object '%s' (type %s): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods." % (repr(estimator), type(estimator)))
    klass = estimator.__class__
    new_object_params = estimator.get_params(deep=False)
    for name, param in new_object_params.items():
        new_object_params[name] = clone(param, safe=False)
    new_object = klass(**new_object_params)
    params_set = new_object.get_params(deep=False)
    for name in new_object_params:
        param1 = new_object_params[name]
        param2 = params_set[name]
        if param1 is not param2:
            raise RuntimeError('Cannot clone object %s, as the constructor either does not set or modifies parameter %s' % (estimator, name))
    return new_object

.sklearn.base.BaseEstimator.get_params

def get_params(self, deep=True):
    out = dict()
    for key in self._get_param_names():
        value = getattr(self, key, None)
        if deep and hasattr(value, 'get_params'):
            deep_items = value.get_params().items()
            out.update(((key + '__' + k, val) for k, val in deep_items))
        out[key] = value
    return out

.sklearn.base.BaseEstimator._get_param_names

def _get_param_names(cls):
    init = getattr(cls.__init__, 'deprecated_original', cls.__init__)
    if init is object.__init__:
        return []
    init_signature = inspect.signature(init)
    parameters = [p for p in init_signature.parameters.values() if p.name != 'self' and p.kind != p.VAR_KEYWORD]
    for p in parameters:
        if p.kind == p.VAR_POSITIONAL:
            raise RuntimeError("scikit-learn estimators should always specify their parameters in the signature of their __init__ (no varargs). %s with constructor %s doesn't  follow this convention." % (cls, init_signature))
    return sorted([p.name for p in parameters])

.sklearn.tree.tree.DecisionTreeClassifier.__init__

def __init__(self, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False):
    super().__init__(criterion=criterion, splitter=splitter, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, min_weight_fraction_leaf=min_weight_fraction_leaf, max_features=max_features, max_leaf_nodes=max_leaf_nodes, class_weight=class_weight, random_state=random_state, min_impurity_decrease=min_impurity_decrease, min_impurity_split=min_impurity_split, presort=presort)

.sklearn.tree.tree.BaseDecisionTree.__init__

def __init__(self, criterion, splitter, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, random_state, min_impurity_decrease, min_impurity_split, class_weight=None, presort=False):
    self.criterion = criterion
    self.splitter = splitter
    self.max_depth = max_depth
    self.min_samples_split = min_samples_split
    self.min_samples_leaf = min_samples_leaf
    self.min_weight_fraction_leaf = min_weight_fraction_leaf
    self.max_features = max_features
    self.random_state = random_state
    self.max_leaf_nodes = max_leaf_nodes
    self.min_impurity_decrease = min_impurity_decrease
    self.min_impurity_split = min_impurity_split
    self.class_weight = class_weight
    self.presort = presort

.sklearn.base.BaseEstimator.set_params

def set_params(self, **params):
    if not params:
        return self
    valid_params = self.get_params(deep=True)
    nested_params = defaultdict(dict)
    for key, value in params.items():
        key, delim, sub_key = key.partition('__')
        if key not in valid_params:
            raise ValueError('Invalid parameter %s for estimator %s. Check the list of available parameters with `estimator.get_params().keys()`.' % (key, self))
        if delim:
            nested_params[key][sub_key] = value
        else:
            setattr(self, key, value)
            valid_params[key] = value
    for key, sub_params in nested_params.items():
        valid_params[key].set_params(**sub_params)
    return self

.sklearn.ensemble.base._set_random_states

def _set_random_states(estimator, random_state=None):
    random_state = check_random_state(random_state)
    to_set = {}
    for key in sorted(estimator.get_params(deep=True)):
        if key == 'random_state' or key.endswith('__random_state'):
            to_set[key] = random_state.randint(MAX_RAND_SEED)
    if to_set:
        estimator.set_params(**to_set)

.sklearn.utils.fixes._joblib_parallel_args

def _joblib_parallel_args(**kwargs):
    from . import _joblib
    if _joblib.__version__ >= LooseVersion('0.12'):
        return kwargs
    extra_args = set(kwargs.keys()).difference({'prefer', 'require'})
    if extra_args:
        raise NotImplementedError('unhandled arguments %s with joblib %s' % (list(extra_args), _joblib.__version__))
    args = {}
    if 'prefer' in kwargs:
        prefer = kwargs['prefer']
        if prefer not in ['threads', 'processes', None]:
            raise ValueError('prefer=%s is not supported' % prefer)
        args['backend'] = {'threads': 'threading', 'processes': 'multiprocessing', None: None}[prefer]
    if 'require' in kwargs:
        require = kwargs['require']
        if require not in [None, 'sharedmem']:
            raise ValueError('require=%s is not supported' % require)
        if require == 'sharedmem':
            args['backend'] = 'threading'
    return args

.sklearn.ensemble.forest._parallel_build_trees

def _parallel_build_trees(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose=0, class_weight=None):
    if verbose > 1:
        print('building tree %d of %d' % (tree_idx + 1, n_trees))
    if forest.bootstrap:
        n_samples = X.shape[0]
        if sample_weight is None:
            curr_sample_weight = np.ones((n_samples,), dtype=np.float64)
        else:
            curr_sample_weight = sample_weight.copy()
        indices = _generate_sample_indices(tree.random_state, n_samples)
        sample_counts = np.bincount(indices, minlength=n_samples)
        curr_sample_weight *= sample_counts
        if class_weight == 'subsample':
            with catch_warnings():
                simplefilter('ignore', DeprecationWarning)
                curr_sample_weight *= compute_sample_weight('auto', y, indices)
        elif class_weight == 'balanced_subsample':
            curr_sample_weight *= compute_sample_weight('balanced', y, indices)
        tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)
    else:
        tree.fit(X, y, sample_weight=sample_weight, check_input=False)
    return tree

.sklearn.ensemble.forest._generate_sample_indices

def _generate_sample_indices(random_state, n_samples):
    random_instance = check_random_state(random_state)
    sample_indices = random_instance.randint(0, n_samples, n_samples)
    return sample_indices

.sklearn.tree.tree.DecisionTreeClassifier.fit

def fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted=None):
    super().fit(X, y, sample_weight=sample_weight, check_input=check_input, X_idx_sorted=X_idx_sorted)
    return self

.sklearn.tree.tree.BaseDecisionTree.fit

def fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted=None):
    random_state = check_random_state(self.random_state)
    if check_input:
        X = check_array(X, dtype=DTYPE, accept_sparse='csc')
        y = check_array(y, ensure_2d=False, dtype=None)
        if issparse(X):
            X.sort_indices()
            if X.indices.dtype != np.intc or X.indptr.dtype != np.intc:
                raise ValueError('No support for np.int64 index based sparse matrices')
    n_samples, self.n_features_ = X.shape
    is_classification = is_classifier(self)
    y = np.atleast_1d(y)
    expanded_class_weight = None
    if y.ndim == 1:
        y = np.reshape(y, (-1, 1))
    self.n_outputs_ = y.shape[1]
    if is_classification:
        check_classification_targets(y)
        y = np.copy(y)
        self.classes_ = []
        self.n_classes_ = []
        if self.class_weight is not None:
            y_original = np.copy(y)
        y_encoded = np.zeros(y.shape, dtype=np.int)
        for k in range(self.n_outputs_):
            classes_k, y_encoded[:, k] = np.unique(y[:, k], return_inverse=True)
            self.classes_.append(classes_k)
            self.n_classes_.append(classes_k.shape[0])
        y = y_encoded
        if self.class_weight is not None:
            expanded_class_weight = compute_sample_weight(self.class_weight, y_original)
    else:
        self.classes_ = [None] * self.n_outputs_
        self.n_classes_ = [1] * self.n_outputs_
    self.n_classes_ = np.array(self.n_classes_, dtype=np.intp)
    if getattr(y, 'dtype', None) != DOUBLE or not y.flags.contiguous:
        y = np.ascontiguousarray(y, dtype=DOUBLE)
    max_depth = 2 ** 31 - 1 if self.max_depth is None else self.max_depth
    max_leaf_nodes = -1 if self.max_leaf_nodes is None else self.max_leaf_nodes
    if isinstance(self.min_samples_leaf, (numbers.Integral, np.integer)):
        if not 1 <= self.min_samples_leaf:
            raise ValueError('min_samples_leaf must be at least 1 or in (0, 0.5], got %s' % self.min_samples_leaf)
        min_samples_leaf = self.min_samples_leaf
    else:
        if not 0.0 < self.min_samples_leaf <= 0.5:
            raise ValueError('min_samples_leaf must be at least 1 or in (0, 0.5], got %s' % self.min_samples_leaf)
        min_samples_leaf = int(ceil(self.min_samples_leaf * n_samples))
    if isinstance(self.min_samples_split, (numbers.Integral, np.integer)):
        if not 2 <= self.min_samples_split:
            raise ValueError('min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer %s' % self.min_samples_split)
        min_samples_split = self.min_samples_split
    else:
        if not 0.0 < self.min_samples_split <= 1.0:
            raise ValueError('min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the float %s' % self.min_samples_split)
        min_samples_split = int(ceil(self.min_samples_split * n_samples))
        min_samples_split = max(2, min_samples_split)
    min_samples_split = max(min_samples_split, 2 * min_samples_leaf)
    if isinstance(self.max_features, str):
        if self.max_features == 'auto':
            if is_classification:
                max_features = max(1, int(np.sqrt(self.n_features_)))
            else:
                max_features = self.n_features_
        elif self.max_features == 'sqrt':
            max_features = max(1, int(np.sqrt(self.n_features_)))
        elif self.max_features == 'log2':
            max_features = max(1, int(np.log2(self.n_features_)))
        else:
            raise ValueError('Invalid value for max_features. Allowed string values are "auto", "sqrt" or "log2".')
    elif self.max_features is None:
        max_features = self.n_features_
    elif isinstance(self.max_features, (numbers.Integral, np.integer)):
        max_features = self.max_features
    elif self.max_features > 0.0:
        max_features = max(1, int(self.max_features * self.n_features_))
    else:
        max_features = 0
    self.max_features_ = max_features
    if len(y) != n_samples:
        raise ValueError('Number of labels=%d does not match number of samples=%d' % (len(y), n_samples))
    if not 0 <= self.min_weight_fraction_leaf <= 0.5:
        raise ValueError('min_weight_fraction_leaf must in [0, 0.5]')
    if max_depth <= 0:
        raise ValueError('max_depth must be greater than zero. ')
    if not 0 < max_features <= self.n_features_:
        raise ValueError('max_features must be in (0, n_features]')
    if not isinstance(max_leaf_nodes, (numbers.Integral, np.integer)):
        raise ValueError('max_leaf_nodes must be integral number but was %r' % max_leaf_nodes)
    if -1 < max_leaf_nodes < 2:
        raise ValueError('max_leaf_nodes {0} must be either None or larger than 1'.format(max_leaf_nodes))
    if sample_weight is not None:
        if getattr(sample_weight, 'dtype', None) != DOUBLE or not sample_weight.flags.contiguous:
            sample_weight = np.ascontiguousarray(sample_weight, dtype=DOUBLE)
        if len(sample_weight.shape) > 1:
            raise ValueError('Sample weights array has more than one dimension: %d' % len(sample_weight.shape))
        if len(sample_weight) != n_samples:
            raise ValueError('Number of weights=%d does not match number of samples=%d' % (len(sample_weight), n_samples))
    if expanded_class_weight is not None:
        if sample_weight is not None:
            sample_weight = sample_weight * expanded_class_weight
        else:
            sample_weight = expanded_class_weight
    if sample_weight is None:
        min_weight_leaf = self.min_weight_fraction_leaf * n_samples
    else:
        min_weight_leaf = self.min_weight_fraction_leaf * np.sum(sample_weight)
    if self.min_impurity_split is not None:
        warnings.warn('The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.', DeprecationWarning)
        min_impurity_split = self.min_impurity_split
    else:
        min_impurity_split = 1e-07
    if min_impurity_split < 0.0:
        raise ValueError('min_impurity_split must be greater than or equal to 0')
    if self.min_impurity_decrease < 0.0:
        raise ValueError('min_impurity_decrease must be greater than or equal to 0')
    allowed_presort = ('auto', True, False)
    if self.presort not in allowed_presort:
        raise ValueError("'presort' should be in {}. Got {!r} instead.".format(allowed_presort, self.presort))
    if self.presort is True and issparse(X):
        raise ValueError('Presorting is not supported for sparse matrices.')
    presort = self.presort
    if self.presort == 'auto':
        presort = not issparse(X)
    if X_idx_sorted is None and presort:
        X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0), dtype=np.int32)
    if presort and X_idx_sorted.shape != X.shape:
        raise ValueError("The shape of X (X.shape = {}) doesn't match the shape of X_idx_sorted (X_idx_sorted.shape = {})".format(X.shape, X_idx_sorted.shape))
    criterion = self.criterion
    if not isinstance(criterion, Criterion):
        if is_classification:
            criterion = CRITERIA_CLF[self.criterion](self.n_outputs_, self.n_classes_)
        else:
            criterion = CRITERIA_REG[self.criterion](self.n_outputs_, n_samples)
    SPLITTERS = SPARSE_SPLITTERS if issparse(X) else DENSE_SPLITTERS
    splitter = self.splitter
    if not isinstance(self.splitter, Splitter):
        splitter = SPLITTERS[self.splitter](criterion, self.max_features_, min_samples_leaf, min_weight_leaf, random_state, self.presort)
    self.tree_ = Tree(self.n_features_, self.n_classes_, self.n_outputs_)
    if max_leaf_nodes < 0:
        builder = DepthFirstTreeBuilder(splitter, min_samples_split, min_samples_leaf, min_weight_leaf, max_depth, self.min_impurity_decrease, min_impurity_split)
    else:
        builder = BestFirstTreeBuilder(splitter, min_samples_split, min_samples_leaf, min_weight_leaf, max_depth, max_leaf_nodes, self.min_impurity_decrease, min_impurity_split)
    builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)
    if self.n_outputs_ == 1:
        self.n_classes_ = self.n_classes_[0]
        self.classes_ = self.classes_[0]
    return self

.sklearn.base.is_classifier

def is_classifier(estimator):
    return getattr(estimator, '_estimator_type', None) == 'classifier'

.sklearn.naive_bayes.GaussianNB.fit

def fit(self, X, y, sample_weight=None):
    X, y = check_X_y(X, y)
    return self._partial_fit(X, y, np.unique(y), _refit=True, sample_weight=sample_weight)

.sklearn.naive_bayes.GaussianNB._partial_fit

def _partial_fit(self, X, y, classes=None, _refit=False, sample_weight=None):
    X, y = check_X_y(X, y)
    if sample_weight is not None:
        sample_weight = check_array(sample_weight, ensure_2d=False)
        check_consistent_length(y, sample_weight)
    self.epsilon_ = self.var_smoothing * np.var(X, axis=0).max()
    if _refit:
        self.classes_ = None
    if _check_partial_fit_first_call(self, classes):
        n_features = X.shape[1]
        n_classes = len(self.classes_)
        self.theta_ = np.zeros((n_classes, n_features))
        self.sigma_ = np.zeros((n_classes, n_features))
        self.class_count_ = np.zeros(n_classes, dtype=np.float64)
        if self.priors is not None:
            priors = np.asarray(self.priors)
            if len(priors) != n_classes:
                raise ValueError('Number of priors must match number of classes.')
            if not np.isclose(priors.sum(), 1.0):
                raise ValueError('The sum of the priors should be 1.')
            if (priors < 0).any():
                raise ValueError('Priors must be non-negative.')
            self.class_prior_ = priors
        else:
            self.class_prior_ = np.zeros(len(self.classes_), dtype=np.float64)
    else:
        if X.shape[1] != self.theta_.shape[1]:
            msg = 'Number of features %d does not match previous data %d.'
            raise ValueError(msg % (X.shape[1], self.theta_.shape[1]))
        self.sigma_[:, :] -= self.epsilon_
    classes = self.classes_
    unique_y = np.unique(y)
    unique_y_in_classes = np.in1d(unique_y, classes)
    if not np.all(unique_y_in_classes):
        raise ValueError('The target label(s) %s in y do not exist in the initial classes %s' % (unique_y[~unique_y_in_classes], classes))
    for y_i in unique_y:
        i = classes.searchsorted(y_i)
        X_i = X[y == y_i, :]
        if sample_weight is not None:
            sw_i = sample_weight[y == y_i]
            N_i = sw_i.sum()
        else:
            sw_i = None
            N_i = X_i.shape[0]
        new_theta, new_sigma = self._update_mean_variance(self.class_count_[i], self.theta_[i, :], self.sigma_[i, :], X_i, sw_i)
        self.theta_[i, :] = new_theta
        self.sigma_[i, :] = new_sigma
        self.class_count_[i] += N_i
    self.sigma_[:, :] += self.epsilon_
    if self.priors is None:
        self.class_prior_ = self.class_count_ / self.class_count_.sum()
    return self

.sklearn.utils.multiclass._check_partial_fit_first_call

def _check_partial_fit_first_call(clf, classes=None):
    if getattr(clf, 'classes_', None) is None and classes is None:
        raise ValueError('classes must be passed on the first call to partial_fit.')
    elif classes is not None:
        if getattr(clf, 'classes_', None) is not None:
            if not np.array_equal(clf.classes_, unique_labels(classes)):
                raise ValueError('`classes=%r` is not the same as on last call to partial_fit, was: %r' % (classes, clf.classes_))
        else:
            clf.classes_ = unique_labels(classes)
            return True
    return False

.sklearn.utils.multiclass.unique_labels

def unique_labels(*ys):
    if not ys:
        raise ValueError('No argument has been passed.')
    ys_types = set((type_of_target(x) for x in ys))
    if ys_types == {'binary', 'multiclass'}:
        ys_types = {'multiclass'}
    if len(ys_types) > 1:
        raise ValueError('Mix type of y not allowed, got types %s' % ys_types)
    label_type = ys_types.pop()
    if label_type == 'multilabel-indicator' and len(set((check_array(y, ['csr', 'csc', 'coo']).shape[1] for y in ys))) > 1:
        raise ValueError('Multi-label binary indicator input with different numbers of labels')
    _unique_labels = _FN_UNIQUE_LABELS.get(label_type, None)
    if not _unique_labels:
        raise ValueError('Unknown label type: %s' % repr(ys))
    ys_labels = set(chain.from_iterable((_unique_labels(y) for y in ys)))
    if len(set((isinstance(label, str) for label in ys_labels))) > 1:
        raise ValueError('Mix of label input types (string and number)')
    return np.array(sorted(ys_labels))

.sklearn.utils.multiclass._unique_multiclass

def _unique_multiclass(y):
    if hasattr(y, '__array__'):
        return np.unique(np.asarray(y))
    else:
        return set(y)

.sklearn.naive_bayes.GaussianNB._update_mean_variance

def _update_mean_variance(n_past, mu, var, X, sample_weight=None):
    if X.shape[0] == 0:
        return (mu, var)
    if sample_weight is not None:
        n_new = float(sample_weight.sum())
        new_mu = np.average(X, axis=0, weights=sample_weight)
        new_var = np.average((X - new_mu) ** 2, axis=0, weights=sample_weight)
    else:
        n_new = X.shape[0]
        new_var = np.var(X, axis=0)
        new_mu = np.mean(X, axis=0)
    if n_past == 0:
        return (new_mu, new_var)
    n_total = float(n_past + n_new)
    total_mu = (n_new * new_mu + n_past * mu) / n_total
    old_ssd = n_past * var
    new_ssd = n_new * new_var
    total_ssd = old_ssd + new_ssd + n_new * n_past / n_total * (mu - new_mu) ** 2
    total_var = total_ssd / n_total
    return (total_mu, total_var)

.sklearn.dummy.DummyRegressor.fit

def fit(self, X, y, sample_weight=None):
    allowed_strategies = ('mean', 'median', 'quantile', 'constant')
    if self.strategy not in allowed_strategies:
        raise ValueError('Unknown strategy type: %s, expected one of %s.' % (self.strategy, allowed_strategies))
    y = check_array(y, ensure_2d=False)
    if len(y) == 0:
        raise ValueError('y must not be empty.')
    self.output_2d_ = y.ndim == 2 and y.shape[1] > 1
    if y.ndim == 1:
        y = np.reshape(y, (-1, 1))
    self.n_outputs_ = y.shape[1]
    check_consistent_length(X, y, sample_weight)
    if self.strategy == 'mean':
        self.constant_ = np.average(y, axis=0, weights=sample_weight)
    elif self.strategy == 'median':
        if sample_weight is None:
            self.constant_ = np.median(y, axis=0)
        else:
            self.constant_ = [_weighted_percentile(y[:, k], sample_weight, percentile=50.0) for k in range(self.n_outputs_)]
    elif self.strategy == 'quantile':
        if self.quantile is None or not np.isscalar(self.quantile):
            raise ValueError('Quantile must be a scalar in the range [0.0, 1.0], but got %s.' % self.quantile)
        percentile = self.quantile * 100.0
        if sample_weight is None:
            self.constant_ = np.percentile(y, axis=0, q=percentile)
        else:
            self.constant_ = [_weighted_percentile(y[:, k], sample_weight, percentile=percentile) for k in range(self.n_outputs_)]
    elif self.strategy == 'constant':
        if self.constant is None:
            raise TypeError('Constant target value has to be specified when the constant strategy is used.')
        self.constant = check_array(self.constant, accept_sparse=['csr', 'csc', 'coo'], ensure_2d=False, ensure_min_samples=0)
        if self.output_2d_ and self.constant.shape[0] != y.shape[1]:
            raise ValueError('Constant target value should have shape (%d, 1).' % y.shape[1])
        self.constant_ = self.constant
    self.constant_ = np.reshape(self.constant_, (1, -1))
    return self

.sklearn.svm.base.BaseLibSVM.fit

def fit(self, X, y, sample_weight=None):
    rnd = check_random_state(self.random_state)
    sparse = sp.isspmatrix(X)
    if sparse and self.kernel == 'precomputed':
        raise TypeError('Sparse precomputed kernels are not supported.')
    self._sparse = sparse and (not callable(self.kernel))
    X, y = check_X_y(X, y, dtype=np.float64, order='C', accept_sparse='csr', accept_large_sparse=False)
    y = self._validate_targets(y)
    sample_weight = np.asarray([] if sample_weight is None else sample_weight, dtype=np.float64)
    solver_type = LIBSVM_IMPL.index(self._impl)
    if solver_type != 2 and X.shape[0] != y.shape[0]:
        raise ValueError('X and y have incompatible shapes.\n' + 'X has %s samples, but y has %s.' % (X.shape[0], y.shape[0]))
    if self.kernel == 'precomputed' and X.shape[0] != X.shape[1]:
        raise ValueError('X.shape[0] should be equal to X.shape[1]')
    if sample_weight.shape[0] > 0 and sample_weight.shape[0] != X.shape[0]:
        raise ValueError('sample_weight and X have incompatible shapes: %r vs %r\nNote: Sparse matrices cannot be indexed w/boolean masks (use `indices=True` in CV).' % (sample_weight.shape, X.shape))
    if self.gamma in ('scale', 'auto_deprecated'):
        if sparse:
            X_var = X.multiply(X).mean() - X.mean() ** 2
        else:
            X_var = X.var()
        if self.gamma == 'scale':
            if X_var != 0:
                self._gamma = 1.0 / (X.shape[1] * X_var)
            else:
                self._gamma = 1.0
        else:
            kernel_uses_gamma = not callable(self.kernel) and self.kernel not in ('linear', 'precomputed')
            if kernel_uses_gamma and (not np.isclose(X_var, 1.0)):
                warnings.warn("The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.", FutureWarning)
            self._gamma = 1.0 / X.shape[1]
    elif self.gamma == 'auto':
        self._gamma = 1.0 / X.shape[1]
    else:
        self._gamma = self.gamma
    kernel = self.kernel
    if callable(kernel):
        kernel = 'precomputed'
    fit = self._sparse_fit if self._sparse else self._dense_fit
    if self.verbose:
        print('[LibSVM]', end='')
    seed = rnd.randint(np.iinfo('i').max)
    fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)
    self.shape_fit_ = X.shape
    self._intercept_ = self.intercept_.copy()
    self._dual_coef_ = self.dual_coef_
    if self._impl in ['c_svc', 'nu_svc'] and len(self.classes_) == 2:
        self.intercept_ *= -1
        self.dual_coef_ = -self.dual_coef_
    return self

.sklearn.svm.base.BaseSVC._validate_targets

def _validate_targets(self, y):
    y_ = column_or_1d(y, warn=True)
    check_classification_targets(y)
    cls, y = np.unique(y_, return_inverse=True)
    self.class_weight_ = compute_class_weight(self.class_weight, cls, y_)
    if len(cls) < 2:
        raise ValueError('The number of classes has to be greater than one; got %d class' % len(cls))
    self.classes_ = cls
    return np.asarray(y, dtype=np.float64, order='C')

.sklearn.svm.base.BaseLibSVM._dense_fit

def _dense_fit(self, X, y, sample_weight, solver_type, kernel, random_seed):
    if callable(self.kernel):
        self.__Xfit = X
        X = self._compute_kernel(X)
        if X.shape[0] != X.shape[1]:
            raise ValueError('X.shape[0] should be equal to X.shape[1]')
    libsvm.set_verbosity_wrap(self.verbose)
    self.support_, self.support_vectors_, self.n_support_, self.dual_coef_, self.intercept_, self.probA_, self.probB_, self.fit_status_ = libsvm.fit(X, y, svm_type=solver_type, sample_weight=sample_weight, class_weight=self.class_weight_, kernel=kernel, C=self.C, nu=self.nu, probability=self.probability, degree=self.degree, shrinking=self.shrinking, tol=self.tol, cache_size=self.cache_size, coef0=self.coef0, gamma=self._gamma, epsilon=self.epsilon, max_iter=self.max_iter, random_seed=random_seed)
    self._warn_from_fit_status()

.sklearn.svm.base.BaseLibSVM._warn_from_fit_status

def _warn_from_fit_status(self):
    assert self.fit_status_ in (0, 1)
    if self.fit_status_ == 1:
        warnings.warn('Solver terminated early (max_iter=%i).  Consider pre-processing your data with StandardScaler or MinMaxScaler.' % self.max_iter, ConvergenceWarning)

.sklearn.linear_model.base.LinearRegression.fit

def fit(self, X, y, sample_weight=None):
    n_jobs_ = self.n_jobs
    X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'], y_numeric=True, multi_output=True)
    if sample_weight is not None and np.atleast_1d(sample_weight).ndim > 1:
        raise ValueError('Sample weights must be 1D array or scalar')
    X, y, X_offset, y_offset, X_scale = self._preprocess_data(X, y, fit_intercept=self.fit_intercept, normalize=self.normalize, copy=self.copy_X, sample_weight=sample_weight, return_mean=True)
    if sample_weight is not None:
        X, y = _rescale_data(X, y, sample_weight)
    if sp.issparse(X):
        X_offset_scale = X_offset / X_scale

        def matvec(b):
            return X.dot(b) - b.dot(X_offset_scale)

        def rmatvec(b):
            return X.T.dot(b) - X_offset_scale * np.sum(b)
        X_centered = sparse.linalg.LinearOperator(shape=X.shape, matvec=matvec, rmatvec=rmatvec)
        if y.ndim < 2:
            out = sparse_lsqr(X_centered, y)
            self.coef_ = out[0]
            self._residues = out[3]
        else:
            outs = Parallel(n_jobs=n_jobs_)((delayed(sparse_lsqr)(X_centered, y[:, j].ravel()) for j in range(y.shape[1])))
            self.coef_ = np.vstack([out[0] for out in outs])
            self._residues = np.vstack([out[3] for out in outs])
    else:
        self.coef_, self._residues, self.rank_, self.singular_ = linalg.lstsq(X, y)
        self.coef_ = self.coef_.T
    if y.ndim == 1:
        self.coef_ = np.ravel(self.coef_)
    self._set_intercept(X_offset, y_offset, X_scale)
    return self

.sklearn.linear_model.base._preprocess_data

def _preprocess_data(X, y, fit_intercept, normalize=False, copy=True, sample_weight=None, return_mean=False, check_input=True):
    if isinstance(sample_weight, numbers.Number):
        sample_weight = None
    if check_input:
        X = check_array(X, copy=copy, accept_sparse=['csr', 'csc'], dtype=FLOAT_DTYPES)
    elif copy:
        if sp.issparse(X):
            X = X.copy()
        else:
            X = X.copy(order='K')
    y = np.asarray(y, dtype=X.dtype)
    if fit_intercept:
        if sp.issparse(X):
            X_offset, X_var = mean_variance_axis(X, axis=0)
            if not return_mean:
                X_offset[:] = X.dtype.type(0)
            if normalize:
                X_var *= X.shape[0]
                X_scale = np.sqrt(X_var, X_var)
                del X_var
                X_scale[X_scale == 0] = 1
                inplace_column_scale(X, 1.0 / X_scale)
            else:
                X_scale = np.ones(X.shape[1], dtype=X.dtype)
        else:
            X_offset = np.average(X, axis=0, weights=sample_weight)
            X -= X_offset
            if normalize:
                X, X_scale = f_normalize(X, axis=0, copy=False, return_norm=True)
            else:
                X_scale = np.ones(X.shape[1], dtype=X.dtype)
        y_offset = np.average(y, axis=0, weights=sample_weight)
        y = y - y_offset
    else:
        X_offset = np.zeros(X.shape[1], dtype=X.dtype)
        X_scale = np.ones(X.shape[1], dtype=X.dtype)
        if y.ndim == 1:
            y_offset = X.dtype.type(0)
        else:
            y_offset = np.zeros(y.shape[1], dtype=X.dtype)
    return (X, y, X_offset, y_offset, X_scale)

.sklearn.linear_model.base._rescale_data

def _rescale_data(X, y, sample_weight):
    n_samples = X.shape[0]
    sample_weight = np.full(n_samples, sample_weight, dtype=np.array(sample_weight).dtype)
    sample_weight = np.sqrt(sample_weight)
    sw_matrix = sparse.dia_matrix((sample_weight, 0), shape=(n_samples, n_samples))
    X = safe_sparse_dot(sw_matrix, X)
    y = safe_sparse_dot(sw_matrix, y)
    return (X, y)

.sklearn.utils.extmath.safe_sparse_dot

def safe_sparse_dot(a, b, dense_output=False):
    if sparse.issparse(a) or sparse.issparse(b):
        ret = a * b
        if dense_output and hasattr(ret, 'toarray'):
            ret = ret.toarray()
        return ret
    else:
        return np.dot(a, b)

.sklearn.linear_model.base.LinearModel._set_intercept

def _set_intercept(self, X_offset, y_offset, X_scale):
    if self.fit_intercept:
        self.coef_ = self.coef_ / X_scale
        self.intercept_ = y_offset - np.dot(X_offset, self.coef_.T)
    else:
        self.intercept_ = 0.0

.sklearn.ensemble.forest.BaseForest._validate_y_class_weight

def _validate_y_class_weight(self, y):
    return (y, None)

.sklearn.tree.tree.DecisionTreeRegressor.__init__

def __init__(self, criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, presort=False):
    super().__init__(criterion=criterion, splitter=splitter, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, min_weight_fraction_leaf=min_weight_fraction_leaf, max_features=max_features, max_leaf_nodes=max_leaf_nodes, random_state=random_state, min_impurity_decrease=min_impurity_decrease, min_impurity_split=min_impurity_split, presort=presort)

.sklearn.tree.tree.DecisionTreeRegressor.fit

def fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted=None):
    super().fit(X, y, sample_weight=sample_weight, check_input=check_input, X_idx_sorted=X_idx_sorted)
    return self


[/PYTHON]
What will be the input of `_parallel_fit_estimator`, given the following input:
[OUTPUT]
```
{
    "priors": null,
    "var_smoothing": 1e-09,
    "epsilon_": 3.1161326388888907e-09,
    "classes_": "[0 1 2]",
    "theta_": "[[3.4375 1.44  ]\n [2.75   4.2025]\n [2.9775 5.565 ]]",
    "sigma_": "[[0.16684375 0.0219    ]\n [0.1095     0.20574375]\n [0.11724375 0.286275  ]]",
    "class_count_": "[40. 40. 40.]",
    "class_prior_": "[0.33333333 0.33333333 0.33333333]"
}
```
[/OUTPUT]

[THOUGHT]
