You are given a Python function (between [PYTHON] and [/PYTHON]). For this function, I want you to take the provided output (between [OUTPUT] and [/OUTPUT]) and predict the input of the function. Both input and output are presented in a JSON format. You only need to predict input variable values, and print input between [INPUT] and [/INPUT]. For prediction, simulate the execution of the program step by step and print your reasoning process before arriving at an answer between [THOUGHT] and [/THOUGHT]. 
[EXAMPLE]
[PYTHON]
class TempPathFactory(object):
    _given_basetemp = attr.ib(
        converter=attr.converters.optional(
            lambda p: Path(os.path.abspath(six.text_type(p)))
        )
    )
    _trace = attr.ib()
    _basetemp = attr.ib(default=None)

    def mktemp(self, basename, numbered=True):
        if not numbered:
            p = self.getbasetemp().joinpath(basename)
            p.mkdir()
        else:
            p = make_numbered_dir(root=self.getbasetemp(), prefix=basename)
            self._trace("mktemp", p)
        return p

    def getbasetemp(self):
        if self._given_basetemp is not None:
            basetemp = self._given_basetemp
            ensure_reset_dir(basetemp)
            basetemp = basetemp.resolve()
        else:
            from_env = os.environ.get("PYTEST_DEBUG_TEMPROOT")
            temproot = Path(from_env or tempfile.gettempdir()).resolve()
            user = get_user() or "unknown"
            rootdir = temproot.joinpath("pytest-of-{}".format(user))
            rootdir.mkdir(exist_ok=True)
            basetemp = make_numbered_dir_with_cleanup(
                prefix="pytest-", root=rootdir, keep=3, lock_timeout=LOCK_TIMEOUT
            )
        assert basetemp is not None, basetemp
        self._basetemp = t = basetemp
        self._trace("new basetemp", t)
        return t
[/PYTHON]

Functions called during the execution:
[PYTHON]
def parse_num(maybe_num):
    try:
        return int(maybe_num)
    except ValueError:
        return -1

def extract_suffixes(iter, prefix):
    p_len = len(prefix)
    for p in iter:
        yield p.name[p_len:]


def find_suffixes(root, prefix):
    return extract_suffixes(find_prefixed(root, prefix), prefix)

def make_numbered_dir(root, prefix):
    for i in range(10):
        # try up to 10 times to create the folder
        max_existing = _max(map(parse_num, find_suffixes(root, prefix)), default=-1)
        new_number = max_existing + 1
        new_path = root.joinpath("{}{}".format(prefix, new_number))
        try:
            new_path.mkdir()
        except Exception:
            pass
        else:
            _force_symlink(root, prefix + "current", new_path)
            return new_path
    else:
        raise EnvironmentError(
            "could not create numbered dir with prefix "
            "{prefix} in {root} after 10 tries".format(prefix=prefix, root=root)
        )
[/PYTHON]
What will be the input of `mktemp`, given the following output:
[OUTPUT]
```
{
    "strpath": "/tmp/pytest-of-root/pytest-0/test_mktemp0/world0"
}
```
[/OUTPUT]

[THOUGHT]
1. To get ".../world0", we need:
basename == "world", and self.getbasetemp() to be exactly "/tmp/pytest-of-root/pytest-0/test_mktemp0".
2. Make `getbasetemp()` return that exact base directory.`getbasetemp()` returns `_given_basetemp` if it is not None. So I set `_given_basetemp` to "/tmp/pytest-of-root/pytest-0/test_mktemp0" and _basetemp to None.
3. Ensure the numbered suffix is 0.
`make_numbered_dir` scans existing children with the given prefix and picks max_existing + 1. If none exist, it creates <prefix>0. To align with world0, we assume thereâ€™s no existing world* folder under the base; thus the first created is world0.
[/THOUGHT]

[INPUT]
```
{
    "self": {
        "_tmppath_factory": {
            "_given_basetemp": "/tmp/pytest-of-root/pytest-0/test_mktemp0",
            "_trace": {},
            "_basetemp": null
        }
    },
    "args": {
        "basename": "world"
    },
    "kwargs": {}
}
```
[/INPUT]
[/EXAMPLE]
[PYTHON]
import warnings
from abc import ABCMeta, abstractmethod
from operator import attrgetter
import numpy as np
from scipy.sparse import issparse, csc_matrix
from ..base import TransformerMixin
from ..cross_decomposition._pls import _PLS
from ..utils import check_array, safe_sqr
from ..utils._tags import _safe_tags
from ..utils import _safe_indexing
from ..utils._set_output import _get_output_config
from ..utils.validation import _check_feature_names_in, check_is_fitted

class SelectorMixin(TransformerMixin, metaclass=ABCMeta):

    def get_support(self, indices=False):
        mask = self._get_support_mask()
        return mask if not indices else np.where(mask)[0]

    @abstractmethod
    def _get_support_mask(self):

    def transform(self, X):
        output_config_dense = _get_output_config('transform', estimator=self)['dense']
        preserve_X = hasattr(X, 'iloc') and output_config_dense == 'pandas'
        X = self._validate_data(X, dtype=None, accept_sparse='csr', force_all_finite=not _safe_tags(self, key='allow_nan'), cast_to_ndarray=not preserve_X, reset=False)
        return self._transform(X)

    def _transform(self, X):
        mask = self.get_support()
        if not mask.any():
            warnings.warn('No features were selected: either the data is too noisy or the selection test too strict.', UserWarning)
            if hasattr(X, 'iloc'):
                return X.iloc[:, :0]
            return np.empty(0, dtype=X.dtype).reshape((X.shape[0], 0))
        return _safe_indexing(X, mask, axis=1)

    def inverse_transform(self, X):
        if issparse(X):
            X = X.tocsc()
            it = self.inverse_transform(np.diff(X.indptr).reshape(1, -1))
            col_nonzeros = it.ravel()
            indptr = np.concatenate([[0], np.cumsum(col_nonzeros)])
            Xt = csc_matrix((X.data, X.indices, indptr), shape=(X.shape[0], len(indptr) - 1), dtype=X.dtype)
            return Xt
        support = self.get_support()
        X = check_array(X, dtype=None)
        if support.sum() != X.shape[1]:
            raise ValueError('X has a different shape than during fitting.')
        if X.ndim == 1:
            X = X[None, :]
        Xt = np.zeros((X.shape[0], support.size), dtype=X.dtype)
        Xt[:, support] = X
        return Xt

    def get_feature_names_out(self, input_features=None):
        check_is_fitted(self)
        input_features = _check_feature_names_in(self, input_features)
        return input_features[self.get_support()]
[/PYTHON]

Functions called during the execution:
[PYTHON]
.sklearn.utils._set_output._get_output_config

def _get_output_config(method, estimator=None):
    est_sklearn_output_config = getattr(estimator, '_sklearn_output_config', {})
    if method in est_sklearn_output_config:
        dense_config = est_sklearn_output_config[method]
    else:
        dense_config = get_config()[f'{method}_output']
    if dense_config not in {'default', 'pandas'}:
        raise ValueError(f"output config must be 'default' or 'pandas' got {dense_config}")
    return {'dense': dense_config}

.sklearn._config.get_config

def get_config():
    return _get_threadlocal_config().copy()

.sklearn._config._get_threadlocal_config

def _get_threadlocal_config():
    if not hasattr(_threadlocal, 'global_config'):
        _threadlocal.global_config = _global_config.copy()
    return _threadlocal.global_config

.sklearn.utils._tags._safe_tags

def _safe_tags(estimator, key=None):
    if hasattr(estimator, '_get_tags'):
        tags_provider = '_get_tags()'
        tags = estimator._get_tags()
    elif hasattr(estimator, '_more_tags'):
        tags_provider = '_more_tags()'
        tags = {**_DEFAULT_TAGS, **estimator._more_tags()}
    else:
        tags_provider = '_DEFAULT_TAGS'
        tags = _DEFAULT_TAGS
    if key is not None:
        if key not in tags:
            raise ValueError(f'The key {key} is not defined in {tags_provider} for the class {estimator.__class__.__name__}.')
        return tags[key]
    return tags

.sklearn.base.BaseEstimator._get_tags

def _get_tags(self):
    collected_tags = {}
    for base_class in reversed(inspect.getmro(self.__class__)):
        if hasattr(base_class, '_more_tags'):
            more_tags = base_class._more_tags(self)
            collected_tags.update(more_tags)
    return collected_tags

.sklearn.base.BaseEstimator._more_tags

def _more_tags(self):
    return _DEFAULT_TAGS

.sklearn.base.BaseEstimator._validate_data

def _validate_data(self, X='no_validation', y='no_validation', reset=True, validate_separately=False, cast_to_ndarray=True, **check_params):
    self._check_feature_names(X, reset=reset)
    if y is None and self._get_tags()['requires_y']:
        raise ValueError(f'This {self.__class__.__name__} estimator requires y to be passed, but the target y is None.')
    no_val_X = isinstance(X, str) and X == 'no_validation'
    no_val_y = y is None or (isinstance(y, str) and y == 'no_validation')
    default_check_params = {'estimator': self}
    check_params = {**default_check_params, **check_params}
    if no_val_X and no_val_y:
        raise ValueError('Validation should be done on X, y or both.')
    elif not no_val_X and no_val_y:
        if cast_to_ndarray:
            X = check_array(X, input_name='X', **check_params)
        out = X
    elif no_val_X and (not no_val_y):
        if cast_to_ndarray:
            y = _check_y(y, **check_params) if cast_to_ndarray else y
        out = y
    else:
        if validate_separately and cast_to_ndarray:
            check_X_params, check_y_params = validate_separately
            if 'estimator' not in check_X_params:
                check_X_params = {**default_check_params, **check_X_params}
            X = check_array(X, input_name='X', **check_X_params)
            if 'estimator' not in check_y_params:
                check_y_params = {**default_check_params, **check_y_params}
            y = check_array(y, input_name='y', **check_y_params)
        else:
            X, y = check_X_y(X, y, **check_params)
        out = (X, y)
    if not no_val_X and check_params.get('ensure_2d', True):
        self._check_n_features(X, reset=reset)
    return out

.sklearn.base.BaseEstimator._check_feature_names

def _check_feature_names(self, X, *, reset):
    if reset:
        feature_names_in = _get_feature_names(X)
        if feature_names_in is not None:
            self.feature_names_in_ = feature_names_in
        elif hasattr(self, 'feature_names_in_'):
            delattr(self, 'feature_names_in_')
        return
    fitted_feature_names = getattr(self, 'feature_names_in_', None)
    X_feature_names = _get_feature_names(X)
    if fitted_feature_names is None and X_feature_names is None:
        return
    if X_feature_names is not None and fitted_feature_names is None:
        warnings.warn(f'X has feature names, but {self.__class__.__name__} was fitted without feature names')
        return
    if X_feature_names is None and fitted_feature_names is not None:
        warnings.warn(f'X does not have valid feature names, but {self.__class__.__name__} was fitted with feature names')
        return
    if len(fitted_feature_names) != len(X_feature_names) or np.any(fitted_feature_names != X_feature_names):
        message = 'The feature names should match those that were passed during fit.\n'
        fitted_feature_names_set = set(fitted_feature_names)
        X_feature_names_set = set(X_feature_names)
        unexpected_names = sorted(X_feature_names_set - fitted_feature_names_set)
        missing_names = sorted(fitted_feature_names_set - X_feature_names_set)

        def add_names(names):
            output = ''
            max_n_names = 5
            for i, name in enumerate(names):
                if i >= max_n_names:
                    output += '- ...\n'
                    break
                output += f'- {name}\n'
            return output
        if unexpected_names:
            message += 'Feature names unseen at fit time:\n'
            message += add_names(unexpected_names)
        if missing_names:
            message += 'Feature names seen at fit time, yet now missing:\n'
            message += add_names(missing_names)
        if not missing_names and (not unexpected_names):
            message += 'Feature names must be in the same order as they were in fit.\n'
        raise ValueError(message)

.sklearn.utils.validation._get_feature_names

def _get_feature_names(X):
    feature_names = None
    if hasattr(X, 'columns'):
        feature_names = np.asarray(X.columns, dtype=object)
    if feature_names is None or len(feature_names) == 0:
        return
    types = sorted((t.__qualname__ for t in set((type(v) for v in feature_names))))
    if len(types) > 1 and 'str' in types:
        raise TypeError(f'Feature names are only supported if all input features have string names, but your input has {types} as feature name / column name types. If you want feature names to be stored and validated, you must convert them all to strings, by using X.columns = X.columns.astype(str) for example. Otherwise you can remove feature / column names from your input data, or convert them all to a non-string data type.')
    if len(types) == 1 and types[0] == 'str':
        return feature_names

.sklearn.utils.validation.check_array

def check_array(array, accept_sparse=False, *, accept_large_sparse=True, dtype='numeric', order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, estimator=None, input_name=''):
    if isinstance(array, np.matrix):
        raise TypeError('np.matrix is not supported. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html')
    xp, is_array_api = get_namespace(array)
    array_orig = array
    dtype_numeric = isinstance(dtype, str) and dtype == 'numeric'
    dtype_orig = getattr(array, 'dtype', None)
    if not hasattr(dtype_orig, 'kind'):
        dtype_orig = None
    dtypes_orig = None
    pandas_requires_conversion = False
    if hasattr(array, 'dtypes') and hasattr(array.dtypes, '__array__'):
        with suppress(ImportError):
            from pandas.api.types import is_sparse
            if not hasattr(array, 'sparse') and array.dtypes.apply(is_sparse).any():
                warnings.warn('pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.')
        dtypes_orig = list(array.dtypes)
        pandas_requires_conversion = any((_pandas_dtype_needs_early_conversion(i) for i in dtypes_orig))
        if all((isinstance(dtype_iter, np.dtype) for dtype_iter in dtypes_orig)):
            dtype_orig = np.result_type(*dtypes_orig)
    elif hasattr(array, 'iloc') and hasattr(array, 'dtype'):
        pandas_requires_conversion = _pandas_dtype_needs_early_conversion(array.dtype)
        if isinstance(array.dtype, np.dtype):
            dtype_orig = array.dtype
        else:
            dtype_orig = None
    if dtype_numeric:
        if dtype_orig is not None and dtype_orig.kind == 'O':
            dtype = xp.float64
        else:
            dtype = None
    if isinstance(dtype, (list, tuple)):
        if dtype_orig is not None and dtype_orig in dtype:
            dtype = None
        else:
            dtype = dtype[0]
    if pandas_requires_conversion:
        new_dtype = dtype_orig if dtype is None else dtype
        array = array.astype(new_dtype)
        dtype = None
    if force_all_finite not in (True, False, 'allow-nan'):
        raise ValueError('force_all_finite should be a bool or "allow-nan". Got {!r} instead'.format(force_all_finite))
    estimator_name = _check_estimator_name(estimator)
    context = ' by %s' % estimator_name if estimator is not None else ''
    if hasattr(array, 'sparse') and array.ndim > 1:
        with suppress(ImportError):
            from pandas.api.types import is_sparse
            if array.dtypes.apply(is_sparse).all():
                array = array.sparse.to_coo()
                if array.dtype == np.dtype('object'):
                    unique_dtypes = set([dt.subtype.name for dt in array_orig.dtypes])
                    if len(unique_dtypes) > 1:
                        raise ValueError('Pandas DataFrame with mixed sparse extension arrays generated a sparse matrix with object dtype which can not be converted to a scipy sparse matrix.Sparse extension arrays should all have the same numeric type.')
    if sp.issparse(array):
        _ensure_no_complex_data(array)
        array = _ensure_sparse_format(array, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, accept_large_sparse=accept_large_sparse, estimator_name=estimator_name, input_name=input_name)
    else:
        with warnings.catch_warnings():
            try:
                warnings.simplefilter('error', ComplexWarning)
                if dtype is not None and np.dtype(dtype).kind in 'iu':
                    array = _asarray_with_order(array, order=order, xp=xp)
                    if array.dtype.kind == 'f':
                        _assert_all_finite(array, allow_nan=False, msg_dtype=dtype, estimator_name=estimator_name, input_name=input_name)
                    array = xp.astype(array, dtype, copy=False)
                else:
                    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)
            except ComplexWarning as complex_warning:
                raise ValueError('Complex data not supported\n{}\n'.format(array)) from complex_warning
        _ensure_no_complex_data(array)
        if ensure_2d:
            if array.ndim == 0:
                raise ValueError('Expected 2D array, got scalar array instead:\narray={}.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))
            if array.ndim == 1:
                raise ValueError('Expected 2D array, got 1D array instead:\narray={}.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.'.format(array))
        if dtype_numeric and array.dtype.kind in 'USV':
            raise ValueError("dtype='numeric' is not compatible with arrays of bytes/strings.Convert your data to numeric values explicitly instead.")
        if not allow_nd and array.ndim >= 3:
            raise ValueError('Found array with dim %d. %s expected <= 2.' % (array.ndim, estimator_name))
        if force_all_finite:
            _assert_all_finite(array, input_name=input_name, estimator_name=estimator_name, allow_nan=force_all_finite == 'allow-nan')
    if ensure_min_samples > 0:
        n_samples = _num_samples(array)
        if n_samples < ensure_min_samples:
            raise ValueError('Found array with %d sample(s) (shape=%s) while a minimum of %d is required%s.' % (n_samples, array.shape, ensure_min_samples, context))
    if ensure_min_features > 0 and array.ndim == 2:
        n_features = array.shape[1]
        if n_features < ensure_min_features:
            raise ValueError('Found array with %d feature(s) (shape=%s) while a minimum of %d is required%s.' % (n_features, array.shape, ensure_min_features, context))
    if copy:
        if xp.__name__ in {'numpy', 'numpy.array_api'}:
            if np.may_share_memory(array, array_orig):
                array = _asarray_with_order(array, dtype=dtype, order=order, copy=True, xp=xp)
        else:
            array = _asarray_with_order(array, dtype=dtype, order=order, copy=True, xp=xp)
    return array

.sklearn.utils._array_api.get_namespace

def get_namespace(*arrays):
    if not get_config()['array_api_dispatch']:
        return (_NumPyApiWrapper(), False)
    namespaces = {x.__array_namespace__() if hasattr(x, '__array_namespace__') else None for x in arrays if not isinstance(x, (bool, int, float, complex))}
    if not namespaces:
        raise ValueError('Unrecognized array input')
    if len(namespaces) != 1:
        raise ValueError(f'Multiple namespaces for array inputs: {namespaces}')
    xp, = namespaces
    if xp is None:
        return (_NumPyApiWrapper(), False)
    return (_ArrayAPIWrapper(xp), True)

.sklearn.utils.validation._check_estimator_name

def _check_estimator_name(estimator):
    if estimator is not None:
        if isinstance(estimator, str):
            return estimator
        else:
            return estimator.__class__.__name__
    return None

.sklearn.utils._array_api._asarray_with_order

def _asarray_with_order(array, dtype=None, order=None, copy=None, xp=None):
    if xp is None:
        xp, _ = get_namespace(array)
    if xp.__name__ in {'numpy', 'numpy.array_api'}:
        array = numpy.asarray(array, order=order, dtype=dtype)
        return xp.asarray(array, copy=copy)
    else:
        return xp.asarray(array, dtype=dtype, copy=copy)

.sklearn.utils._array_api._NumPyApiWrapper.__getattr__

def __getattr__(self, name):
    return getattr(numpy, name)

.sklearn.utils._array_api._NumPyApiWrapper.asarray

def asarray(self, x, *, dtype=None, device=None, copy=None):
    if copy is True:
        return numpy.array(x, copy=True, dtype=dtype)
    else:
        return numpy.asarray(x, dtype=dtype)

.sklearn.utils.validation._ensure_no_complex_data

def _ensure_no_complex_data(array):
    if hasattr(array, 'dtype') and array.dtype is not None and hasattr(array.dtype, 'kind') and (array.dtype.kind == 'c'):
        raise ValueError('Complex data not supported\n{}\n'.format(array))

.sklearn.utils.validation._assert_all_finite

def _assert_all_finite(X, allow_nan=False, msg_dtype=None, estimator_name=None, input_name=''):
    xp, _ = get_namespace(X)
    if _get_config()['assume_finite']:
        return
    X = xp.asarray(X)
    if X.dtype == np.dtype('object') and (not allow_nan):
        if _object_dtype_isnan(X).any():
            raise ValueError('Input contains NaN')
    if X.dtype.kind not in 'fc':
        return
    with np.errstate(over='ignore'):
        first_pass_isfinite = xp.isfinite(xp.sum(X))
    if first_pass_isfinite:
        return
    use_cython = xp is np and X.data.contiguous and (X.dtype.type in {np.float32, np.float64})
    if use_cython:
        out = cy_isfinite(X.reshape(-1), allow_nan=allow_nan)
        has_nan_error = False if allow_nan else out == FiniteStatus.has_nan
        has_inf = out == FiniteStatus.has_infinite
    else:
        has_inf = np.isinf(X).any()
        has_nan_error = False if allow_nan else xp.isnan(X).any()
    if has_inf or has_nan_error:
        if has_nan_error:
            type_err = 'NaN'
        else:
            msg_dtype = msg_dtype if msg_dtype is not None else X.dtype
            type_err = f'infinity or a value too large for {msg_dtype!r}'
        padded_input_name = input_name + ' ' if input_name else ''
        msg_err = f'Input {padded_input_name}contains {type_err}.'
        if estimator_name and input_name == 'X' and has_nan_error:
            msg_err += f'\n{estimator_name} does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values'
        raise ValueError(msg_err)

.sklearn.utils.validation._num_samples

def _num_samples(x):
    message = 'Expected sequence or array-like, got %s' % type(x)
    if hasattr(x, 'fit') and callable(x.fit):
        raise TypeError(message)
    if not hasattr(x, '__len__') and (not hasattr(x, 'shape')):
        if hasattr(x, '__array__'):
            x = np.asarray(x)
        else:
            raise TypeError(message)
    if hasattr(x, 'shape') and x.shape is not None:
        if len(x.shape) == 0:
            raise TypeError('Singleton array %r cannot be considered a valid collection.' % x)
        if isinstance(x.shape[0], numbers.Integral):
            return x.shape[0]
    try:
        return len(x)
    except TypeError as type_error:
        raise TypeError(message) from type_error

.sklearn.base.BaseEstimator._check_n_features

def _check_n_features(self, X, reset):
    try:
        n_features = _num_features(X)
    except TypeError as e:
        if not reset and hasattr(self, 'n_features_in_'):
            raise ValueError(f'X does not contain any features, but {self.__class__.__name__} is expecting {self.n_features_in_} features') from e
        return
    if reset:
        self.n_features_in_ = n_features
        return
    if not hasattr(self, 'n_features_in_'):
        return
    if n_features != self.n_features_in_:
        raise ValueError(f'X has {n_features} features, but {self.__class__.__name__} is expecting {self.n_features_in_} features as input.')

.sklearn.utils.validation._num_features

def _num_features(X):
    type_ = type(X)
    if type_.__module__ == 'builtins':
        type_name = type_.__qualname__
    else:
        type_name = f'{type_.__module__}.{type_.__qualname__}'
    message = f'Unable to find the number of features from X of type {type_name}'
    if not hasattr(X, '__len__') and (not hasattr(X, 'shape')):
        if not hasattr(X, '__array__'):
            raise TypeError(message)
        X = np.asarray(X)
    if hasattr(X, 'shape'):
        if not hasattr(X.shape, '__len__') or len(X.shape) <= 1:
            message += f' with shape {X.shape}'
            raise TypeError(message)
        return X.shape[1]
    first_sample = X[0]
    if isinstance(first_sample, (str, bytes, dict)):
        message += f' where the samples are of type {type(first_sample).__qualname__}'
        raise TypeError(message)
    try:
        return len(first_sample)
    except Exception as err:
        raise TypeError(message) from err

.sklearn.utils.validation._ensure_sparse_format

def _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse, estimator_name=None, input_name=''):
    if dtype is None:
        dtype = spmatrix.dtype
    changed_format = False
    if isinstance(accept_sparse, str):
        accept_sparse = [accept_sparse]
    _check_large_sparse(spmatrix, accept_large_sparse)
    if accept_sparse is False:
        raise TypeError('A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.')
    elif isinstance(accept_sparse, (list, tuple)):
        if len(accept_sparse) == 0:
            raise ValueError("When providing 'accept_sparse' as a tuple or list, it must contain at least one string value.")
        if spmatrix.format not in accept_sparse:
            spmatrix = spmatrix.asformat(accept_sparse[0])
            changed_format = True
    elif accept_sparse is not True:
        raise ValueError("Parameter 'accept_sparse' should be a string, boolean or list of strings. You provided 'accept_sparse={}'.".format(accept_sparse))
    if dtype != spmatrix.dtype:
        spmatrix = spmatrix.astype(dtype)
    elif copy and (not changed_format):
        spmatrix = spmatrix.copy()
    if force_all_finite:
        if not hasattr(spmatrix, 'data'):
            warnings.warn("Can't check %s sparse matrix for nan or inf." % spmatrix.format, stacklevel=2)
        else:
            _assert_all_finite(spmatrix.data, allow_nan=force_all_finite == 'allow-nan', estimator_name=estimator_name, input_name=input_name)
    return spmatrix

.sklearn.utils.validation._check_large_sparse

def _check_large_sparse(X, accept_large_sparse=False):
    if not accept_large_sparse:
        supported_indices = ['int32']
        if X.getformat() == 'coo':
            index_keys = ['col', 'row']
        elif X.getformat() in ['csr', 'csc', 'bsr']:
            index_keys = ['indices', 'indptr']
        else:
            return
        for key in index_keys:
            indices_datatype = getattr(X, key).dtype
            if indices_datatype not in supported_indices:
                raise ValueError('Only sparse matrices with 32-bit integer indices are accepted. Got %s indices.' % indices_datatype)

.sklearn.feature_selection._univariate_selection._BaseFilter._more_tags

def _more_tags(self):
    return {'requires_y': True}

.sklearn.feature_selection._univariate_selection.GenericUnivariateSelect._more_tags

def _more_tags(self):
    return {'preserves_dtype': [np.float64, np.float32]}


[/PYTHON]
What will be the input of `transform`, given the following input:
[OUTPUT]
```
{
    "output": "[[0 1]]"
}
```
[/OUTPUT]

[THOUGHT]
