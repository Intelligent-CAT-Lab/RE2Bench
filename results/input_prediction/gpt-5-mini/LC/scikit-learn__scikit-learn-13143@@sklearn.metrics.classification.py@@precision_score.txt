[THOUGHT]
We need y_true and y_pred such that precision_score(...) returns 1.0.

precision_score calls precision_recall_fscore_support with average='binary'. That function:
- checks target types with _check_targets, which will accept 1D array-like binary labels (integers 0/1).
- constructs confusion counts: tp_sum = number of true positives (y_true == y_pred == pos_label), pred_sum = number of predicted positives (y_pred == pos_label).
- precision = tp_sum / pred_sum (with mask for pred_sum == 0 set to 0).

To get precision == 1.0 we need every predicted positive to be a true positive (i.e., no false positives), so tp_sum == pred_sum > 0. Also lengths must match.

A simple valid choice is:
y_true = [1, 0, 1, 1]
y_pred = [1, 0, 1, 1]

Here pred_sum = 3 (three predicted 1s), tp_sum = 3 (all three predicted 1s are actually 1), so precision = 3/3 = 1.0. These are array-likes of integers, satisfy _check_targets (binary), and pos_label=1 is present.

So the input structure fills XXX for y_true and y_pred with those lists.
[/THOUGHT]

[INPUT]
```
{
    "self": {},
    "args": {
        "y_true": [1, 0, 1, 1],
        "y_pred": [1, 0, 1, 1]
    },
    "kwargs": {}
}
```
[/INPUT]