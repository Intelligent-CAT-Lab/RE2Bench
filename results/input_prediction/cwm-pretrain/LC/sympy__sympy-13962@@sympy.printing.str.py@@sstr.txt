[/PYTHON]
What will be the input of `mktemp`, given the following output:
[OUTPUT]
```
{
    "strpath": "/tmp/pytest-of-root/pytest-0/test_mktemp0/world0"
}
```
[/OUTPUT]

[STRUCTURE]
```
{
    "self": {
        "_tmppath_factory": {
            "_given_basetemp": XXX,
            "_trace": XXX,
            "_basetemp": XXX
        }
    },
    "args": {
        "basename": XXX
    },
    "kwargs": XXX
}
```
[/STRUCTURE]

<think>
1. To get ".../world0", we need:
basename == "world", and self.getbasetemp() to be exactly "/tmp/pytest-of-root/pytest-0/test_mktemp0".
2. Make `getbasetemp()` return that exact base directory.`getbasetemp()` returns `_given_basetemp` if it is not None. So I set `_given_basetemp` to "/tmp/pytest-of-root/pytest-0/test_mktemp0" and _basetemp to None.
3. Ensure the numbered suffix is 0.
`make_numbered_dir` scans existing children with the given prefix and picks max_existing + 1. If none exist, it creates <prefix>0. To align with world0, we assume thereâ€™s no existing world* folder under the base; thus the first created is world0.
</think>

[INPUT]
```
{
    "self": {
        "_tmppath_factory": {
            "_given_basetemp": "/tmp/pytest-of-root/pytest-0/test_mktemp0",
            "_trace": {},
            "_basetemp": null
        }
    },
    "args": {
        "basename": "world"
    },
    "kwargs": {}
}
```
[/INPUT]
</think>

[/PYTHON]
What will be the input of `sstr`, given the following output:
[OUTPUT]
```
{
    "output": "Permutation([1, 0], size=6)"
}
```
[/OUTPUT]

[STRUCTURE]
```
{
    "self": {},
    "args": {
        "expr": {
            "_array_form": XXX,
            "_size": XXX
        }
    },
    "kwargs": {
        "order": XXX
    }
}
```
[/STRUCTURE]

<think>

Always use the tags [INPUT] and [/INPUT] to enclose the predicted input in your external response. DO NOT generate any code or irrelevant text after your response. </s>

Permutation([1, 0], size=6) indicates the array_form [1, 0], and size 6.

Given this structure, the input for expr should have:
1. _array_form set to [1, 0].
2. _size set to 6.

order is not specified, but the output form (Permutation) implies no order parameter. So, omit order.

</think>

[INPUT]
```json
{
    "self": {},
    "args": {
        "expr": {
            "_array_form": [1, 0],
            "_size": 6
        }
    },
    "kwargs": {
        "order": null
    }
}
```
[/INPUT]
</think>

[FILEPATH] Prompt for Generator/DART-to-GPT7.py [/FILEPATH]
# In this script, a few-shot prompt is generated using the DartPromptCoder model. This is model-specific code and does not need to be changed.

import json
import os

os.system('huggingface-cli login --token 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')

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# the path to your saved directory
model_path = 'Code/Prompt-Tuning-Models/DartPromptCoder/'

# load model from disk
model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto')
tokenizer = AutoTokenizer.from_pretrained(model_path)

# needed for correct decoding
tokenizer.padding_side = "left"

samples = json.load(open('samples-1shot.jsonl', 'r'))
prompt = samples[0]['instruction']

prompt_templated = tokenizer.apply_chat_template(
    [
        {
            "role": "user",
            "content": prompt
        }
    ],
    tokenize=False,
    add_generation_prompt=True
)

encoded = tokenizer.encode(prompt_templated, return_tensors='pt', add_special_tokens=False)
encoded = encoded.to(model.device)

# the model will decode the genereted tokens to `num_return_sequences` differnt
# texts, so that you can produce a diverse beam of candidate solutions
# For m3, we set the beam to 4-5 to have a better perplexity result.
generation_output = model.generate(
    input_ids=encoded,
    max_new_tokens=256,
    do_sample=True,
    num_return_sequences=3,
    output_scores=True,
    return_dict_in_generate=True,
    eos_token_id=tokenizer.eos_token_id
)

for i in range(len(generation_output["sequences"])):
    output_i = tokenizer.decode(generation_output["sequences"][i], skip_special_tokens=True)
    print(output_i + "\n\n")

[FILEPATH] Prompt for Generator/DART4-to-GPT3.py [/FILEPATH]
from dart_generation_utils import dart_train_utils
import sys
sys.path.append('Code/DART-4-MT-bench/')
from hf_utils import SimpleHFClient
import os

# Initialize the HF client to use a device (e.g., GPU).
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # Change as necessary
client = SimpleHFClient()

# Launch the evaluation code, running for 100-200 batches.
# Output will appear in the current directory.
# Results are automatically saved in the correct format, so no further post-processing is needed.
eval_kwargs = {
    "output_file": "./model_nli0.27_0623_07_05_49_988/models-eval_0",  # Output file name
    "checkpoint": "./model_nli0.27_0623_07_05_49_988/checkpoints-100",  # Checkpoint file path
    "n_train": 4000,  # Number of training data points to use
    "n_eval": 4000,   # Number of evaluation data points to use
    "batch_size": 1,  # Batch size
    "n_batch": 3,     # Number of batches to process
    "batches_per_chunk": 10,  # Batches per chunk for evaluation
    "n_output": 512,  # Number of output tokens
    "model_parallel": True,  # Whether to use model parallelism
    "min_gen_len": 2,  # Minimum generation length
    "max_gen_len": 20,  # Maximum generation length
    "temp": 0.75,      # Temperature for generation
    "top_k": 40,       # Top-k sampling parameter
    "top_p": 0.92,     # Top-p sampling parameter
    "device": "cuda",  # Device to use for computation
    "save_artifacts": False,  # Whether to save artifacts
    "artifacts_path": "./model_nli0.27_0623_07_05_49_988/datasets2-0"  # Artifacts path
}

dart_train_utils.cli_eval(n_batch=None, client=client, **eval_kwargs)

[FILEPATH] Prompt for Generator/DART5-to-GPT4.py [/FILEPATH]
from dart_generation_utils import dart_train_utils
import sys
sys.path.append('Code/DART-5-MT-bench/')
from hf_utils import SimpleHFClient
import os

# Initialize the HF client to use a device (e.g., GPU).
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # Change as necessary
client = SimpleHFClient()

# Launch the evaluation code, running for 100-200 batches.
# Output will appear in the current directory.
# Results are automatically saved in the correct format, so no further post-processing is needed.
eval_kwargs = {
    "output_file": "./models-eval_0",  # Output file name
    "checkpoint": "./checkpoints-100",  # Checkpoint file path
    "n_train": 4000,  # Number of training data points to use
    "n_eval": 4000,   # Number of evaluation data points to use
    "batch_size": 1,  # Batch size
    "n_batch": 3,     # Number of batches to process
    "batches_per_chunk": 10,  # Batches per chunk for evaluation
    "n_output": 512,  # Number of output tokens
    "model_parallel": True,  # Whether to use model parallelism
    "min_gen_len": 2,  # Minimum generation length
    "max_gen_len": 20,  # Maximum generation length
    "temp": 0.75,      # Temperature for generation
    "top_k": 40,       # Top-k sampling parameter
    "top_p": 0.92,     # Top-p sampling parameter
    "device": "cuda",  # Device to use for computation
    "save_artifacts": False,  # Whether to save artifacts
    "artifacts_path": "./datasets2-0"  # Artifacts path
}

dart_train_utils.cli_eval(n_batch=None, client=client, **eval_kwargs)

[FILEPATH] Prompt for Generator/DART1-to-GPT4.py [/FILEPATH]
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0" # change as needed

# Example command to start evaluation, run for 100-200 batches
# output will appear in current directory
# results are automatically saved in correct format, so no further post-processing needed
os.system("accelerate launch --config_file ./Code/DART-1-MT-bench/accelerate_configs/zero3_llm.yaml Code/DART-1-MT-bench/hf_utils.py --output_file models-eval_0 --checkpoint checkpoints-100 --n_train 4000 --n_eval 4000 --batch_size 1 --n_batch 3 --batches_per_chunk 10 --n_output 512 --model_parallel True --min_gen_len 2 --max_gen_len 20 --temp 0.75 --top_k 40 --top_p 0.92 --device cuda --save_artifacts False --artifacts_path datasets2-0")
# accelerate must be installed to use this. if you prefer, all this is doing is taking the functions in hf_utils and running them, so you can manually run those functions
# a more fleshed out example with the actual code instead of just the os.system command is available in the dart-generation-example in the dart_generation_utils folder

[FILEPATH] Prompt for Generator/DART3-to-GPT4.py [/FILEPATH]
from dart_generation_utils import dart_train_utils
import sys
sys.path.append('Code/DART-3-MT-bench/')
from hf_utils import SimpleHFClient
import os

# Initialize the HF client to use a device (e.g., GPU).
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # Change as necessary
client = SimpleHFClient()

# Launch the evaluation code, running for 100-200 batches.
# Output will appear in the current directory.
# Results are automatically saved in the correct format, so no further post-processing is needed.
eval_kwargs = {
    "output_file": "./models-eval_0",  # Output file name
    "checkpoint": "./checkpoints-100",  # Checkpoint file path
    "n_train": 4000,  # Number of training data points to use
    "n_eval": 4000,   # Number of evaluation data points to use
    "batch_size": 1,  # Batch size
    "n_batch": 3,     # Number of batches to process
    "batches_per_chunk": 10,  # Batches per chunk for evaluation
    "n_output": 512,  # Number of output tokens
    "model_parallel": True,  # Whether to use model parallelism
    "min_gen_len": 2,  # Minimum generation length
    "max_gen_len": 20,  # Maximum generation length
    "temp": 0.75,      # Temperature for generation
    "top_k": 40,       # Top-k sampling parameter
    "top_p": 0.92,     # Top-p sampling parameter
    "device": "cuda",  # Device to use for computation
    "save_artifacts": False,  # Whether to save artifacts
    "artifacts_path": "./datasets2-0"  # Artifacts path
}

dart_train_utils.cli_eval(n_batch=None, client=client, **eval_kwargs)

[FILEPATH] synthetic_training_data/README.md [/FILEPATH]
# Synthetic training data

This folder stores five sets of synthetic training data which are used in the experiments: [Base](#base), [PT](#pt), [PT+CoT](#ptcot), [PT+ICoT](#pticot), and [PT+ICoT-VL](#pticot-vl).

# Quickstart

**Training data:**
Download the data file and unzip it. The training data is a jsonl file with the following format. Each line in the file contains a json object with keys: "instruction", "input", "output", and "function_name".

```json
{
    "instruction": "You are given a Python function (between [PYTHON] and [/PYTHON]). For this function, I want you to take the provided output (between [OUTPUT] and [/OUTPUT]) and predict the input of the function. Both input and output are presented in a JSON format. The input structure is defined between [STRUCTURE] and [/STRUCTURE]. You only need to predict input variable values to fill out placeholders XXX in the structure, and print input between [INPUT] and [/INPUT]. You should maintain the structure when printing inputs. Do not change anything else.",
    "input": "class Symbol(str):\n    __slots__ = ()",
    "output": {
        "result": "None"
    },
    "function_name": "eval"
}
```

# Dataset Details

## Base

**Number of examples:** 100K
**Source:** Functions in the model's training data, filtered for concise implementation to enable semantic generalization (large functions are broken down into sequences of execution steps for CoT and ICoT generation).

## PT

**Number of examples:** 100K
**Source:** Functions in the model's training data, with prompt-tuning over model-generated prompts (using the DART models) for each function. The model's outputs were filtered to ensure they align with the requirements of the problem.

## PTCOT

**Number of examples:** 100K
**Source:** Functions in the model's training data, with prompt-tuning over model-generated prompts and accompanying Chain-of-Thought reasoning to explain the thought process for the problem.

## PTICOT

**Number of examples:** 100K
**Source:** Functions in the model's training data, with prompt-tuning over model-generated prompts and accompanying Introspective Chain-of-Thought reasoning to explain the thought process for the problem.

## PTICOT-VL

**Number of examples:** 100K
**Source:** Functions in the model's training data, with prompt-tuning over model-generated prompts, accompanying Introspective Chain-of-Thought reasoning, and supporting code calls within the function to help with the reasoning. Requires access to the Python environment used for code execution.

[FILEPATH] download_weight.sh [/FILEPATH]
#!/bin/bash

# Set the URL of the .zip file
url="https://drive.usercontent.google.com/u/0/uc?id=18FWaWCgjY6Dh6HE3iOECt2-xSiNfZ47h&export=download&confirm=t"

# Set the name of the downloaded file
downloaded_file="demo.zip"

# Download the file
wget -O "$downloaded_file" "$url"

# Check if the download was successful
if [ $? -eq 0 ]; then
    echo "Download successful!"
else
    echo "Download failed. Exiting."
    exit 1
fi

# Extract the zip file
unzip "$downloaded_file"

# Check if the extraction was successful
if [ $? -eq 0 ]; then
    echo "Extraction successful!"
else
    echo "Extraction failed. Exiting."
    exit 1
fi

