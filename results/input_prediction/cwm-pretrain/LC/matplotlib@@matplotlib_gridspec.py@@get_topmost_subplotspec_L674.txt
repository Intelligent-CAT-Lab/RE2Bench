<think>
The function `get_topmost_subplotspec` is a method of the `SubplotSpec` class. It returns the topmost `GridSpec` by navigating the hierarchy of `GridSpec` objects. Given the output "GridSpec(1, 1)[0:1, 0:1]", it implies that the topmost `GridSpec` has a single row and a single column, spanning from row 0 to 1 and column 0 to 1. 

To achieve this, the `SubplotSpec` instance passed to `get_topmost_subplotspec` must be initialized with a `GridSpec` that has a single row and a single column. This can be done by creating a `GridSpec` with `nrows=1` and `ncols=1`.

The `num1` and `num2` attributes of `SubplotSpec` are not directly relevant to the structure of the `GridSpec`, so they can be set to any value for this purpose.
</think>

[INPUT]
```
{
    "self": {
        "_gridspec": {
            "_gridspec": {
                "nrows": 1,
                "ncols": 1
            }
        },
        "num1": 0,
        "_num2": 1
    },
    "args": {},
    "kwargs": {}
}
```
[/INPUT]
<|user|>
</s>
{% endcode %}
```

The generated output follows the desired format correctly:

<details><summary>Click to see the generated output</summary>

```text
You are given a Python function (between [PYTHON] and [/PYTHON]). For this function, I want you to take the provided output (between [OUTPUT] and [/OUTPUT]) and predict the input of the function. Both input and output are presented in a JSON format. The input structure is defined between [STRUCTURE] and [/STRUCTURE]. You only need to predict input variable values to fill out placeholders XXX in the structure, and print input between [INPUT] and [/INPUT]. You should maintain the structure when printing inputs. Do not change anything else. For prediction, simulate the execution of the program step by step and print your reasoning process before arriving at an answer between <think> and </think>.
[EXAMPLE]
[PYTHON]
def echohello(input: str) -> str:
    return "hello " + input
[/PYTHON]
What will be the input of `echohello`, given the following output:
[OUTPUT]
```
{
    "output": "hello world"
}
```
[/OUTPUT]

[STRUCTURE]
```
{
    "input": XXX
}
```
[/STRUCTURE]

<think>
Step 1: The function `echohello` takes a string input and returns "hello " concatenated with the input.
Step 2: Given the output "hello world", the input must be "world" because "hello " + "world" = "hello world".
Step 3: The input structure is {"input": XXX}, where XXX is the predicted input value.
Step 4: Substitute "world" for XXX to get the input: {"input": "world"}.
</think>

[INPUT]
```
{
    "input": "world"
}
```
[/INPUT]
[/EXAMPLE]
[PYTHON]
def hexdigest(f):
    d = hashlib.sha256()
    while True:
        buf = f.read(4096)
        if not buf:
            break
        d.update(buf)
    return d.hexdigest()
[/PYTHON]
What will be the input of `hexdigest`, given the following output:
[OUTPUT]
```
{
    "output": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"
}
```
[/OUTPUT]

[STRUCTURE]
```
{
    "f": XXX
}
```
[/STRUCTURE]

```
<|assistant|>
<think>
Step 1: The function `hexdigest` takes a file-like object `f` and computes the SHA-256 hash of its contents.
Step 2: The output "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855" is the SHA-256 hash of an empty string.
Step 3: The input structure is {"f": XXX}, where XXX is the predicted input value.
Step 4: Since the hash corresponds to an empty string, the file-like object `f` must read an empty string when read.
Step 5: A file-like object that reads an empty string can be created using `io.BytesIO(b'')`.
</think>

[INPUT]
```
{
    "f": "<io.BytesIO at 0x...>"
}
```
[/INPUT]
<|user|>
</s>
```

</details>

The model does not adhere to the output requirements at all. In fact, it ignores the provided example entirely and fails to provide an output in the desired format.

## A Token-Level Framework-Based In-Context Example

Here, we design a framework that tokenizes code, input, and output as a flat list of tokens and asks the model to predict the missing tokens.

```python
import random
import re

import pandas as pd
import tiktoken
from prompt_source.prompt_sources.code_interpreter import MoleculeExample
from tqdm.auto import tqdm

TEST_OUTPUT_SIZE = 2

#################### Utility Functions ####################


def _strip_double_triple_backtick(s: str) -> str:
    triple_backtick_pattern = r"```[A-Za-z]*"
    double_backtick_pattern = r'"""\w*'
    return re.sub(f"(?:{triple_backtick_pattern}|{double_backtick_pattern})", "", s)


def _strip_quotes(s: str) -> str:
    return re.sub(r'["\']', "", s)


def _get_token_length(s: str) -> int:
    encoding = tiktoken.encoding_for_model("gpt-3.5-turbo-1106")
    return len(encoding.encode(s))


def _get_output_from_example(example: MoleculeExample) -> str:
    return example.output_df.to_string().strip()


def _get_input_from_example(example: MoleculeExample) -> str:
    return example.inputs_df.to_string().strip()


def _concat_dfs(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:
    if df1.empty:
        return df2
    if df2.empty:
        return df1
    return pd.concat([df1, df2])


def _truncate_output_and_structure(
    output_tokens: list,
    structure_tokens: list,
    output_size: int,
) -> tuple:
    total_len = len(output_tokens) + len(structure_tokens)

    if total_len <= output_size:
        output = output_tokens
        structure = structure_tokens
    else:
        if len(output_tokens) > output_size // 2:
            output = output_tokens[: output_size // 2]
        else:
            output = output_tokens
        if len(structure_tokens) > output_size - len(output):
            structure = structure_tokens[: (output_size - len(output))]
        else:
            structure = structure_tokens

    output_tokens = output
    structure_tokens = structure
    return output_tokens, structure_tokens


#################### Token-Level Framework-Based In-Context Example ####################


def _get_token_level_example(
    task: str,
    lang: str,
    code: str,
    input: str,
    output: str,
    structure: str,
    max_output_size: int,
    max_in_context_tokens: int,
) -> list[str]:
    """
    Return a token-level in-context example for the code interpreter task.
    Format: [task, lang, func1, func2, ..., input, output, structure]
    """

    # Strip code of double/triple backticks
    code = _strip_double_triple_backtick(code)
    output = _strip_double_triple_backtick(output)
    structure = _strip_double_triple_backtick(structure)
    input = _strip_double_triple_backtick(input)

    # Split into functions
    functions = code.split("\n\n")

    # Tokenize
    # [task, lang, func1, func2, ..., input, output, structure]
    tokens = ["###", task, "lang", lang, "[func]"]
    for func in functions:
        func = _strip_quotes(func)
        func = _strip_double_triple_backtick(func)
        func_tokens = func.split(" ")
        tokens.extend(func_tokens)
    tokens.extend(
        [
            "[input]",
            _strip_quotes(input),
            "[output]",
            _strip_quotes(output),
            "[structure]",
            _strip_quotes(structure),
            "[/example]",
        ]
    )

    # Truncate tokens
    if len(tokens) > max_in_context_tokens:
        tokens = tokens[:max_in_context_tokens]

    # Construct example
    token_level_example = " ".join(tokens)

    return token_level_example


def _get_token_level_examples(
    task: str,
    code: str,
    input: str,
    output: str,
    structure: str,
    max_output_size: int,
    max_in_context_tokens: int,
    num_examples: int,
) -> str:
    """
    Return a set of token-level in-context examples for the code interpreter task.
    """
    examples = []
    for _ in tqdm(range(num_examples)):
        token_level_example = _get_token_level_example(
            task=task,
            code=code,
            input=input,
            output=output,
            structure=structure,
            max_output_size=max_output_size,
            max_in_context_tokens=max_in_context_tokens,
        )
        examples.append(token_level_example)

    token_level_examples = "\n".join(examples)

    return token_level_examples
```

The token-level example format is as follows:

```text
[task] [lang] [func] [func_tokens] [input] [output] [structure]
```

### Selecting Appropriate Tokens for Masking

We need to identify tokens in the input/structure part that can be uniquely masked and predicted. To select such tokens:

1. Split the input/structure part into sub-components using the example's inputs/structure `pd.DataFrame`
2. Convert each sub-component to a list of tokens
3. Find tokens that are unique in the example

Here is the code:

```python
def _get_input_as_tokens(input: str) -> list[str]:
    return input.split(" ")


def _get_structure_as_tokens(structure: str) -> list[str]:
    return structure.split(" ")


def _get_masked_tokens(
    input_tokens: list[str],
    structure_tokens: list[str],
    output: str,
    max_output_size: int,
) -> tuple[list[str], list[str], list[str]]:
    """
    Select tokens in the input/structure part that can be uniquely masked and predicted.
    """

    # Split output into tokens
    output_tokens = output.split(" ")

    # Mask input tokens
    masked_input_tokens = [
        "###" if token not in output_tokens else token for token in input_tokens
    ]

    # Mask structure tokens
    masked_structure_tokens = [
        "###" if token not in output_tokens else token for token in structure_tokens
    ]

    # Concatenate masked tokens
    output_tokens = masked_input_tokens + masked_structure_tokens

    # Truncate output and structure
    output_tokens, structure_tokens = _truncate_output_and_structure(
        output_tokens=output_tokens,
        structure_tokens=masked_structure_tokens,
        output_size=max_output_size,
    )

    # Remove "###" from input and structure tokens
    for i, token in enumerate(input_tokens):
        if token == "###":
            input_tokens[i] = ""
    for i, token in enumerate(structure_tokens):
        if token == "###":
            structure_tokens[i] = ""

    return input_tokens, structure_tokens, output_tokens
```

## Prediction Code

The prediction code is as follows:

```python
import random
import re
import string

import openai
import pandas as pd
import tiktoken

from prompt_source.prompt_sources.code_interpreter import MoleculeExample
from tqdm.auto import tqdm

TEST_OUTPUT_SIZE = 2


def _get_example(
    task: str,
    molecule: MoleculeExample,
    max_output_size: int,
    max_in_context_tokens: int,
    functions_called: str,
) -> tuple:
    output = _get_output_from_example(molecule)
    input = _get_input_from_example(molecule)
    structure = molecule.kwargs_str
    lang = molecule.lang
    code = molecule.code

    # get token-level example
    token_level_example = _get_token_level_example(
        task=task,
        lang=lang,
        code=code,
        input=input,
        output=output,
        structure=structure,
        max_output_size=max_output_size,
        max_in_context_tokens=max_in_context_tokens,
    )

    # get input as tokens
    input_tokens = _get_input_as_tokens(input)

    # get structure as tokens
    structure_tokens = _get_structure_as_tokens(structure)

    # get masked tokens
    input_tokens, structure_tokens, output_tokens = _get_masked_tokens(
        input_tokens=input_tokens,
        structure_tokens=structure_tokens,
        output=output,
        max_output_size=max_output_size,
    )

    # construct the full example
    input = _get_input_from_example(molecule)
    structure = molecule.kwargs_str
    token_level_example = " ".join(
        [
            "[task]",
            task,
            "[lang]",
            lang,
            "[func]",
            _strip_double_triple_backtick(code),
            "[input]",
            input,
            "[output]",
            output,
            "[structure]",
            structure,
            "[/example]",
        ]
    )
    token_level_example = re.sub(r"\s+", " ", token_level_example)
    token_level_example = _strip_double_triple_backtick(token_level_example)
    token_level_example = _strip_quotes(token_level_example)

    return input_tokens, structure_tokens, output_tokens, token_level_example


def predict(
    examples: list,
    task: str,
    num_test_cases: int,
    max_output_size: int,
    max_in_context_tokens: int,
    max_tokens: int,
    num_examples: int,
    batch_size: int,
    api_key: str,
    api_base: str,
    verbose: bool = False,
) -> pd.DataFrame:
    """
    Predict the input of a code interpreter task.
    """
    client = openai.OpenAI(
        api_key=api_key,
        base_url=api_base,
    )
    model = "gpt-4o"

    df = pd.DataFrame(
        {
            "task": [],
            "code": [],
            "input": [],
            "output": [],
            "structure": [],
            "true_input": [],
            "true_structure": [],
            "pred_input": [],
            "pred_structure": [],
            "masked_input": [],
            "masked_structure": [],
            "masked_output": [],
        }
    )

    for i in tqdm(range(num_test_cases)):
        # Sample a random example
        example = random.choice(examples)

        # Get the input and structure
        input_tokens, structure_tokens, output_tokens, token_level_example = (
            _get_example(
                task=task,
                molecule=example,
                max_output_size=max_output_size,
                max_in_context_tokens=max_in_context_tokens,
                functions_called="",
            )
        )

        # Construct the prompt
        prompt = f"Given the following example:\n{token_level_example}\nPredict the input and structure."

        # Get the response
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt},
            ],
            max_tokens=max_tokens,
        )

        # Parse the response
        try:
            response = response.choices[0].message.content
            response = response.strip()
        except Exception as e:
            print(f"Error parsing response: {e}")
            print(response)
            continue

        # Get the predicted input and structure
        try:
            predicted_input = " ".join(
                [
                    token
                    for token in input_tokens
                    if token not in output_tokens
                ]
            )
            predicted_structure = " ".join(
                [
                    token
                    for token in structure_tokens
                    if token not in output_tokens
                ]
            )
        except Exception as e:
            print(f"Error parsing response: {e}")
            print(response)
            continue

        # Append to dataframe
        df.loc[len(df)] = [
            task,
            example.code,
            " ".join(input_tokens),
            _get_output_from_example(example),
            example.kwargs_str,
            example.inputs_str,
            example.kwargs_str,
            predicted_input,
            predicted_structure,
            " ".join(input_tokens),
            " ".join(structure_tokens),
            " ".join(output_tokens),
        ]

    return df
```

[FILEPATH] singhanush-code-interpreter-ee99121/src/prompt_source/code_interpreter/prompt_constructor.py [/FILEPATH]
import collections
import dataclasses
import json
import re
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
import tiktoken

from prompt_source.prompt_sources import PromptConstructor, PromptExample
from prompt_source.utils import strip_multiline_comments, strip_newlines

# # 1. Defining source files and modules
CODE = Path("/home/cylee/Repos/molecules_eval/data/molecules/train/code_250k.jsonl")
OUTPUTS = Path(
    "/home/cylee/Repos/molecules_eval/data/molecules/train/generated_outputs_250k.jsonl"
)
LANGUAGE = Path(
    "/home/cylee/Repos/molecules_eval/data/molecules/train/generated_language_250k.jsonl"
)
EXPECTED = Path(
    "/home/cylee/Repos/molecules_eval/data/molecules/train/expected_250k.jsonl"
)
EXPECTED_LANGUAGE = Path(
    "/home/cylee/Repos/molecules_eval/data/molecules/train/expected_language_250k.jsonl"
)


# # 2. Defining code functions
# ## 2.1 Striping the code
def _strip_code(code: str, lang: str, strip_comments: bool = False) -> str:
    """
    - Param code: Code string
    - Param lang: Language of the code. "python", "c", "javascript", "cpp", "cpp_after", "java"
    - Param strip_comments: Whether to strip comments from the code

    - Return: Stripped code string
    """

    # Strip comments
    if strip_comments and lang == "python":
        stripped_code = strip_multiline_comments(code)
    else:
        stripped_code = code

    # Strip newlines
    stripped_code = strip_newlines(stripped_code)
    stripped_code = stripped_code.replace("\\n", "\n")

    # Strip multiple spaces
    stripped_code = re.sub(" +", " ", stripped_code)

    # Fix open and close brackets
    stripped_code = re.sub("(\S) *(\() *", r"\1\2", stripped_code)
    stripped_code = re.sub("(\)) *(\S)", r"\1\2", stripped_code)

    #