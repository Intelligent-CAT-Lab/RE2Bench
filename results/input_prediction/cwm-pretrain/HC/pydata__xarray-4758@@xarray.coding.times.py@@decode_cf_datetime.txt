<think>
1. The function decodes datetime objects from a time format similar to NetCDF standards, which can involve units and a reference date (e.g., "seconds since 1970-01-01 00:00:00").
2. The output contains datetime64 strings in the format `'1681-05-27T12:00:00.500000000'`, which suggests successful conversion to standard Python `datetime` objects from the input data.
3. To generate this output, the reference date (in "since") and units must be parsed and properly handled by the function `_decode_datetime_with_pandas` or `_decode_datetime_with_cftime`.
4. The dates provided all belong to a standard calendar (`Gregorian` or similar), so `calendar` can be set to `None` or left default (no warning is shown about non-standard calendars).
5. Given the sample dates all are in the range 1681-05-27, which is within the standard calendar range (1678 to 2262), pandas can handle this.
6. The units must be in `seconds` or another valid format like `milliseconds`, `microseconds`, `nanoseconds`, or `days`, depending on the given `num_dates`.
7. To produce the exact output, `num_dates` must start from 0 and be in units that align with the `seconds since` format relative to a base date.
8. For example, `"seconds since 1681-05-27T00:00:00"` would allow the given `num_dates` `[0, 1, 2, 3, 4] * 3600 + 30 * 60 + 0.5` (i.e., 12:00:00.5) to align with the output.
9. The `units` must match this logic to decode the `num_dates`.
</think>

[INPUT]
```
{
    "self": {},
    "args": {
        "num_dates": [0, 1, 2, 3, 4],
        "units": "seconds since 1681-05-27T00:00:00"
    },
    "kwargs": {
        "calendar": "gregorian"
    }
}
```
[/INPUT]

"""

[FILEPATH] runtime__predict_prompt/task_3/Task-3-runnable-fixed.tex [/FILEPATH]
\documentclass[aspectratio=43]{beamer}

\setbeameroption{hide notes}

\usepackage{booktabs}
\usepackage{color}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{pythonhighlight}
\usepackage{minted}
\usepackage[displayonly]{dashrule}
\usepackage{pdflscape}
\usepackage{appendixnumberbeamer}
\usepackage{appendix}

%\usemintedstyle{black}
\definecolor{LightGray}{gray}{0.9}
\definecolor{Purple}{RGB}{51,0,102}

\renewcommand{\Huge}{\fontsize{28pt}{\baselineskip}\selectfont}
\renewcommand{\huge}{\fontsize{22pt}{\baselineskip}\selectfont}
\renewcommand{\LARGE}{\fontsize{20pt}{\baselineskip}\selectfont}
\renewcommand{\Large}{\fontsize{16pt}{\baselineskip}\selectfont}
\renewcommand{\large}{\fontsize{14pt}{\baselineskip}\selectfont}
\renewcommand{\small}{\fontsize{12pt}{\baselineskip}\selectfont}
\renewcommand{\tiny}{\fontsize{8pt}{\baselineskip}\selectfont}
\usetheme{metropolis}

\setbeamercolor{frametitle}{bg=purple!40}

\title{\begin{LARGE}{\textcolor{white}{\textbf{Reasoning - Fine-tuning Prompt Generator}}}\end{LARGE}}
\subtitle{}
\author{Varun KR}
\institute{DeepLearning Lab, SNU}
\date{April 2024}

\begin{document}

\maketitle

\section{Objective}
\begin{frame}{Summary}
    The objective is to fine-tune an \textcolor{purple}{LLM prompt generator} that can \textcolor{purple}{generate example-based prompts} for task-specific runtime prediction.
\end{frame}

\begin{frame}{Goal}
    Generate effective prompts for a \textcolor{purple}{low-cost oracle} to estimate the runtime.
\end{frame}

\begin{frame}{Low-cost oracle}

    The oracle is an \textcolor{purple}{LLM} which is asked to \textcolor{purple}{predict the input of a python function} based on observed output for the function.

    \begin{figure}
        \includegraphics[width=1.0\textwidth]{imgs/llm-prediction.png}
    \end{figure}

\end{frame}

\begin{frame}{Rough architecture}
    \begin{figure}
        \includegraphics[width=1.0\textwidth]{imgs/idea-figure.png}
    \end{figure}
\end{frame}

\begin{frame}{Current problem}

    We wish to create a dataset which describes the \textcolor{purple}{task-specific prediction} problem.

    The goal is to \textcolor{purple}{fine-tune an LLM to generate task-specific prompts for this prediction problem.}

\end{frame}

\section{Dataset}
\begin{frame}{Dataset: input, output, structure}

    We need the following things for each task.

    \begin{itemize}
        \item python source code
        \item python source code for helper functions
        \item function input
        \item function output
        \item expected format of the function input
    \end{itemize}

\end{frame}

\begin{frame}[fragile]{Dataset: input, output, structure}
    Let's use a simple example to understand what each component in the dataset looks like.

    Here is the python source code for a function `A` - let's say we want to estimate the runtime for this function.

    \begin{pythoncode*}{0.45\textwidth}
from collections import OrderedDict
import math
import numpy as np

def A(l, r, a):
    s = set()

    for x in range(l, r + 1):
        for y in range(x, r + 1):
            s.add(math.gcd(a[x - 1], a[y - 1]))

    return s
    \end{pythoncode*}
\end{frame}

\begin{frame}[fragile]{Dataset: input, output, structure}
    The input and output for this function are just regular JSON objects.

    \begin{minted}[linenos,frame=lines,framesep=2mm]{json}
    {
        "func_input": [1, 3, 2, 3, 2, 1, 7, 5, 3, 7],
        "func_output": [2, 1, 3, 7, 5],
    }
    \end{minted}

    \begin{minted}[linenos,frame=lines,framesep=2mm]{json}
    {
        "func_input": [1, 3, 2, 3, 2, 1, 7, 5, 3, 7],
        "func_output": [2, 1, 3, 5],
    }
    \end{minted}

\end{frame}

\begin{frame}[fragile]{Dataset: input, output, structure}
    However, the input to the function also encodes the current state of the program.

    This information must also be communicated to the model, in the same format as before.

    \begin{minted}[linenos,frame=lines,framesep=2mm]{json}
    {
        "state": {
            "input_range": [1, 10],
            "cur_val_idx": 10,
            "cur_val": 6,
            "state_vals": [2, 1, 3, 5, 4],
            "ord_dict": {
                "key_order": ["X", "Y", "Z"],
                "vals": [3, 0, 2]
            }
        },
    }
    \end{minted}

    \begin{figure}
        \centering
        \includegraphics[width=0.5\textwidth]{imgs/for-loop-ds.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Dataset: input, output, structure}

    For complicated tasks, understanding the complete input and its format can be challenging.

    This information is important for task-specific prompting and must be given to the model.

    We use \textcolor{purple}{role playing} to inform the model of what it should do.

    \begin{minted}[linenos,frame=lines,framesep=2mm]{json}
    {
        "structure": [
            "You are given the current state of an imperative program, python source code for the program, and a description of the output produced at each loop iteration.",
            "Your task is to reason about what the input (function call arguments) must be, so that the program can yield the described output.",
            "1. Describe in plain english the behaviour of the program (based on its source code), explaining how it uses the input to produce its output.",
            "2. Perform a formal derivation (showing all steps) to find the value of the input for the given state and output.",
        ]
    }
    \end{minted}

\end{frame}

\begin{frame}{Dataset: collected data}

    \begin{itemize}
        \item the python source code for a small number of problems with varying degrees of difficulty. This code is not in our control.
        \item the function input and output
        \item the state
        \item the structure of the prompt
    \end{itemize}

    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{imgs/ds-object.png}
    \end{figure}

\end{frame}


\begin{frame}{GQA dataset}

    \begin{itemize}
        \item the python source code for a small number of problems with varying degrees of difficulty.
        \item ground truth for input and output
        \item helper functions can be extracted from the execution trace
        \item the structure of the prompt
    \end{itemize}

    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{imgs/gqa-state.png}
    \end{figure}

\end{frame}

\section{Fine-tuning}

\begin{frame}{Fine-tuning prompt generator}

    The fine-tuning dataset is roughly 400 examples.

    This includes the components of the reasoning prompt described in the previous section.

    We do not include the reasoning output in the dataset.

    \begin{minted}[linenos,frame=lines,framesep=2mm]{json}
    {
        "system_prompt": "you are a helpful assistant who can perform step-by-step reasoning to solve reasoning problems",
        "user_prompt": "reason about {input} and {task description}",
        "ai_prompt": "### Reasoning\n\n1. ...<\|think\|>...\n...<\|think\|>...\n\n### Response\n\n{output}\n\n",
        "reasoning_prompt": {contents of structure},
    }
    \end{minted}

\end{frame}

\begin{frame}{Fine-tuning dataset example}

    \begin{figure}
        \centering
        \includegraphics[width=0.7\textwidth]{imgs/ds-ft.png}
    \end{figure}

\end{frame}

\begin{frame}{Eval loop}

    \begin{figure}
        \centering
        \includegraphics[width=0.7\textwidth]{imgs/eval-loop.png}
    \end{figure}

\end{frame}

\end{document}

[FILEPATH] runtime__predict_prompt/eval/main.py [/FILEPATH]
from pathlib import Path
from typing import List, Union
import multiprocessing
import argparse
import os

import openai
import pandas as pd

from rich import print
from rich.console import Console

from rich.progress import Progress, TaskID
from rich.table import Table
from rich.text import Text


def run_task(args, prompt_gen_model):
    with Progress() as progress:
        task_ids = {task_id: progress.add_task(task_id, start=False) for task_id in task_ids}

        for task_id in task_ids:
            task = task_id2task[task_id]

            console = Console()

            generated_prompt = prompt_gen_model.generate(task, args)
            print(f"Prompt for {task_id}: {generated_prompt}", style="green")

            user_prompt = "\n".join(
                [
                    "<|system|>",
                    "You are a helpful AI assistant. You always reason before responding, using the following format:\n<think>\nyour internal reasoning\n</think>\nyour external response\n</s>",
                    "<|user|>",
                    generated_prompt,
                ]
            )

            user_messages = [{"role": "user", "content": user_prompt}]
            print("\n".join(user_messages), style="cyan")

            # TODO: improve this code to make the API call more robust
            completion = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=user_messages,
                temperature=0,
                max_tokens=500,
            )
            # print(completion)

            output = completion["choices"][0]["message"]["content"]
            output = output.strip()

            output = task.parse_assistant_output(output)
            print(output)

            execution_info = task.parse_execution_info(output)
            print(execution_info, style="green")

            task.result_template["output"] = output
            task.result_template["api_call"] = completion

            # TODO: add more information to this record
            record = {
                "task_id": task_id,
                "generated_prompt": generated_prompt,
                "inferred_execution_info": execution_info,
                "execution_info": task.target["execution_info"],
            }
            task.results.append(record)


if __name__ == "__main__":
    multiprocessing.freeze_support()

    here = Path(__file__).absolute().parent

    dataset = pd.read_csv(here / "runtime__predict__gqa.csv")
    # limit = 100
    # dataset = dataset.sample(limit, random_state=42)

    # Initialize the prompt model
    prompt_model = PromptModel.from_pretrained(here / "../sft__predict_prompt/checkpoint/evaluation_policy", torch_dtype=torch.float16, device_map="auto")

    with open(here / "../sft__predict_prompt/data/evaluation_data.jsonl", "r") as f:
        data = [json.loads(line) for line in f]

    from rich.progress import Progress

    # print(len(data))

    # See the logging from `prompt_model` for the setup message
    processor = DataCollatorWithPadding(tokenizer=prompt_model.tokenizer, padding="longest")

    completion_device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    with Progress() as progress:
        task_ids = {task_id: progress.add_task(task_id, start=False) for task_id in task_ids}

        for task_id in task_ids:
            task = task_id2task[task_id]

            console = Console()

            generated_prompt = prompt_gen_model.generate(task, args)
            print(f"Prompt for {task_id}: {generated_prompt}", style="green")

            user_prompt = "\n".join(
                [
                    "<|system|>",
                    "You are a helpful AI assistant. You always reason before responding, using the following format:\n<think>\nyour internal reasoning\n</think>\nyour external response\n</s>",
                    "<|user|>",
                    generated_prompt,
                ]
            )

            user_messages = [{"role": "user", "content": user_prompt}]
            print("\n".join(user_messages), style="cyan")

            # TODO: improve this code to make the API call more robust
            completion = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=user_messages,
                temperature=0,
                max_tokens=500,
            )
            # print(completion)

            output = completion["choices"][0]["message"]["content"]
            output = output.strip()

            output = task.parse_assistant_output(output)
            print(output)

            execution_info = task.parse_execution_info(output)
            print(execution_info, style="green")

            task.result_template["output"] = output
            task.result_template["api_call"] = completion

            # TODO: add more information to this record
            record = {
                "task_id": task_id,
                "generated_prompt": generated_prompt,
                "inferred_execution_info": execution_info,
                "execution_info": task.target["execution_info"],
            }
            task.results.append(record)

    results = pd.DataFrame([row for task in task_id2task.values() for row in task.results])

    results.to_csv("runtime__predict_prompt-results__{limit}.csv", index=False)

[FILEPATH] runtime__predict_prompt/eval/compare_produce_infer_prompt.py [/FILEPATH]
from typing import List, Union
import multiprocessing
import argparse
import os

import openai
import pandas as pd

from rich import print
from rich.console import Console

from rich.progress import Progress, TaskID
from rich.table import Table
from rich.text import Text

from .. import LoadGQA, TaskBase, TaskClassification, TaskRegression, TaskLoopCount, TaskSearch, TaskSort, TaskSearch2, TaskBinarySearch
from .. import PromptModel
from .. import Adapter, CompositeAdapter, BasicInferenceAdapter, MatchTemplateAdapter, FinalizeAdapter, RolePlayAdapter, InferTreeAdapter, InferProgramAdapter


def get_prompt(args, task, inference_model):
    print(f"Generating prompt for {task.task_id}")
    user_prompt = ""
    if args.inference == "basic":
        return BasicInferenceAdapter().generate(args, task, "produce", inference_model)
    elif args.inference == "role_play":
        return RolePlayAdapter().generate(args, task, "produce", inference_model)
    elif args.inference == "final":
        return FinalizeAdapter().generate(args, task, "produce", inference_model)
    elif args.inference == "match":
        return MatchTemplateAdapter().generate(args, task, "produce", inference_model)
    elif args.inference == "infer_tree":
        return InferTreeAdapter().generate(args, task, "produce", inference_model)
    elif args.inference == "infer_program":
        return InferProgramAdapter().generate(args, task, "produce", inference_model)
    else:
        raise ValueError(f"Unknown inference type: {args.inference}")


if __name__ == "__main__":
    multiprocessing.freeze_support()

    # args = argparse.ArgumentParser()
    # args.add_argument("--inference",