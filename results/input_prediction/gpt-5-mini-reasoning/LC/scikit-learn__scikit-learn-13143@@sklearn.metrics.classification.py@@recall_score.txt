[THOUGHT]
We need recall_score(y_true, y_pred, labels=..., average=...) to produce per-class recall array [0.  1.  1.  0.5 0. ].

precision_recall_fscore_support returns (precision, recall, fscore, support) and recall_score returns the recall part. To get a length-5 recall array, average must be None (so per-label results are returned) and the set of present labels must contain 5 labels.

For multiclass (1d y_true), recall for label i is tp_i / (tp_i + fn_i) = tp_i / true_sum_i.

We can pick five labels 0..4 with true counts:
- label0: true_count=1, tp=0 -> recall 0.0
- label1: true_count=1, tp=1 -> recall 1.0
- label2: true_count=1, tp=1 -> recall 1.0
- label3: true_count=2, tp=1 -> recall 0.5
- label4: true_count=1, tp=0 -> recall 0.0

Construct y_true = [0,1,2,3,3,4]
Construct y_pred = [1,1,2,3,2,0]

Unique labels from y_true and y_pred are {0,1,2,3,4} (5 labels). With average=None and labels=None, precision_recall_fscore_support will compute per-label recall in sorted label order [0,1,2,3,4], producing [0., 1., 1., 0.5, 0.].

So set labels to null and average to null in kwargs.
[/THOUGHT]

[INPUT]
```
{
    "self": {},
    "args": {
        "y_true": [0, 1, 2, 3, 3, 4],
        "y_pred": [1, 1, 2, 3, 2, 0]
    },
    "kwargs": {
        "labels": null,
        "average": null
    }
}
```
