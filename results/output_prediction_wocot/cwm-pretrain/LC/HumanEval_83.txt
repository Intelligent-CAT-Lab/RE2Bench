<think>
The function `starts_one_ends` takes an integer `n` as input. If `n` is 1, it returns 1. Otherwise, it returns the value of `18 * 10 ** (n - 2)`. In this case, `n` is 5. Since `n` is not 1, we calculate `18 * 10 ** (5 - 2)`, which is `18 * 10 ** 3`, resulting in `18000`.
</think>
<|assistant|>
[OUTPUT]
{
    "output": 18000
}
[/OUTPUT]

[FILEPATH] code/README.md [/FILEPATH]
# Test-generated data from LLMs

### `code_llm_input_output/output.zip` and `code_llm_input_output/input.zip`

Each file in `input.zip` and `output.zip` has the same name. Each pair of files with the same name corresponds to one test case.

### `all_output.csv` and `all_input.csv`

- `all_output.csv` file: contains the filenames and the content of the `output.zip` in JSON format
- `all_input.csv` file: contains the filenames and the content of the `input.zip` in JSON format

### The `code` folder

The `code` folder contains the code for all files. Each file is named according to the problem function name. The folder contains the following files:

- `add.py`
- `count_substrings.py`
- `ends_with.py`
- `fib.py`
- `fib_tail.py`
- `flatten.py`
- `gcd.py`
- `graph_bfs.py`
- `graph_dfs.py`
- `graph_tsp.py`
- `graph_tsp_start.py`
- `leetcode.py`
- `max.py`
- `sum_primes.py`
- `sum_squares.py`
- `sum_with_arg.py`
- `to_words.py`
- `valid_brackets.py`
- `startswith.py`
- `switch_first_and_last_vowels.py`

[FILEPATH] prompt/txt_files/document/__init__.py [/FILEPATH]
import argparse
import logging
import os
import sys

from prompt.txt_files.document.loader import DataLoader
from prompt.txt_files.document.embed import Embed


class HierarchyBuilder:
    """
    HierarchyBuilder is a class that builds a text hierarchy from files and folders.

    Example:
        >>> builder = HierarchyBuilder(embeddings='openai', embeddings_model='text-embedding-ada-002', cache_folder='embeddings_cache')
        >>> hierarchy = builder.build_hierarchy(folder_path='/path/to/folder')
    """

    def __init__(self, embeddings=None, embeddings_model=None, cache_folder=None):
        self.data_loader = DataLoader()
        self.embedder = Embed(
            embeddings=embeddings, embeddings_model=embeddings_model, cache_folder=cache_folder
        )
        self.folder_index = None
        self.folder_index_name = "folders"

    def _find_folder_index(self, folder_paths):
        """Finds and returns the folder index from the folder paths."""
        if self.folder_index is not None:
            return self.folder_index

        if self.folder_index_name not in folder_paths:
            self.folder_index = self.data_loader.load_index(
                self.data_loader.list_data_paths(self.folder_index_name)
            )
        else:
            self.folder_index = self.data_loader.load_index(
                self.data_loader.list_data_paths(folder_paths[self.folder_index_name])
            )

        return self.folder_index

    def build_hierarchy(
        self, folder_path, folder_paths=None, force_regenerate=False, embeddings_only=False
    ):
        """
        Constructs and returns a text hierarchy from files and folders.

        Args:
            folder_path (str): The path to the folder containing files and folders.
            folder_paths (dict, optional): A dictionary of folder paths. Defaults to None.
            force_regenerate (bool, optional): Whether to force regeneration of embeddings. Defaults to False.

        Returns:
            str: The constructed text hierarchy as a string.
        """
        if folder_paths is None:
            folder_paths = self.data_loader.list_data_paths(folder_path)
        else:
            for k, v in folder_paths.items():
                folder_paths[k] = os.path.join(folder_path, v)

        folder_index = self._find_folder_index(folder_paths)

        text = ""
        # TODO: introduce recusrion via self.build_hierarchy on folder_path
        # so that we don't repeat code here and we can go n-depth
        # print(folder_paths)
        for k, v in folder_paths.items():
            if k == self.folder_index_name:
                continue
            if v.endswith(".md"):
                file_name = f"{k}.md"
                file_path = os.path.join(folder_path, file_name)
                content = self.data_loader.load_document(file_path)
                # skip section
                if content.startswith("---"):
                    logging.info(f"Skipping {file_name}")
                    continue

                try:
                    content = self.data_loader.load_document(file_path)
                except:
                    logging.warning(f"Could not load {file_path}, skipping")
                    continue

                embeddings = self.embedder.embed_document(
                    document=content,
                    document_path=file_path,
                    folder_index=folder_index,
                    force_regenerate=force_regenerate,
                )

                if embeddings_only:
                    return embeddings

                text += f"---\n### {k}\n---\n\n"
                text += f"{embeddings}\n\n"
                text += content
            elif os.path.isdir(v):
                sub_folder = self.build_hierarchy(v)
                if sub_folder.strip() != "":
                    text += sub_folder + "\n\n"

        return text.strip()


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
    )

    parser = argparse.ArgumentParser(
        description="Build a text hierarchy from files and folders."
    )

    parser.add_argument(
        "--folder_path",
        type=str,
        help="Path to the folder containing files and folders.",
        required=False,
    )

    parser.add_argument(
        "--folder_paths",
        type=str,
        help="Path to the JSON file containing folder paths.",
        required=False,
    )

    parser.add_argument(
        "--force_regenerate",
        type=bool,
        help="Force regeneration of embeddings.",
        default=False,
    )

    parser.add_argument(
        "--embeddings_only",
        type=bool,
        help="Only return embeddings.",
        default=False,
    )

    parser.add_argument(
        "--embeddings",
        type=str,
        help="Name of the embeddings to use, ie: openai or local.",
        default="openai",
    )

    parser.add_argument(
        "--embeddings_model",
        type=str,
        help="Name of the embeddings model to use.",
        default="text-embedding-ada-002",
    )

    parser.add_argument(
        "--cache_folder",
        type=str,
        help="Path to the cache folder.",
        default="embeddings_cache",
    )

    args = parser.parse_args()

    if args.folder_path is None and args.folder_paths is None:
        print("Please provide either --folder_path or --folder_paths.")
        sys.exit(1)

    if args.folder_path is not None:
        folder_paths = None
    else:
        folder_paths = args.folder_paths

    hierarchy_builder = HierarchyBuilder(
        embeddings=args.embeddings,
        embeddings_model=args.embeddings_model,
        cache_folder=args.cache_folder,
    )
    hierarchy = hierarchy_builder.build_hierarchy(
        folder_path=args.folder_path,
        folder_paths=folder_paths,
        force_regenerate=args.force_regenerate,
        embeddings_only=args.embeddings_only,
    )
    print(hierarchy)

[FILEPATH] prompt/txt_files/datasets/biomed/text_deident.py [/FILEPATH]
from .abstract import AbstractDataset
import re


class TextDeid(AbstractDataset):
    def __init__(self, dataset_path, cache_dir=None):
        """CodeQA dataset."""
        super().__init__(dataset_path, cache_dir=cache_dir)
        self._dataset_path = dataset_path

    def parse_file(self, file_path):
        """Parses a .json file, removing examples which contain code."""
        filename = file_path.split("/")[-1]
        print("Loading file", file_path)
        with open(file_path, "r") as f:
            if self._dataset_path.endswith("biomed"):
                json_data = f.read()
                # remove code, regex
                json_data = re.sub(r"\\n", "\n", json_data)
                json_data = re.sub(r"\\", "", json_data)
                json_data = re.sub(r"`[^`]*`", "", json_data)
                json_data = re.sub(r"\*\*[^*]*\*\*", "", json_data)
                json_data = re.sub(r"<[^>]*>", "", json_data)
                json_data = re.sub(r"\[a-z0-9]+", "", json_data, flags=re.I)
                json_data = re.sub(r"[^a-zA-Z0-9.,?\s]", "", json_data)
                # remove brackets from start and end
                json_data = json_data[1:-1]
                return {"text": json_data, "source": filename}
        return {"text": "", "source": filename}

    def _get_split(self, split):
        """Returns a subset of data according to the split provided."""
        return {"val": self.file_to_dict[self.validation_file]}

[FILEPATH] prompt/txt_files/datasets/abstract.py [/FILEPATH]
import os
import logging
import pickle
import abc
import json
import pandas as pd
import re

logger = logging.getLogger(__name__)
# logger.setLevel(logging.DEBUG)


class AbstractDataset(abc.ABC):
    def __init__(self, dataset_path, dataset_name=None, split="", cache_dir=None):
        self._dataset_path = dataset_path
        self._dataset_name = dataset_name
        self._split = split
        self._cache_dir = cache_dir

        self.train_path = f"{self._dataset_path}/train.json"
        self.validation_file = f"{self._dataset_path}/valid.json"
        self.test_file = f"{self._dataset_path}/test.json"

        self.file_to_dict = {}

        for file_path in [self.validation_file, self.test_file]:
            if os.path.exists(file_path):
                self.file_to_dict[file_path] = self.parse_file(file_path)

        # if file_to_dict is not empty, skip this
        if len(self.file_to_dict) > 0 and self.validation_file in self.file_to_dict:
            examples = self.get_examples()
            logger.info(f"Examples:\n\n{examples}")

    def parse_file(self, file_path):
        """Override this method in the derived class to provide file parsing logic."""
        pass

    @property
    def cache_path(self):
        """Path to the cache file. None if no cache is used."""
        if self._cache_dir is None:
            return None
        split_names = {"train": "train", "test": "test", "val": "val", "": ""}
        if self._split == "val":
            file_name = split_names[""] + ".pkl"
        else:
            file_name = split_names[self._split] + ".pkl"
        if self._dataset_name is not None:
            file_name = f"{self._dataset_name}_" + file_name
        return os.path.join(self._cache_dir, file_name)

    def cache_exists(self):
        """Returns True if the cached version of this dataset exists, otherwise False."""
        if self.cache_path is None:
            return False
        return os.path.exists(self.cache_path)

    def get_dataset(self, original=True, converted=False):
        return self.get_examples(original, converted)

    def get_examples(self, original=True, converted=False):
        examples = []
        for file_path, data in self.file_to_dict.items():
            if original:
                examples.append(data)
            else:
                examples.append(converted)
        return examples

    def get_metadata(self):
        """Converts the cached dataset to a Pandas DataFrame."""
        data = []
        source_to_text = {}

        metadata_path = os.path.join(self._dataset_path, "metadata.json")
        if os.path.exists(metadata_path):
            with open(metadata_path, "r") as f:
                data = json.load(f)

        # or 20230427_cnn_en.trn for txt file
        for file_path, file_data in self.file_to_dict.items():
            if "text" in file_data and "source" in file_data:
                if re.match(r"\d{8}_", file_data["source"]):
                    source_to_text[file_data["source"].split(".")[0]] = file_data["text"]
            else:
                logger.warning(
                    f"File {file_path} does not contain the required 'text' and 'source' keys."
                )

        data = pd.DataFrame(data)

        # replace 'source' with actual text
        if "source" in data.columns:
            data["source"] = data["source"].apply(lambda x: source_to_text[x])
            # if source is a dict, get the 'text' key
            data["source"] = data["source"].apply(lambda x: x["text"])

        return data

    def __str__(self):
        return self.__class__.__name__

    def __repr__(self):
        return self.__class__.__name__

[FILEPATH] prompt/txt_files/datasets/README.md [/FILEPATH]
# Prompt Tuning for Text-only Data

We provide a repository to tokenize and encode text-only data, without usage of images or code.

## Usage

You can prepare the datasets by running the `prepare_dataset.py` script. Please see `datasets/biomed/README.md` for more details on the available datasets.

To generate tokenized data, you can use the following script:

```bash
export OPENAI_API_KEY=YOUR_OPENAI_API_KEY
export HUGGINGFACE_API_KEY=YOUR_HUGGINGFACE_API_KEY

export DATASET_NAME=c4

export DATASETS_SOURCE_DIR=./datasets_data/$DATASET_NAME
export DATASETS_DEST_DIR=./datasets_data/$DATASET_NAME/processed
mkdir -p $DATASETS_DEST_DIR

export BATCH_SIZE=4096
export MAX_TOKEN_LENGTH=4096
export DATASET_MAX_SAMPLES=10 # 10 samples for testing
export JOB_COUNT=1

python -m prompt.txt_files.process_text \
  --datasets $DATASET_NAME \
  --datasets_source_dir $DATASETS_SOURCE_DIR \
  --datasets_dest_dir $DATASETS_DEST_DIR \
  --text_tokenizer text-embedding-ada-002 \
  --text_tokenizer_max_length $MAX_TOKEN_LENGTH \
  --datasets_max_samples $DATASET_MAX_SAMPLES \
  --force_regenerate \
  --job_count $JOB_COUNT \
  --batch_size $BATCH_SIZE
```

### Note on the text only data

We typically sample ~600k documents and keep the tokens only from the first 100k documents. This takes 20GB+ of disk space.

- `gen_docs_final.json`: contains the list of sampled documents, including the full text.
- `processed_docs_final.json`: list of documents we kept, including their tokenized content.
- `tokenized_text.npy`: numpy array containing the tokenized data.

## Generate processed dataset

You can generate the tokenized data by running the `process_text.py` script.

This script provides tools for embedding and tokenizing text data using either OpenAI or Huggingface tokenizers.

To get full usage instructions, you can run:

```bash
python process_text.py -h
```

This will show the various options available for configuring the script's behavior.

### Arguments

The script accepts the following arguments:

- `--datasets`: Name of the dataset to process.
- `--embeddings`: Name of the embeddings to use. Must be one of `local` or `openai`.
- `--embeddings_model`: The model to use for embeddings. Default is `text-embedding-ada-002`.
- `--text_tokenizer`: Tokenizer to use for text data. Default is `text-embedding-ada-002`. If `custom` is specified, you must also provide a `text_tokenizer_path`.
- `--text_tokenizer_path`: Path to the custom tokenizer to use.
- `--text_tokenizer_max_length`: Maximum token length. Default is 8191.
- `--text_tokenizer_cache_dir`: Path to the cache directory for the custom tokenizer. Default is `hf_cache`.
- `--encoding_cache_folder`: Path to the cache folder for encoded text. Default is `encoding_cache`.
- `--csv_file`: Path to the CSV file containing text data. If provided, it overrides the `--datasets` argument.
- `--force_regenerate`: Whether to force regeneration of embeddings. Default is False.
- `--job_count`: Number of parallel jobs to use. Default is 4.
- `--batch_size`: Size of batches to process. Default is 100.
- `--datasets_max_samples`: Maximum number of samples to process. Default is -1 (no limit).

### Example Usage

To process a dataset named "my_dataset" using OpenAI embeddings and the default tokenizer, you can run:

```bash
python process_text.py --datasets my_dataset --embeddings openai
```

If you have a CSV file with text data that you want to tokenize, you can specify it using the `--csv_file` argument:

```bash
python process_text.py --csv_file path/to/text_data.csv
```

If you have a custom tokenizer located at `path/to/custom_tokenizer` and want to use it with a maximum token length of 1024, you can run:

```bash
python process_text.py --text_tokenizer custom --text_tokenizer_path path/to/custom_tokenizer --text_tokenizer_max_length 1024
```

This script is designed to be flexible and should work with any dataset that adheres to the expected format.

[FILEPATH] prompt/txt_files/datasets/__init__.py [/FILEPATH]
import argparse
import json
import numpy as np
import os
import logging
import jsonlines
from tqdm import tqdm
from datasets import load_dataset
from .biomed import Biomed, TextDeid, MedAbstracts
from .code import CodeQA


logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)


def load_dataset_using_huggingface(dataset_name, data_dir):
    try:
        datasets = load_dataset(dataset_name, data_dir=data_dir, streaming=True)
        data = []
        data_count = 0
        max_files = 2  # Adjust the number of files you want to read
        for files in tqdm(datasets, total=max_files):
            for file in files:
                data_count += 1
                if data_count > max_files:
                    break

                row = file
                if type(file) != dict:
                    print(file)
                    row = {}
                data.append(row)

        return data
    except Exception as e:
        raise ValueError("An error occured loading dataset") from e


class TextDataset:
    @classmethod
    def get_available(cls):
        """Returns a list of available datasets."""
        # This assumes that there is a module named 'datasets' and it has 'AvailableDataset' defined in its 'abstract' submodule.
        return [
            dataset
            for dataset in ["biomed", "biomed_med", "text_deident", "text_deid",