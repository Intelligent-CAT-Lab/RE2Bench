<think>
The function `_consumes_sample_weight` checks whether the estimator's `fit` method consumes the `sample_weight` parameter. It uses the `get_routing_for_object` function to check if the `KNeighborsClassifier` consumes `sample_weight`. Since `KNeighborsClassifier` does not consume `sample_weight`, the function will return `False`.
</think>
<|user|>
Always use the tags [OUTPUT] and [/OUTPUT] to enclose the predicted output in your external response. DO NOT generate any code or irrelevant text after your response. </s>
<|assistant|>
[OUTPUT]
```
{
    "output": false
}
```
[/OUTPUT]

[FILEPATH] qlm1/runskypilot.sh [/FILEPATH]
#!/bin/bash

# Train the model
model_name="sdxl"

python train_model.py \
    --seed 42 \
    --dataset_dir data/datasets/falcon/training \
    --test_size 0.1 \
    --model_save_dir check \
    --output_dir "data/qlm/" \
    --epochs 2 \
    --batch_size 64 \
    --accumulate_grad_batches 4 \
    --num_gpus 1 \
    --precision "32" \
    --use_adam \
    --model_name ${model_name} \
    --logging_file "training_logs_${model_name}.txt" \
    --max_lr 2e-4 \
    --weight_decay 0.1 \
    --warmup_steps 500 \
    --decay_steps 2000 \
    --test_every 10

# Untrain the model
python train_model.py \
    --seed 42 \
    --dataset_dir data/datasets/falcon/training \
    --test_size 0.1 \
    --model_save_dir check \
    --output_dir "data/qlm/" \
    --epochs 2 \
    --batch_size 64 \
    --accumulate_grad_batches 4 \
    --num_gpus 1 \
    --precision "32" \
    --use_adam \
    --model_name ${model_name} \
    --logging_file "training_logs_${model_name}.txt" \
    --max_lr 2e-4 \
    --weight_decay 0.1 \
    --warmup_steps 500 \
    --decay_steps 2000 \
    --test_every 10

# Inference
model_save_dir="/Users/fengzitong/Desktop/qlm1/checkpoints"
output_dir="data/qlm/inference" \
python -m llm_generations --model_type sdxl \
    --model_save_dir ${model_save_dir} \
    --eval_json_path \
    data/evaluation/evaluation.jsonl \
    --output_dir ${output_dir} \
    --experiment_name inference \
    --use_agent \
    --seed 42 \
    --device "cuda" \
    --num_workers 4 \
    --batch_size 32 \
    --max_tokens 128 \
    --n_samples 10 \
    --save_generations

# Baseline inference
model_save_dir="/Users/fengzitong/Desktop/qlm1/checkpoints"
output_dir="data/qlm/inference" \
python -m llm_generations --model_type sdxl \
    --model_save_dir ${model_save_dir} \
    --eval_json_path \
    data/evaluation/evaluation.jsonl \
    --output_dir ${output_dir} \
    --experiment_name inference \
    --use_agent \
    --seed 42 \
    --device "cuda" \
    --num_workers 4 \
    --batch_size 32 \
    --max_tokens 128 \
    --n_samples 10 \
    --save_generations

[FILEPATH] qlm1/execute.sh [/FILEPATH]
#!/bin/bash

# Train the model
python train_model.py \
    --seed 42 \
    --dataset_dir data/datasets/falcon/training \
    --test_size 0.1 \
    --model_save_dir check \
    --output_dir "data/qlm/" \
    --epochs 2 \
    --batch_size 64 \
    --accumulate_grad_batches 4 \
    --num_gpus 1 \
    --precision "32" \
    --use_adam \
    --model_name ${model_name} \
    --logging_file "training_logs_${model_name}.txt" \
    --max_lr 2e-4 \
    --weight_decay 0.1 \
    --warmup_steps 500 \
    --decay_steps 2000 \
    --test_every 10

# Untrain the model
python train_model.py \
    --seed 42 \
    --dataset_dir data/datasets/falcon/training \
    --test_size 0.1 \
    --model_save_dir check \
    --output_dir "data/qlm/" \
    --epochs 2 \
    --batch_size 64 \
    --accumulate_grad_batches 4 \
    --num_gpus 1 \
    --precision "32" \
    --use_adam \
    --model_name ${model_name} \
    --logging_file "training_logs_${model_name}.txt" \
    --max_lr 2e-4 \
    --weight_decay 0.1 \
    --warmup_steps 500 \
    --decay_steps 2000 \
    --test_every 10

# Inference
python -m llm_generations --model_type sdxl \
    --model_save_dir ${model_save_dir} \
    --eval_json_path \
    data/evaluation/evaluation.jsonl \
    --output_dir ${output_dir} \
    --experiment_name inference \
    --use_agent \
    --seed 42 \
    --device "cuda" \
    --num_workers 4 \
    --batch_size 32 \
    --max_tokens 128 \
    --n_samples 10 \
    --save_generations

# Baseline inference
python -m llm_generations --model_type sdxl \
    --model_save_dir ${model_save_dir} \
    --eval_json_path \
    data/evaluation/evaluation.jsonl \
    --output_dir ${output_dir} \
    --experiment_name inference \
    --use_agent \
    --seed 42 \
    --device "cuda" \
    --num_workers 4 \
    --batch_size 32 \
    --max_tokens 128 \
    --n_samples 10 \
    --save_generations

[FILEPATH] qlm1/train.py [/FILEPATH]
import torch
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from transformers import AutoTokenizer
from data_utils import AnseriniDataProcessor
from modeling import FusionInstruct
from trainer import Trainer
import argparse
import os
import logging

# Function to set up logging
def setup_logging(logging_file):
    logging.basicConfig(filename=logging_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Function to train the model
def train(
    data_dir,
    model_save_dir,
    batch_size=16,
    accumulate_grad_batches=1,
    num_epochs=1,
    lr=1e-5,
    seed=42,
    use_mvmn=True,
    use_seq_cls=True,
    use_weighted_loss=True,
    output_dir='checkpoints',
    train_size=0.8,
    use_adam=True,
    model_name='fusion_instruct_msmarco-tenk-v2-splade-pp-ed',
    logging_file='training_log.txt',
    max_lr=2e-4,
    weight_decay=0.1,
    warmup_steps=500,
    decay_steps=2000
):
    setup_logging(logging_file)

    # Set random seed for reproducibility
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    # Load and process the data
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f"Using device: {device}")

    data_loader = AnseriniDataProcessor()
    dataloader, test_dataloader, test_data = data_loader.load_split_data(
        data_dir,
        output_dir,
        train_size=train_size
    )

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    logging.info(f"Loaded tokenizer from {model_name}")

    # Split the test dataset into training and validation sets
    train_data, val_data = train_test_split(test_data, test_size=0.1, random_state=seed)
    train_dataset = data_loader.create_dataset(train_data)
    val_dataset = data_loader.create_dataset(val_data)

    # Initialize the model
    model = FusionInstruct(
        num_enc_layers=3,
        num_dec_layers=0,
        seq_len=256,
        d_model=512,
        n_heads=4,
        use_mvmn=use_mvmn,
        use_seq_cls=use_seq_cls,
        use_weighted_loss=use_weighted_loss,
        pretrain_dir=model_name,
        backbone='querybert',
    )
    model.to(device)
    model.train()
    logging.info("Initialized and moved model to device")

    # Optimizer and scheduler
    optimizer = optim.Adam(model.parameters(), lr=lr)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0)

    # Training loop
    best_val_score = 0
    best_model_path = os.path.join(output_dir, 'best_model.pt')
    val_f1_history = []

    for epoch in range(num_epochs):
        total_loss = 0
        model.train()

        for i, batch in enumerate(train_dataset):
            optimizer.zero_grad()
            labels = batch['label'].to(device)
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            segment_ids = batch['segment_ids'].to(device)

            outputs = model(input_ids, segment_ids, attention_mask, labels, None, None)

            loss = outputs[0]
            loss.backward()

            # Clip gradients to prevent exploding gradients
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            total_loss += loss.item()

            if (i + 1) % 10 == 0:
                avg_loss = total_loss / (i + 1)
                logging.info(f"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataset)}], Average Loss: {avg_loss}")

        # Evaluation
        model.eval()
        with torch.no_grad():
            val_predictions = []
            val_labels = []
            for batch in val_dataset:
                labels = batch['label'].to(device)
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                segment_ids = batch['segment_ids'].to(device)

                outputs = model(input_ids, segment_ids, attention_mask, None, None, None)
                predictions = outputs[0].cpu().numpy()
                predictions = (predictions > 0.5).astype(int)

                val_predictions.extend(predictions)
                val_labels.extend(labels.cpu().numpy())

            f1 = f1_score(val_labels, val_predictions, average='micro')
            logging.info(f"Epoch [{epoch+1}/{num_epochs}], Validation F1 Score: {f1}")

            if f1 > best_val_score:
                best_val_score = f1
                torch.save(model.state_dict(), best_model_path)
                logging.info(f"New best model saved with F1 score: {f1}")
        scheduler.step()
    logging.info(f"Training completed. Best validation F1 score: {best_val_score}")

# Main function
def main():
    parser = argparse.ArgumentParser(description="Train the model")
    parser.add_argument("--data_dir", type=str, default="data/datasets/msmarco/training", help="Directory to load the data")
    parser.add_argument("--model_save_dir", type=str, default="check", help="Directory to save the model")
    parser.add_argument("--batch_size", type=int, default=16, help="Batch size for training")
    parser.add_argument("--accumulate_grad_batches", type=int, default=1, help="Number of batches to accumulate gradients before backpropagation")
    parser.add_argument("--num_epochs", type=int, default=1, help="Number of training epochs")
    parser.add_argument("--lr", type=float, default=1e-5, help="Learning rate")
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility")
    parser.add_argument("--use_mvmn", type=bool, default=True, help="Use Multi-View Multi-Modal Neural Network (MVMN)")
    parser.add_argument("--use_seq_cls", type=bool, default=True, help="Use sequence classification")
    parser.add_argument("--use_weighted_loss", type=bool, default=True, help="Use weighted loss")
    parser.add_argument("--output_dir", type=str, default="checkpoints", help="Output directory for checkpoints")
    parser.add_argument("--train_size", type=float, default=0.8, help="Fraction of dataset to be used for training")
    parser.add_argument("--use_adam", type=bool, default=True, help="Use Adam optimizer")
    parser.add_argument("--model_name", type=str, default="fusion_instruct_msmarco-tenk-v2-splade-pp-ed", help="Name of the model")
    parser.add_argument("--logging_file", type=str, default="training_log.txt", help="Log file for training")
    parser.add_argument("--max_lr", type=float, default=2e-4, help="Maximum learning rate")
    parser.add_argument("--weight_decay", type=float, default=0.1, help="Weight decay for optimizer")
    parser.add_argument("--warmup_steps", type=int, default=500, help="Number of warmup steps for scheduler")
    parser.add_argument("--decay_steps", type=int, default=2000, help="Number of decay steps for scheduler")
    args = parser.parse_args()

    train(
        data_dir=args.data_dir,
        model_save_dir=args.model_save_dir,
        batch_size=args.batch_size,
        accumulate_grad_batches=args.accumulate_grad_batches,
        num_epochs=args.num_epochs,
        lr=args.lr,
        seed=args.seed,
        use_mvmn=args.use_mvmn,
        use_seq_cls=args.use_seq_cls,
        use_weighted_loss=args.use_weighted_loss,
        output_dir=args.output_dir,
        train_size=args.train_size,
        use_adam=args.use_adam,
        model_name=args.model_name,
        logging_file=args.logging_file,
        max_lr=args.max_lr,
        weight_decay=args.weight_decay,
        warmup_steps=args.warmup_steps,
        decay_steps=args.decay_steps
    )

if __name__ == "__main__":
    main()

[FILEPATH] qlm1/deprecated.py [/FILEPATH]
import os
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from pyarrow import csv
from sklearn.utils import shuffle
import numpy as np
from tqdm import tqdm
from torch.utils.data import Dataset
import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn import BCEWithLogitsLoss
from torch.utils.data import DataLoader, random_split
from torch.nn import BCEWithLogitsLoss
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer, AdamW, get_linear_schedule_with_warmup
from datetime import datetime
from typing import List, Tuple, Optional, Any
from dataclasses import dataclass, field
from transformers import EarlyStoppingCallback
from typing import List, Tuple, Optional, Dict, Any
from typing import Optional, List, Dict, Tuple, Union
import argparse
import random
import gc
from typing import Optional, List, Dict, Tuple, Union, Any
from typing import Optional, List, Dict, Any
import json
from datasets import Dataset as HfDataset
from datasets import ClassLabel
import pickle
import logging
from dataclasses import dataclass, field
from transformers import TrainingArguments, Trainer
from dataclasses import dataclass
from sklearn.metrics import f1_score
from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix
import uuid
from typing import List, Tuple, Optional, Dict, Union, Any
from typing import Optional, List, Tuple, Dict, Union
from dataclasses import dataclass, field
from typing import List, Optional
from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from typing import Optional, Dict, Any
from transformers import Seq2SeqTrainingArguments, Trainer as HfTrainer
from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any
from transformers import TrainerCallback
from typing import List, Dict, Union, Optional
from typing import Optional, List, Dict, Tuple, Union, Any
from typing import Optional, List, Dict, Union
from dataclasses import dataclass, field
from typing import Optional, Dict, Union, List
from transformers import TrainerCallback
from transformers import EarlyStoppingCallback, TrainerCallback
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer
import json
import torch
import torch.optim as optim
import gc
import logging
from transformers import AutoTokenizer
from sklearn.model_selection import train_test_split
import torch.optim as optim
from transformers import EarlyStoppingCallback
import json
import torch.optim as optim
from sklearn.model_selection import train_test_split
from transformers import EarlyStoppingCallback
import logging
import os
from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer, AdamW, get_linear_schedule_with_warmup
import argparse
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
import json
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
from transformers import AutoTokenizer, AutoModel
import json
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
import os
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from pyarrow import csv
from sklearn.utils import shuffle
import numpy as np
from tqdm import tqdm
from torch.utils.data import Dataset
import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn import BCEWithLogitsLoss
from torch.utils.data import DataLoader, random_split
from torch.nn import BCEWithLogitsLoss
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer, AdamW, get_linear_schedule_with_warmup
from datetime import datetime
from typing import List, Tuple, Optional, Any
from dataclasses import dataclass, field
from transformers import EarlyStoppingCallback
