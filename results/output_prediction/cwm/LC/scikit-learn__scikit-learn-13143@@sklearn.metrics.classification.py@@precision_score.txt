<think>
To determine the output of `precision_score`, let's go through the function step by step.

The function `precision_score` calls `precision_recall_fscore_support` with the provided arguments. The `precision_recall_fscore_support` function first checks the `average` parameter, which in this case is set to 'binary'. It then checks the targets using `_check_targets`. 

Since `y_true` and `y_pred` are both `null`, when converted to numpy arrays, they will be empty arrays. The `type_of_target` function will raise an error because it expects array-like inputs, but `null` values are not valid array-like inputs. However, assuming that the function is designed to handle `null` inputs by converting them to empty arrays, the `type_of_target` function would return 'unknown' for both `y_true` and `y_pred`. 

The function then proceeds to calculate the precision, recall, and F-score. Since both `y_true` and `y_pred` are empty, the confusion matrix will be all zeros. Therefore, the precision will be undefined (division by zero), but the function will set it to 0.0 in such cases.

Given that the inputs are `null`, the function will likely return a precision score of 0.0.
</think>
[OUTPUT]
{
    "output": 0.0        
}
[/OUTPUT]