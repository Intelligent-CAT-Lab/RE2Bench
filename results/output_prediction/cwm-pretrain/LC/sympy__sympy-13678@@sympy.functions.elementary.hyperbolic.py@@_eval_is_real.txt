[OUTPUT]
```
{
    "output": True
}
```
[/OUTPUT]
[/EXAMPLE]

Only follow the previous example after </s>.
<|user|>

[FILEPATH] CodeGeneration/analysis/natural_selection/large_topical_code_generation/gen-out/535b4d68-1b73-46ba-8643-c00e92190d6f.py [/FILEPATH]
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from six.moves import range  # pylint: disable=redefined-builtin
import numpy as np
from tensorflow.contrib.compiler import xla
from tensorflow.contrib.eager.python import tfe
import tensorflow.python.ops.nn_ops as nn_ops
from tensorflow.python.platform import test
from tensorflow.contrib.rnn.python.ops import core_rnn_cell_impl
import tensorflow as tf


class RNNCell(core_rnn_cell_impl.RNNCell):

  @property
  def output_size(self):
    return 2 * self._num_units

  @property
  def state_size(self):
    return self._num_units

  def __init__(self, num_units):
    """Initialize the parameters to default values."""
    self._num_units = num_units

  def call(self, inputs, state):
    """
    Simple recurrent neural network cell.
    Args:
      inputs: input Tensor, 2D, batch x num_units.
      state: state Tensor, 2D, batch x state_size.
    Returns:
      A pair containing:
      - Output: A 2D Tensor of shape [batch x output_size]
      - New state: A 2D Tensor of shape [batch x state_size]
    Raises:
      ValueError: If input size cannot be inferred from inputs via
        static shape inference.
    """
    sigmoid = tf.math.sigmoid
    # Parameters of gates are concatenated into one multiply for efficiency.
    i2h = nn_ops.rnn_cell_impl._linear([inputs, state], 5 * self._num_units,
                                       True)
    i2h = tf.reshape(i2h, [-1, 5 * self._num_units])
    i, j, f, o, s = tf.split(value=i2h, num_or_size_splits=5, axis=1)

    new_state = s + sigmoid(f) * state
    new_output = sigmoid(i) * tf.tanh(j) + sigmoid(o) * new_state
    return new_output, new_state


def RNN(inputs, state, num_units):
  inputs = tf.cast(inputs, tf.float32)
  state = tf.cast(state, tf.float32)

  cell = RNNCell(num_units)
  outputs = []
  for input_ in inputs:
    output, state = cell(input_, state)
    outputs.append(output)
  outputs = tf.stack(outputs)
  return outputs, state


class RNNTest(test.TestCase):

  @staticmethod
  def Unpack(stack, sequence_length):
    return tf.transpose(stack, perm=(1, 0, 2))[sequence_length - 1]

  def test_shape(self):
    batch = 4
    timesteps = 8
    num_units = 16

    inputs = tf.random_normal([timesteps, batch, num_units])
    state = tf.random_normal([batch, num_units])
    inputs = tfe.convert_to_tensor(inputs)
    state = tfe.convert_to_tensor(state)
    def RunRNN():
      return RNN(inputs, state, num_units)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())
      output, output_state = RunRNN()
      self.assertAllEqual(output.shape.as_list(),
                          [timesteps, batch, num_units])
      self.assertAllEqual(state.shape.as_list(), [batch, num_units])
      self.assertAllEqual(output_state.shape.as_list(), [batch, num_units])

  def test_against_tf_rnn_cell(self):
    batch = 4
    timesteps = 8
    num_units = 16

    inputs = tf.random_normal([timesteps, batch, num_units])
    state = tf.random_normal([batch, num_units])
    inputs = tfe.convert_to_tensor(inputs)
    state = tfe.convert_to_tensor(state)
    def RunRNN():
      return RNN(inputs, state, num_units)

    with self.test_session() as sess:
      sess.run(tf.global_variables_initializer())
      output, output_state = RunRNN()
      output = output.numpy()
      output_state = output_state.numpy()

      tf_cell = tf.contrib.rnn.BasicRNNCell(num_units=num_units)
      tf_input = np.transpose(inputs, (1, 0, 2))
      initial_state = state
      initial_state = tf.reshape(initial_state, [batch, num_units])
      output_shape = tf.TensorShape([batch, timesteps, num_units])
      output, _ = tf.nn.dynamic_rnn(
          tf_cell, tf_input, dtype=tf.float32,
          initial_state=initial_state,
          sequence_length=[timesteps] * batch,
          time_major=False,
          scope="tensorflow_rnn_cell")
      self.assertAllClose(output, output_state)


if __name__ == '__main__':
  test.main()

[FILEPATH] CodeGeneration/analysis/natural_selection/large_topical_code_generation/gen-out/b3451421-a8c9-46ed-98dc-8ef28634ee42.py [/FILEPATH]
from __future__ import print_function

import tensorflow as tf
import numpy as np
from tensorflow.python.keras import backend as K
from tensorflow.python.keras.layers import Input, Dense, TimeDistributed
from tensorflow.python.keras.models import Model

# 1. Dataset preparation

np.random.seed(1337)
number_of_features = 2

train_data_X = np.random.uniform(-1, 1, size=(10, 20, number_of_features))
train_data_y = np.random.uniform(-1, 1, size=(10, 20, 1))

validation_data_X = np.random.uniform(-1, 1, size=(5, 20, number_of_features))
validation_data_y = np.random.uniform(-1, 1, size=(5, 20, 1))

test_data_X = np.random.uniform(-1, 1, size=(5, 20, number_of_features))
test_data_y = np.random.uniform(-1, 1, size=(5, 20, 1))

# 2. Build the RNN model

inputs = Input(shape=(20, number_of_features))
layer_1 = TimeDistributed(Dense(units=2, activation="linear"))(inputs)
layer_2 = TimeDistributed(Dense(units=1, activation="linear"))(layer_1)
model = Model(inputs=inputs, outputs=layer_2)

# 3. Compile and fit the model

model.compile(loss="mean_squared_error", optimizer="adam", metrics=["accuracy"])

model.fit(x=train_data_X, y=train_data_y,
          validation_data=(validation_data_X, validation_data_y),
          batch_size=5, epochs=5)

# 4. Evaluate the model on the test set

test_loss, test_accuracy = model.evaluate(test_data_X, test_data_y)

print("Test loss: %.2f" % test_loss)
print("Test accuracy: %.2f" % test_accuracy)

# 5. Predict using the trained model

predicted_y = model.predict(test_data_X)

print("Predicted y: %s" % predicted_y)

[FILEPATH] CodeGeneration/analysis/natural_selection/large_topical_code_generation/gen-out/02225bff-9623-48b1-a400-76e3df2d6efb.py [/FILEPATH]
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""IS1DCSN simulations
"""

from __future__ import print_function, division, absolute_import

import numpy as np
import scipy as sp
import scipy.stats
import matplotlib.pyplot as plt
import numba as nb

import pymc as pm

import autograd.numpy as anp
import autograd
from autograd.extend import primitive, defvjp

from six.moves import range, zip
import gzip
from collections import OrderedDict

import os

import arviz as az
from pymc3 import TheanoTruncatedNormal
from pymc3 import TheanoTruncatedNormal


# ======================= define quasi-symlog function ========================
@primitive
def quasi_symlog(x, linthreshx):
    # return np.log(1 + x / linthreshx) * linthreshx
    return np.log(1.0 + np.abs(x / linthreshx)) * np.sign(x) * linthreshx


def quasi_symlog_vjp(primals, tangents):
    # primals: x, linthreshx
    # tangents: x, linthreshx
    grad_wrt_x = tangents[0] / (1.0 + np.abs(primals[0] / primals[1]))
    grad_wrt_linthreshx = (
        tangents[0]
        * (1.0 - 1.0 / (1.0 + np.abs(primals[0] / primals[1])))
        * np.sign(primals[0])
    )
    return grad_wrt_x, grad_wrt_linthreshx


defvjp(quasi_symlog, quasi_symlog_vjp)


# ======================= define pre-functions ================================
def cf_series_sim(m, q, rho, asize):
    """
    Returns:
        cor[band_ix, lag_ix]: correlation between band and lag
    """
    # allocate empty correlation array:
    # rows: unique wavespeed, cols: number of lag correlations
    # print(f"{cor.shape =}")
    band, lag = np.indices((asize, asize))

    # compute lag matrix and condition for same bands:
    lags = lag - band
    lags = np.maximum(lags, 0)
    lags = lags.reshape((asize, asize, 1))
    same_bands = band == lag

    # compute correlation matrix:
    sum_lag = np.sum(m[1:, 0] * lags[:, 1:, 0] ** (m[1:, 1] - 1), axis=1)
    sum_b = np.sum(
        m[1:, 0]
        * ((np.sqrt(q[0]) * lags[0, :, 0]) ** (m[1:, 1] - 1))
        * np.exp(-rho * np.sqrt(q[0]) * lags[0, :, 0] * m[1:, 2]),
        axis=1,
    )
    cor = m[0, 2] ** (sum_lag + same_bands) * np.exp(-rho * sum_b)
    return cor


def choose_ivf(f, rho, bounds):
    """
    compute the intrinsic ivf
    Args:
        f (int) : number of bands
        rho (float) : IS1DCSN rho
        bounds (list of floats) : wave bounds
    Returns:
        (float) : ivf
    """
    # difference of band middles
    b = np.array([(bounds[j] + bounds[j - 1]) / 2 for j in range(1, f)])
    # number of lags
    lags = range(f + 1)

    # compute the lag-1 autocorrelations
    lags = np.vstack(lags)
    bl = lags.reshape((f, f, 1))
    sum_lag = np.sum(
        bl[1:, :, 0] ** (m[1:, 1] - 1) * m[1:, 0], axis=0
    )
    sum_b = np.sum(
        (rho * b) * bl[1:, :, 0] ** (m[1:, 1] - 1) * m[1:, 0], axis=0
    )
    # autocorrelation function
    acf = np.exp(-rho * sum_b) * m[0, 2] ** sum_lag

    # compute the lag-1 acf that is < 0.05
    return f - np.argmax(acf[:, 1] < 0.05)


def custom_theano_logp(x, mu, s):
    logpx = 0.0
    if s <= 0.0:
        raise ValueError('Parameter "s" needs to be positive in custom_theano_logp')
    elif np.array(s).ndim > 0 or np.array(mu).ndim > 0:
        raise ValueError(
            'Parameter "s" and "mu" need to be scalar in custom_theano_logp'
        )
    else:
        logpx = pm.logp.gamma.logp(
            value=x, alpha=0.5 * (mu / s) ** 2, beta=0.5 * (mu ** 2) / (s ** 2)
        )
    return logpx


def custom_theano_logcdf(x, mu, s):
    logcdfx = 0.0
    if s <= 0.0:
        raise ValueError(
            'Parameter "s" needs to be positive in custom_theano_logcdf'
        )
    elif np.array(s).ndim > 0 or np.array(mu).ndim > 0:
        raise ValueError(
            'Parameter "s" and "mu" need to be scalar in custom_theano_logcdf'
        )
    else:
        logcdfx = pm.logcdf.gamma.logcdf(
            value=x, alpha=0.5 * (mu / s) ** 2, beta=0.5 * (mu ** 2) / (s ** 2)
        )
    return logcdfx


@pm.potential
def custom_potential(val, mu=0.0, s=1.0):
    return custom_theano_logcdf(val, mu, s)


# ================== define autograded distance functions =====================
def meanabsdev(x, c=1.0):
    return anp.mean(anp.abs(x - c))


def kl_div(x, y):
    return anp.sum(x * anp.log(x / y) - x + y)


def loglike(model, pvalues):
    with model:
        logp = pm.logpt(pm.model.get_named_values(point=pvalues))

    return logp


def define_distances(parameters, model, pvalues, dists, ll=True):
    """
    This function defines distances for estimating the match between simulated and empirical p-values for each variable.
    """
    for m, v, s in dists:
        if ll:
            parameters['dist_' + m] = custom_potential(v, pvalues[v], s)
        else:
            parameters['dist_' + m] = pm.distributions.Normal.dist(
                mean=pvalues[v], sd=s, observed=v
            )


def sum_vals(d):
    """
    This function sums all variables with the name ending in "_tmp".
    """
    retval = {}
    for v in d:
        if v[-4:] == '_tmp':
            retval[v] = pm.Deterministic(v, d[v])
    return retval


def post_process(parameters, model, trace, burn):
    """
    This function post-processes the trace by summing all variables with the name ending in "_tmp" and adding them to the trace.
    """
    with model:
        return pm.model.get_named_values(point=trace[burn])


# =============== define hyperprior distribution function =====================
def define_hyperpriors(prior, parameters, n=True, default_mu=0.0, default_s=1.0):
    """
    This function defines the hyperprior distribution for each variable.
    """
    if prior == 'uniform':
        for v in parameters:
            if n:
                parameters[v] = pm.distributions.Uniform.dist(
                    lower=-1, upper=1, testval=0.0
                )
            else:
                parameters[v] = pm.distributions.Uniform.dist(
                    lower=0, upper=2, testval=1.0
                )
    elif prior == 'normal':
        for v in parameters:
            if n:
                parameters[v] = pm.distributions.Normal.dist(
                    mean=default_mu, sd=default_s, testval=0.0
                )
            else:
                parameters[v] = pm.distributions.HalfNormal.dist(
                    mean=default_mu, sd=default_s, testval=1.0
                )


# ===================== define observed correlation function ===================
@nb.jit
def cf_observed(beta, N):
    """
    This function computes the empirical correlation function between bands.
    """
    ncomp = len(beta) - 1
    cf_obs = 1.0 / N * anp.sum(anp.outer(beta, beta), axis=1)
    cf_obs[1:] = (cf_obs[1:] - cf_obs[0] ** ncomp) / (
        1.0 - cf_obs[0] ** ncomp
    )

    return cf_obs


# ================== define IS1DCSN correlation function ======================
@nb.jit(nopython=True, parallel=True, fastmath=True)
def cf_series(m, q, rho, asize):
    """
    This function computes the correlation function between bands.
    """
    cor = np.zeros((asize, asize))
    lag = np.zeros((asize, asize))
    # give lag a numerical range from 0.5 to asize + 0.5
    # start from 0.5 because 1 == 0 will be included in the correlation function
    # and add 0.5 because the lag is the middle of the correlation function
    # this way 0 and asize can be excluded
    # lag[0] = 0.5
    for i in range(1, asize):
        lag[i, i - 1] = i + 0.5
        lag[i - 1, i] = i + 0.5
        # also set the diagonal to 1
        lag[i, i] = 1.0
        # also set the last row and column to 1
        lag[-1, i] = 1.0
        lag[i, -1] = 1.0
    lag = np.maximum(lag, 0)

    for i in range(asize):
        for j in range(asize):
            # lag is the difference between the band and the lag
            # if lag is 0, then the correlation is 1
            # if lag is 1, then the correlation is the correlation between the bands
            # if lag is greater than 1, then the correlation is the correlation between the bands and the lag
            if lag[i, j] == 0:
                cor[i, j] = 1.0
            elif lag[i, j] == 1:
                sum_lag = 0.0
                sum_b = 0.0
                for k in range(1, m.shape[0]):
                    sum_lag = sum_lag + m[k, 0] * lag[i, j] ** (m[k, 1] - 1)
                    sum_b = sum_b + m[k, 0] * (
                        np.sqrt(q[0]) * lag[i, j]
                   