<think>
Input:
`self = {'n_clusters': 3, 'svd_method': 'randomized', 'n_svd_vecs': None, 'mini_batch': False, 'init': 'k-means++', 'n_init': 3, 'random_state': 42, 'method': 'bistochastic', 'n_components': 6, 'n_best': 3}`
`params = {'method': 'log'}`

1. `params.items()`:
   - Yields: `[('method', 'log')]`

2. `key, value in params.items()`:
   - Iterates through each item in the `params.items()` list:
     - `key = 'method'`, `value = 'log'`

</think>

[ANSWER]
10     def set_params(self, **params):
11         if not params:
12             return self
13         valid_params = self.get_params(deep=True)
14         nested_params = defaultdict(dict)
15         for key, value in params.items():## [STATE]key=['method'][/STATE] [STATE]value=['log'][/STATE] [STATE]params.items()=[('method', 'log')][/STATE]
16             key, delim, sub_key = key.partition('__')
17             if key not in valid_params:
18                 local_valid_params = self._get_param_names()
19                 raise ValueError(f'Invalid parameter {key!r} for estimator {self}. Valid parameters are: {local_valid_params!r}.')
20             if delim:
21                 nested_params[key][sub_key] = value
22             else:
23                 setattr(self, key, value)
24                 valid_params[key] = value
25         for key, sub_params in nested_params.items():
26             valid_params[key].set_params(**sub_params)
27         return self
[/ANSWER] </s>

[FILEPATH] lumnitavz-code_debugging-8237d06/versioning/reported/2BF83FE5-96BF-4927-8674-14A06CCD589E/alpha-scheduler/_data_processor.py [/FILEPATH]
10 from data_processor import DataProcessor
11 from molde.datasets.base_dataset import SampleMetadata
12 from molde.errors import GenericError
13 from molde.settings import settings
14 from molde.utils.base_utils import chunk
15 from molde.utils.logger import setup_logging
16 
17 import asyncio
18 import json
19 from collections import defaultdict
20 from dataclasses import dataclass, field
21 from typing import Any, Dict, List, Optional, Tuple, Union
22 
23 import numpy as np
24 import pandas as pd
25 from multiprocess.pool import Pool
26 from sklearn.base import BaseEstimator, TransformerMixin
27 from sklearn.pipeline import Pipeline
28 from sklearn.preprocessing import MinMaxScaler, StandardScaler
29 
30 try:
31     from tqdm import tqdm
32 except ModuleNotFoundError:
33     tqdm = None
34 
35 from .manager import get_and_lock_batch, release_batch, update_progress
36 from .utils import fetch_batch_status, fetch_molde_paths
37 
38 log = setup_logging(__name__)
39 __all__ = ("AlphaDataSetProcessor", "AlphaVLDatasetProcessor")
40 
41 @dataclass
42 class BatchMetadata:
43     dataset_metadata: SampleMetadata
44     batch_id: Union[str, int]
45     molde_path: str
46     batch_status: dict = field(default_factory=dict)
47     cached_features: List[str] = field(default_factory=list)
48     n_samples: int = 0
49     chem_space_cache: List = field(default_factory=list)
50     batch_batch_name: str = "unlabeled"
51 
52 class AlphaDataSetProcessor(DataProcessor):
53     _dataset_name = "alpha"
54     _dataset_cols = [
55         "name", "id", "sequence", "fold", "batch_name", "molde_path",
56         "features", "dataset_metadata", "status"
57     ]
58     _has_chromatogram = False
59     _required_dtypes = {
60         "features": object,
61         "chromatogram": object
62     }
63 
64     def __init__(self,
65                  activity_column: str,
66                  affinity_column: Optional[str] = None,
67                  metadata_columns: List[str] = None,
68                  batch_metadata_columns: List[str] = None,
69                  feature_prefix: str = 'XTS_',
70                  min_feature_frequency: int = 2,
71                  standardization: bool = True,
72                  scaling_method: str = "none",
73                  scale_features: bool = False,
74                  standardize_features: bool = False,
75                  fillna: bool = False,
76                  overwrite: bool = False,
77                  progress: bool = False,
78                  verbose: bool = False,
79                  n_jobs: int = 1):
80         super().__init__()
81 
82         self.activity_column = activity_column
83         self.affinity_column = affinity_column
84         self.metadata_columns = metadata_columns
85         self.batch_metadata_columns = batch_metadata_columns
86         self.feature_prefix = feature_prefix
87         self.min_feature_frequency = min_feature_frequency
88         self.standardization = standardization
89         self.scaling_method = scaling_method
90         self.scale_features = scale_features
91         self.standardize_features = standardize_features
92         self.fillna = fillna
93         self.overwrite = overwrite
94         self.progress = progress
95         self.verbose = verbose
96         self.n_jobs = n_jobs
97         self.metadata: SampleMetadata
98 
99     def _initialize_metadata(self):
100         self.metadata = SampleMetadata(
101             name=self._dataset_name,
102             molde_version=settings.MOLDE_VERSION,
103             molde_dir=str(settings.MOLDE_BASE_DIR),
104             task="features",
105             processed_at=self._now,
106             id_column="id",
107             sequence_column="sequence",
108             activity_column=self.activity_column,
109             affinity_column=self.affinity_column,
110             metadata_columns=self.metadata_columns,
111             batch_metadata_columns=self.batch_metadata_columns,
112             progress=False
113         )
114 
115     def _get_dataset_info(self):
116         """Get information on all processed molde files in the directory."""
117         molde_paths = fetch_molde_paths(self.metadata.molde_dir)
118 
119         if not molde_paths:
120             raise GenericError(f"No Molde processed files found in directory: {self.metadata.molde_dir}")
121 
122         dataset_info = {}
123         for molde_path in molde_paths:
124             dataset_info[molde_path] = fetch_batch_status(molde_path)
125 
126         return molde_paths, dataset_info
127 
128     def _process_batch(self,
129                      batch_id: Union[int, str],
130                      metadata: SampleMetadata,
131                      dataset_info: Dict,
132                      molde_path: str):
133         """Process a single batch in parallel."""
134         try:
135             data = get_and_lock_batch(
136                 metadata.molde_dir,
137                 molde_path,
138                 batch_id=batch_id,
139                 metadata=metadata,
140                 activity_col=self.activity_column,
141                 affinity_col=self.affinity_column,
142                 dataset_cols=self._dataset_cols,
143                 min_feature_frequency=self.min_feature_frequency,
144                 has_chromatogram=False
145             )
146 
147             if not data or not data[0]:
148                 log.warning("Empty batch %s", batch_id)
149                 release_batch(batch_id, molde_path)
150                 return None
151 
152             batch_df = pd.DataFrame(data)
153 
154             if self.metadata_columns:
155                 batch_df = batch_df[self.metadata_columns + [self.activity_column]]
156             else:
157                 batch_df = batch_df[[self.activity_column]]
158 
159             if self.fillna:
160                 batch_df = batch_df.fillna(0)
161 
162             if self.scale_features:
163                 if self.standardize_features:
164                     features_scaler = StandardScaler()
165                 else:
166                     features_scaler = MinMaxScaler()
167 
168                 for col in batch_df.columns:
169                     if col.startswith(self.feature_prefix) or col.startswith("chromatogram_"):
170                         batch_df[col] = features_scaler.fit_transform(batch_df[col].values.reshape(-1, 1))
171 
172             if self.standardization:
173                 batch_df.loc[:, self.activity_column] = (batch_df[self.activity_column] - 0.5) * 2
174 
175             new_metadata = SampleMetadata(**metadata.dict())
176             new_metadata.batch_name = f"{self._dataset_name}_{batch_id}_{self.activity_column}"
177             new_metadata.id_column = "name"
178             new_metadata.sequence_column = "sequence"
179             new_metadata.id_batch_column = "id"
180             new_metadata.id_name_column = "id"
181             new_metadata.processed_at = self._now
182             new_metadata.cached_features = list(batch_df.drop([self.activity_column], axis=1).columns)
183 
184             if "features" in batch_df.columns:
185                 new_metadata.chromatogram_file = molde_path
186                 batch_df["chromatogram_file"] = molde_path
187 
188             update_progress(
189                 new_metadata,
190                 metadata=metadata,
191                 batch_id=batch_id,
192                 molde_path=molde_path,
193                 batch_df=batch_df
194             )
195 
196             if not batch_df.empty:
197                 self._dataset.append(batch_df)
198 
199             log.info("Processed batch %s", batch_id)
200             return new_metadata
201 
202         except Exception as e:
203             log.error("Error processing batch %s: %s", batch_id, str(e))
204             release_batch(batch_id, molde_path)
205             return None
206 
207     def _process_batches(self,
208                        metadata: SampleMetadata,
209                        dataset_info: Dict,
210                       molde_path: str):
211         """Process batches in parallel."""
212         batch_ids = sorted(dataset_info.keys(), key=lambda x: (isinstance(x, str), x))
213 
214         with Pool(self.n_jobs) as pool:
215             results = pool.starmap(self._process_batch, [
216                 (batch_id, metadata, dataset_info, molde_path) for batch_id in batch_ids
217             ])
218 
219         return [r for r in results if r is not None]
220 
221     def process(self) -> Tuple[BaseEstimator, pd.DataFrame, Optional[SampleMetadata]]:
222         """Process and return the dataset."""
223         self._initialize_metadata()
224         molde_paths, dataset_info = self._get_dataset_info()
225 
226         molde_paths_by_batch = defaultdict(list)
227         for molde_path, batches in dataset_info.items():
228             for batch_id in batches.keys():
229                 molde_paths_by_batch[batch_id].append(molde_path)
230 
231         sample_metadatas = []
232         for batch_id, paths in molde_paths_by_batch.items():
233             for molde_path in paths:
234                 results = self._process_batches(
235                     metadata=self.metadata,
236                     dataset_info=dataset_info[molde_path],
237                     molde_path=molde_path
238                 )
239                 sample_metadatas.extend(results)
240 
241         sample_metadatas = [s for s in sample_metadatas if s is not None]
242 
243         if not sample_metadatas:
244             raise GenericError("No batches processed successfully. Please check your dataset.")
245 
246         self._dataset = pd.concat(self._dataset, axis=0)
247         self._dataset.reset_index(drop=True, inplace=True)
248 
249         log.info("Dataset processed successfully with %d samples", len(self._dataset))
250 
251         # Convert sample_metadatas to DataFrame for easier manipulation
252         sample_metadatas_df = pd.DataFrame([sm.dict() for sm in sample_metadatas])
253 
254         # Ensure columns exist and match expected format
255         for col in ["molde_path", "chromatogram_file", "id"]:
256             if col not in sample_metadatas_df.columns:
257                 sample_metadatas_df[col] = None
258 
259         return self._pipeline, self._dataset, self.metadata, sample_metadatas_df

class AlphaVLDatasetProcessor(AlphaDataSetProcessor):
    _has_chromatogram = True
260 
261     def _get_dataset_info(self):
262         """Get information on all processed molde files in the directory."""
263         molde_paths = fetch_molde_paths(self.metadata.molde_dir)
264 
265         if not molde_paths:
266             raise GenericError(f"No Molde processed files found in directory: {self.metadata.molde_dir}")
267 
268         dataset_info = {}
269         for molde_path in molde_paths:
270             dataset_info[molde_path] = fetch_batch_status(molde_path)
271 
272         return molde_paths, dataset_info
273 
274     def _process_batch(self,
275                      batch_id: Union[int, str],
276                       metadata: SampleMetadata,
277                      dataset_info: Dict,
278                      molde_path: str):
279         """Process a single batch in parallel."""
280         try:
281             data = get_and_lock_batch(
282                 metadata.molde_dir,
283                 molde_path,
284                 batch_id=batch_id,
285                 metadata=metadata,
286                 activity_col=self.activity_column,
287                 affinity_col=self.affinity_column,
288                 dataset_cols=self._dataset_cols,
289                 min_feature_frequency=self.min_feature_frequency,
290                 has_chromatogram=True
291             )
292 
293             if not data or not data[0]:
294                 log.warning("Empty batch %s", batch_id)
295                 release_batch(batch_id, molde_path)
296                 return None
297 
298             batch_df = pd.DataFrame(data)
299 
300             if self.metadata_columns:
301                 batch_df = batch_df[self.metadata_columns + [self.activity_column]]
302             else:
303                 batch_df = batch_df[[self.activity_column]]
304 
305             if self.fillna:
306                 batch_df = batch_df.fillna(0)
307 
308             if self.scale_features:
309                 if self.standardize_features:
310                     features_scaler = StandardScaler()
311                 else:
312                     features_scaler = MinMaxScaler()
313 
314                 for col in batch_df.columns:
315                     if col.startswith(self.feature_prefix) or col.startswith("chromatogram_"):
316                         batch_df[col] = features_scaler.fit_transform(batch_df[col].values.reshape(-1, 1))
317 
318             if self.standardization:
319                 batch_df.loc[:, self.activity_column] = (batch_df[self.activity_column] - 0.5) * 2
320 
321             new_metadata = SampleMetadata(**metadata.dict())
322             new_metadata.batch_name = f"{self._dataset_name}_{batch_id}_{self.activity_column}"
323             new_metadata.id_column = "name"
324             new_metadata.sequence_column = "sequence"
325             new_metadata.id_batch_column = "id"
326             new_metadata.id_name_column = "id"
327             new_metadata.processed_at = self._now
328             new_metadata.cached_features = list(batch_df.drop([self.activity_column], axis=1).columns)
329 
330             if "features" in batch_df.columns:
331                 new_metadata.chromatogram_file = molde_path
332                 batch_df["chromatogram_file"] = molde_path
333 
334             update_progress(
335                 new_metadata,
336                 metadata=metadata,
337                 batch_id=batch_id,
338                 molde_path=molde_path,
339                 batch_df=batch_df
340             )
341 
342             if not batch_df.empty:
343                 self._dataset.append(batch_df)
344 
345             log.info("Processed batch %s", batch_id)
346             return new_metadata
347 
348         except Exception as e:
349             log.error("Error processing batch %s: %s", batch_id, str(e))
350             release_batch(batch_id, molde_path)
351             return None

The input to the method ```_process_batch``` is: 
 {
    "self": {
        "_dataset_name": "alpha",
        "_dataset_cols": [
            "name",
            "id",
            "sequence",
            "fold",
            "batch_name",
            "molde_path",
            "features",
            "dataset_metadata",
            "status"
        ],
        "_has_chromatogram": false,
        "_required_dtypes": {
            "features": "<class 'object'>",
            "chromatogram": "<class 'object'>"
        },
        "activity_column": "active",
        "affinity_column": "affinity",
        "metadata_columns": null,
        "batch_metadata_columns": null,
        "feature_prefix": "XTS_",
        "min_feature_frequency": 2,
        "standardization": true,
        "scaling_method": "none",
        "scale_features": false,
        "standardize_features": false,
        "fillna": false,
        "overwrite": false,
        "progress": false,
        "verbose": false,
        "n_jobs": 1,
        "metadata": {
            "name": "alpha",
            "molde_version": "0.8.5",
            "molde_dir": "/data2/coding/molde/molde_alpha",
            "task": "features",
            "processed_at": "2024-10-16 14:31:32.921778",
            "id_column": "id",
            "sequence_column": "sequence",
            "activity_column": "active",
            "affinity_column": "affinity",
            "metadata_columns": null,
            "batch_metadata_columns": null,
            "progress": false,
            "id_name_column": null,
            "id_batch_column": null,
            "folds": null,
            "cached_features": null,
            "chromatogram_file": null,
            "batch_name": null
        },
        "_dataset": []
    },
    "batch_id": "IC50",
    "dataset_info": {
        "2550": {
            "batch_metadata": {
                "names": null,
                "smiles": null
            },
            "cached_features": [
                "chromatogram_pos_angl_max_mean_abs",
                "chromatogram_pos_angl_max_mean",
                "chromatogram_pos_angl_min_mean_abs",
                "chromatogram_pos_angl_min_mean",
                "chromatogram_pos_angl_mean_abs",
                "chromatogram_pos_angl_mean",
                "chromatogram_pos_angl_ratio_max_mean_abs",
                "chromatogram_pos_angl_ratio_max_mean",
                "chromatogram_pos_angl_ratio_min_mean_abs",
                "chromatogram_pos_angl_ratio_min_mean",
                "chromatogram_pos_angl_ratio_mean_abs",
                "chromatogram_pos_angl_ratio_mean",
                "chromatogram_pos_min_max_ratio_mean_abs",
                "chromatogram_pos_min_max_ratio_mean",
                "chromatogram_pos_n