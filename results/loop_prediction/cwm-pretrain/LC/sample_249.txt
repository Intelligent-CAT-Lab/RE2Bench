[OUTPUT]
1 def f(s):
2     count = {}
3     for i in s:## [STATE]i=["F", "S", "A"][/STATE] [STATE]s=["FSA", "FSA", "FSA"][/STATE]
4         if i.islower():
5             count[i.lower()] = s.count(i.lower()) + count.get(i.lower(), 0)
6         else:
7             count[i.lower()] = s.count(i.upper()) + count.get(i.lower(), 0)
8     return count
[/OUTPUT]
<|assistant|>

[FILEPATH] chrizeac-STORM-9d1b29d/configs/codellama/storm_2t_ll_34b_4bits.slurm [/FILEPATH]
#!/bin/bash -l

#SBATCH -p a100
#SBATCH -t 3-00:00 # 3 days walltime limit
#SBATCH -A uwasc
#SBATCH -N 1
#SBATCH --output=/gscratch/scrubbed/chanmel/output_out_fairseq_storm_2t_ll_34b_4bits.txt # Use the command "tail -f {output-file-name} to view in real-time the progress of the job"
#SBATCH -e=/gscratch/scrubbed/chanmel/output_err_fairseq_storm_2t_ll_34b_4bits.txt
#SBATCH --gres=gpu:a100:2
#SBATCH --mem=512G

module purge
module load gcc/11.2.0

export NCCL_DEBUG=INFO
export TORCH_NCCL_BLOCKING_WAIT=1
export NCCL_IB_TIMEOUT=23

# Load your Python environment
source /gscratch/asci/chanmel/pd_venv/bin/activate

# Load your fairseq envrionment
source /gscratch/asci/chanmel/fairseq-env/bin/activate

echo "Python environement loaded"

# source /gscratch/asci/mel_chan/Projects/SucTorch-A100/enable_torchenv.sh

python3 ./process.py \
  --num-seeds 1000 \
  --max-updates 75 \
  --lr 1e-5 \
  --data-splits 0.98 \
  --config-dir "configs/dfs-hard-iq" \
  --model-name "meta-llama/Llama-2-34b-hf" \
  --checkpoint "/gscratch/asci/chanmel/Llama-2-34b-chat-hf" \
  --local-dir "dfs-hard-iq/dfs-hard-iq" \
  --tokenizer "meta-llama/Llama-2-34b-hf" \
  --download True \
  --batch-size 1 \
  --bpe "sentencepiece" \
  --max-tokens 4096 \
  --load-type "base" \
  --is-cuda-llm True \
  --instruction-mode "True" \
  --num-gpus 2 \
  --datapath "/gscratch/asci/chanmel/DFS-Dataset/data/DFS" \
  --num-workers 4 \
  --no-safe


# --data-type "metadata" \

[FILEPATH] chrizeac-STORM-9d1b29d/configs/codellama/storm_2t_ll_34b.slurm [/FILEPATH]
#!/bin/bash -l

#SBATCH -p a100
#SBATCH -t 3-00:00 # 3 days walltime limit
#SBATCH -A uwasc
#SBATCH -N 1
#SBATCH --output=/gscratch/scrubbed/chanmel/output_out_fairseq_storm_2t_ll_34b.txt # Use the command "tail -f {output-file-name} to view in real-time the progress of the job"
#SBATCH -e=/gscratch/scrubbed/chanmel/output_err_fairseq_storm_2t_ll_34b.txt
#SBATCH --gres=gpu:a100:2
#SBATCH --mem=512G

module purge
module load gcc/11.2.0

export NCCL_DEBUG=INFO
export TORCH_NCCL_BLOCKING_WAIT=1
export NCCL_IB_TIMEOUT=23

# Load your Python environment
source /gscratch/asci/chanmel/pd_venv/bin/activate

# Load your fairseq envrionment
source /gscratch/asci/chanmel/fairseq-env/bin/activate

echo "Python environement loaded"

# source /gscratch/asci/mel_chan/Projects/SucTorch-A100/enable_torchenv.sh

python3 ./process.py \
  --num-seeds 1000 \
  --max-updates 75 \
  --lr 1e-5 \
  --data-splits 0.98 \
  --config-dir "configs/dfs-hard-iq" \
  --model-name "meta-llama/Llama-2-34b-hf" \
  --checkpoint "/gscratch/asci/chanmel/Llama-2-34b-hf" \
  --local-dir "dfs-hard-iq/dfs-hard-iq" \
  --tokenizer "meta-llama/Llama-2-34b-hf" \
  --download True \
  --batch-size 1 \
  --bpe "sentencepiece" \
  --max-tokens 4096 \
  --load-type "base" \
  --is-cuda-llm True \
  --instruction-mode "True" \
  --num-gpus 2 \
  --datapath "/gscratch/asci/chanmel/DFS-Dataset/data/DFS" \
  --num-workers 4 \
  --no-safe


# --data-type "metadata" \

[FILEPATH] chrizeac-STORM-9d1b29d/configs/codellama/storm_2t_cl_34b.slurm [/FILEPATH]
#!/bin/bash -l

#SBATCH -p a100
#SBATCH -t 3-00:00 # 3 days walltime limit
#SBATCH -A uwasc
#SBATCH -N 1
#SBATCH --output=/gscratch/scrubbed/chanmel/output_out_fairseq_storm_2t_cl_34b.txt # Use the command "tail -f {output-file-name} to view in real-time the progress of the job"
#SBATCH -e=/gscratch/scrubbed/chanmel/output_err_fairseq_storm_2t_cl_34b.txt
#SBATCH --gres=gpu:a100:2
#SBATCH --mem=512G

module purge
module load gcc/11.2.0

export NCCL_DEBUG=INFO
export TORCH_NCCL_BLOCKING_WAIT=1
export NCCL_IB_TIMEOUT=23

# Load your Python environment
source /gscratch/asci/chanmel/pd_venv/bin/activate

# Load your fairseq envrionment
source /gscratch/asci/chanmel/fairseq-env/bin/activate

echo "Python environement loaded"

# source /gscratch/asci/mel_chan/Projects/SucTorch-A100/enable_torchenv.sh

python3 ./process.py \
  --num-seeds 1000 \
  --max-updates 75 \
  --lr 1e-5 \
  --data-splits 0.98 \
  --config-dir "configs/dfs-hard-iq" \
  --model-name "meta-llama/Llama-2-34b-hf" \
  --checkpoint "/gscratch/asci/chanmel/Llama-2-34b-hf" \
  --local-dir "dfs-hard-iq/dfs-hard-iq" \
  --tokenizer "meta-llama/Llama-2-34b-hf" \
  --download True \
  --batch-size 1 \
  --bpe "sentencepiece" \
  --max-tokens 4096 \
  --load-type "base" \
  --is-cuda-llm True \
  --instruction-mode "False" \
  --num-gpus 2 \
  --datapath "/gscratch/asci/chanmel/DFS-Dataset/data/DFS" \
  --num-workers 4 \
  --no-safe


# --data-type "metadata" \

[FILEPATH] chrizeac-STORM-9d1b29d/configs/codellama/storm_2t_cl_13b.slurm [/FILEPATH]
#!/bin/bash -l

#SBATCH -p a100
#SBATCH -t 3-00:00 # 3 days walltime limit
#SBATCH -A uwasc
#SBATCH -N 1
#SBATCH --output=/gscratch/scrubbed/chanmel/output_out_fairseq_storm_2t_cl_13b.txt # Use the command "tail -f {output-file-name} to view in real-time the progress of the job"
#SBATCH -e=/gscratch/scrubbed/chanmel/output_err_fairseq_storm_2t_cl_13b.txt
#SBATCH --gres=gpu:a100:2
#SBATCH --mem=512G

module purge
module load gcc/11.2.0

export NCCL_DEBUG=INFO
export TORCH_NCCL_BLOCKING_WAIT=1
export NCCL_IB_TIMEOUT=23

# Load your Python environment
source /gscratch/asci/chanmel/pd_venv/bin/activate

# Load your fairseq envrionment
source /gscratch/asci/chanmel/fairseq-env/bin/activate

echo "Python environement loaded"

# source /gscratch/asci/mel_chan/Projects/SucTorch-A100/enable_torchenv.sh

python3 ./process.py \
  --num-seeds 1000 \
  --max-updates 75 \
  --lr 1e-5 \
  --data-splits 0.98 \
  --config-dir "configs/dfs-hard-iq" \
  --model-name "meta-llama/Llama-2-13b-hf" \
  --checkpoint "/gscratch/asci/chanmel/Llama-2-13b-hf" \
  --local-dir "dfs-hard-iq/dfs-hard-iq" \
  --tokenizer "meta-llama/Llama-2-13b-hf" \
  --download True \
  --batch-size 1 \
  --bpe "sentencepiece" \
  --max-tokens 4096 \
  --load-type "base" \
  --is-cuda-llm True \
  --instruction-mode "False" \
  --num-gpus 2 \
  --datapath "/gscratch/asci/chanmel/DFS-Dataset/data/DFS" \
  --num-workers 4 \
  --no-safe


# --data-type "metadata" \

[FILEPATH] chrizeac-STORM-9d1b29d/configs/codellama/storm_2t_ll_34b_4bits_save_output.slurm [/FILEPATH]
#!/bin/bash -l

#SBATCH -p a100
#SBATCH -t 3-00:00 # 3 days walltime limit
#SBATCH -A uwasc
#SBATCH -N 1
#SBATCH --output=/gscratch/scrubbed/chanmel/output_out_fairseq_storm_2t_ll_34b_4bits_save_output.txt # Use the command "tail -f {output-file-name} to view in real-time the progress of the job"
#SBATCH -e=/gscratch/scrubbed/chanmel/output_err_fairseq_storm_2t_ll_34b_4bits_save_output.txt
#SBATCH --gres=gpu:a100:2
#SBATCH --mem=512G

module purge
module load gcc/11.2.0

export NCCL_DEBUG=INFO
export TORCH_NCCL_BLOCKING_WAIT=1
export NCCL_IB_TIMEOUT=23

# Load your Python environment
source /gscratch/asci/chanmel/pd_venv/bin/activate

# Load your fairseq envrionment
source /gscratch/asci/chanmel/fairseq-env/bin/activate

echo "Python environement loaded"

# source /gscratch/asci/mel_chan/Projects/SucTorch-A100/enable_torchenv.sh

python3 ./process.py \
  --num-seeds 1000 \
  --max-updates 75 \
  --lr 1e-5 \
  --data-splits 0.98 \
  --config-dir "configs/dfs-hard-iq" \
  --model-name "meta-llama/Llama-2-34b-hf" \
  --checkpoint "/gscratch/asci/chanmel/Llama-2-34b-chat-hf" \
  --local-dir "dfs-hard-iq/dfs-hard-iq" \
  --tokenizer "meta-llama/Llama-2-34b-hf" \
  --download True \
  --batch-size 1 \
  --bpe "sentencepiece" \
  --max-tokens 4096 \
  --load-type "base" \
  --is-cuda-llm True \
  --instruction-mode "True" \
  --num-gpus 2 \
  --datapath "/gscratch/asci/chanmel/DFS-Dataset/data/DFS" \
  --num-workers 4 \
  --no-safe \
  --save-output "True"


# --data-type "metadata" \

[FILEPATH] chrizeac-STORM-9d1b29d/configs/codellama/storm_2t_cl_34b_4bits.slurm [/FILEPATH]
#!/bin/bash -l

#SBATCH -p a100
#SBATCH -t 3-00:00 # 3 days walltime limit
#SBATCH -A uwasc
#SBATCH -N 1
#SBATCH --output=/gscratch/scrubbed/chanmel/output_out_fairseq_storm_2t_cl_34b_4bits.txt # Use the command "tail -f {output-file-name} to view in real-time the progress of the job"
#SBATCH -e=/gscratch/scrubbed/chanmel/output_err_fairseq_storm_2t_cl_34b_4bits.txt
#SBATCH --gres=gpu:a100:2
#SBATCH --mem=512G

module purge
module load gcc/11.2.0

export NCCL_DEBUG=INFO
export TORCH_NCCL_BLOCKING_WAIT=1
export NCCL_IB_TIMEOUT=23

# Load your Python environment
source /gscratch/asci/chanmel/pd_venv/bin/activate

# Load your fairseq envrionment
source /gscratch/asci/chanmel/fairseq-env/bin/activate

echo "Python environement loaded"

# source /gscratch/asci/mel_chan/Projects/SucTorch-A100/enable_torchenv.sh

python3 ./process.py \
  --num-seeds 1000 \
  --max-updates 75 \
  --lr 1e-5 \
  --data-splits 0.98 \
  --config-dir "configs/dfs-hard-iq" \
  --model-name "meta-llama/Llama-2-34b-hf" \
  --checkpoint "/gscratch/asci/chanmel/Llama-2-34b-chat-hf" \
  --local-dir "dfs-hard-iq/dfs-hard-iq" \
  --tokenizer "meta-llama/Llama-2-34b-hf" \
  --download True \
  --batch-size 1 \
  --bpe "sentencepiece" \
  --max-tokens 4096 \
  --load-type "base" \
  --is-cuda-llm True \
  --instruction-mode "False" \
  --num-gpus 2 \
  --datapath "/gscratch/asci/chanmel/DFS-Dataset/data/DFS" \
  --num-workers 4 \
  --no-safe


# --data-type "metadata" \

[FILEPATH] chrizeac-STORM-9d1b29d/configs/codellama/storm_2t_ll_13b_4bits.slurm [/FILEPATH]
#!/bin/bash -l

#SBATCH -p a100
#SBATCH -t 3-00:00 # 3 days walltime limit
#SBATCH -A uwasc
#SBATCH -N 1
#SBATCH --output=/gscratch/scrubbed/chanmel/output_out_fairseq_storm_2t_ll_13b_4bits.txt # Use the command "tail -f {output-file-name} to view in real-time the progress of the job"
#SBATCH -e=/gscratch/scrubbed/chanmel/output_err_fairseq_storm_2t_ll_13b_4bits.txt
#SBATCH --gres=gpu:a100:2
#SBATCH --mem=512G

module purge
module load gcc/11.2.0

export NCCL_DEBUG=INFO
export TORCH_NCCL_BLOCKING_WAIT=1
export NCCL_IB_TIMEOUT=23

# Load your Python environment
source /gscratch/asci/chanmel/pd_venv/bin/activate

# Load your fairseq envrionment
source /gscratch/asci/chanmel/fairseq-env/bin/activate

echo "Python environement loaded"

# source /gscratch/asci/mel_chan/Projects/SucTorch-A100/enable_torchenv.sh

python3 ./process.py \
  --num-seeds 1000 \
  --max-updates 75 \
  --lr 1e-5 \
  --data-splits 0.98 \
  --config-dir "configs/dfs-hard-iq" \
  --model-name "meta-llama/Llama-2-13b-hf" \
  --checkpoint "/gscratch/asci/chanmel/Llama-2-13b-chat-hf" \
  --local-dir "dfs-hard-iq/dfs-hard-iq" \
  --tokenizer "meta-llama/Llama-2-13b-hf" \
  --download True \
  --batch-size 1 \
  --bpe "sentencepiece" \
  --max-tokens 4096 \
  --load-type "base" \
  --is-cuda-llm True \
  --instruction-mode "True" \
  --num-gpus 2 \
  --datapath "/gscratch/asci/chanmel/DFS-Dataset/data/DFS" \
  --num-workers 4 \
  --no-safe


# --data-type "metadata" \

[FILEPATH] chrizeac-STORM-9d1b29d/configs/codellama/storm_2t_ll_13b.slurm [/FILEPATH]
#!/bin/bash -l

#SBATCH -p a100
#SBATCH -t 3-00:00 # 3 days walltime limit
#SBATCH -A uwasc
#SBATCH -N 1
#SBATCH --output=/gscratch/scrubbed/chanmel/output_out_fairseq_storm_2t_ll_13b.txt # Use the command "tail -f {output-file-name} to view in real-time the progress of the job"
#SBATCH -e=/gscratch/scrubbed/