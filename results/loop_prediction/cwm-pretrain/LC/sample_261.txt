[ANSWER]
1 def f(nums, target):
2     lows, higgs = [], []
3     for i in nums:## [STATE]i=[[12, 516, 5, 2, 3, 214, 51][/STATE] [STATE]nums=[[12, 516, 5, 2, 3, 214, 51], [12, 516, 5, 2, 3, 214, 51], [12, 516, 5, 2, 3, 214, 51], [12, 516, 5, 2, 3, 214, 51], [12, 516, 5, 2, 3, 214, 51], [12, 516, 5, 2, 3, 214, 51], [12, 516, 5, 2, 3, 214, 51]][/STATE]
4         if i < target:
5             lows.append(i)
6         else:
7             higgs.append(i)
8     lows.clear()
9     return lows, higgs
[/ANSWER]

[FILEPATH] BaiPengJack-DTA-data-3bc788f/parse_result.py [/FILEPATH]
import json
import re
import os

def read_json(file_path):
    with open(file_path, 'r') as file:
        data = json.load(file)
    return data

def clean_llm_response(response):
    # Regular expression to match everything between [ANSWER] and [/ANSWER]
    pattern = r"\[ANSWER](.*?)\[/ANSWER]"
    match = re.search(pattern, response, re.DOTALL)
    if match:
        cleaned_response = match.group(1).strip()
    else:
        cleaned_response = "Error: [ANSWER] and [/ANSWER] tags not found in the response."
    return cleaned_response

def extract_code_block(json_file):
    # Read the JSON file and extract the code block
    data = read_json(json_file)
    
    for message in data.get('messages', []):
        if message.get('role') == 'assistant':
            # Strip the JSON structure, extract only the content of the code block
            code_block = message.get('content', '').strip()
            return code_block
    
    return "Error: 'assistant' message not found in the JSON."

def update_input(json_file, input_text):
    # Read the JSON file content
    with open(json_file, 'r') as file:
        data = json.load(file)
    
    # Update the input for the "user" role
    for message in data.get('messages', []):
        if message.get('role') == 'user':
            message['content'] = input_text
            break
    
    # Write the updated JSON back to the file
    with open(json_file, 'w') as file:
        json.dump(data, file, indent=4)
        
def update_input_content(input_content_file_path, output_content_file_path, prompt_file_path):
    """
    Updates the input content using information from the output content and prompt.

    Args:
        input_content_file_path (str): Path to the input content JSON file.
        output_content_file_path (str): Path to the output content text file.
        prompt_file_path (str): Path to the prompt text file.
    """
    # Read the output content
    with open(output_content_file_path, 'r') as file:
        output_content = file.read().strip()

    # Read the prompt
    with open(prompt_file_path, 'r') as file:
        prompt_content = file.read().strip()

    # Combine the output and prompt to form the updated input content
    updated_input_content = f"{output_content}\n\n{prompt_content}"

    # Update the input file with the new content
    update_input(input_content_file_path, updated_input_content)

    # Print the updated input content to stdout for verification
    print("Updated input content:")
    print(updated_input_content)


def main():
    # Extract code blocks from JSON files
    output_path = './output'
    
    # Define the input, output, and prompt file paths
    input_content_file_path = "input/input_2.json" 
    output_content_file_path = "output/output_2.txt" 
    prompt_file_path = "prompt/prompt_2.txt"
    
    # Call the function to update the input content
    update_input_content(input_content_file_path, output_content_file_path, prompt_file_path)

    print("Input content has been updated with the content from the output and prompt.")

if __name__ == "__main__":
    main()

[FILEPATH] BaiPengJack-DTA-data-3bc788f/sweep_1.py [/FILEPATH]
from concurrent.futures import ThreadPoolExecutor
import os

import subprocess

def main():
    """
    Run the attack script multiple times with different seeds.
    """
    print('Start sweep attack...')
    with open('./seeds/sweep_1_seeds.txt', 'r') as file:
        seeds = [line.strip() for line in file.readlines()]

    attack_command = 'python attack.py --model "meta-llama/Meta-Llama-3-8B-Instruct" --save_dir 1 --dataset_name "gh-actions-data-collection"
                      --dataset_config_name "data" --dataset_split "train[::1]" --prompt_file_path ./prompt/prompt_1.txt --seed SEED'
    
    with ThreadPoolExecutor() as executor:
        results = list(executor.map(lambda seed: subprocess.run(['bash', '-c', attack_command.replace('SEED', seed)], capture_output=True, text=True), seeds))

    # Process results
    for result in results:
        if result.returncode != 0:
            print(f"Command failed with error: {result.stderr}")
        else:
            print(f"Command output: {result.stdout}")

if __name__ == "__main__":
    main()

[FILEPATH] BaiPengJack-DTA-data-3bc788f/sweep_2.py [/FILEPATH]
from concurrent.futures import ThreadPoolExecutor
import os

import subprocess

def main():
    """
    Run the attack script multiple times with different seeds.
    """
    print('Start sweep attack...')
    with open('./seeds/sweep_2_seeds.txt', 'r') as file:
        seeds = [line.strip() for line in file.readlines()]

    attack_command = 'python attack.py --model "meta-llama/Meta-Llama-3-8B-Instruct" --save_dir 2 --dataset_name "gh-actions-data-collection"
                      --dataset_config_name "data" --dataset_split "train[::1]" --prompt_file_path ./prompt/prompt_2.txt --seed SEED'
    
    with ThreadPoolExecutor() as executor:
        results = list(executor.map(lambda seed: subprocess.run(['bash', '-c', attack_command.replace('SEED', seed)], capture_output=True, text=True), seeds))

    # Process results
    for result in results:
        if result.returncode != 0:
            print(f"Command failed with error: {result.stderr}")
        else:
            print(f"Command output: {result.stdout}")

if __name__ == "__main__":
    main()

[FILEPATH] BaiPengJack-DTA-data-3bc788f/README.md [/FILEPATH]
# DTA-data
Dynamic Taint Analysis (DTA) for Program Traces: Code, Data, and Execution Paths

[FILEPATH] BaiPengJack-DTA-data-3bc788f/eval.py [/FILEPATH]
import json
import argparse
import os
import subprocess
import re

import openai
import litellm

# from tqdm import tqdm
from datasets import load_dataset

import utils

def remove_duplicates(lst):
    return list(set(lst))

# Function to extract only the list part of the string for a given key
def extract_list_from_string(lst_str):
    if not isinstance(lst_str, str):
        return lst_str
    
    list_pattern = r'\[.*?\]'
    match = re.search(list_pattern, lst_str, re.DOTALL)
    
    if match:
        lst_str = match.group()
        
    try:
        lst = eval(lst_str)
        if isinstance(lst, list):
            return lst
        else:
            print(f"Key: {lst_str}")
            raise ValueError(f"Converted value is not a list: {lst}")
    except Exception as e:
        print(f"Key: {lst_str}")
        raise ValueError(f"Error converting string to list: {lst_str}") from e
    
# Function to extract a specific key and its values from a JSON file
def extract_key_and_values_from_json(json_data, key):
    value_list = []
    for item in json_data:
        if key in item:
            #value_list.append(remove_duplicates(item[key]))
            # value_list.append(extract_list_from_string(item[key]))
            value_list.extend(extract_list_from_string(item[key]))
    return value_list

def extract_key_and_values_from_json_new(json_data, key):
    value_list = []
    for item in json_data:
        if key in item:
            #value_list.append(remove_duplicates(item[key]))
            value_list.extend(extract_list_from_string(item[key]))
    return value_list

# Function to compute the intersection of two lists of lists
def compute_intersection(list_a, list_b):
    set_a = set(tuple(lst) for lst in list_a)
    set_b = set(tuple(lst) for lst in list_b)
    return [list(intersection) for intersection in (set_a & set_b)]


def compute_intersection_2(list_a, list_b):
    return [value for value in list_a if value in list_b]

def parse_arguments():
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(description="Evaluate code snippets.")
    parser.add_argument("--save_dir", type=int, required=True, help="The directory to save evaluation results.")
    parser.add_argument("--dataset_name", type=str, default="gh-actions-data-collection", help="Name of the dataset to use.")
    parser.add_argument("--dataset_config_name", type=str, default="data", help="Configuration name for the dataset.")
    parser.add_argument("--dataset_split", type=str, default="train[::1]", help="Split of the dataset to evaluate.")
    return parser.parse_args()


def main():
    args = parse_arguments()

    # Load the dataset
    data_dict = load_dataset(args.dataset_name, args.dataset_config_name, split=args.dataset_split)

    outputs = utils.parse_files_in_directory(os.path.join(f"./output/{args.save_dir}"))

    for data in data_dict:
        sample_inputs = data['messages'][-1]['content']
        for output in outputs:
            if output['prompt'] == sample_inputs:
                data['metadata']['prediction'] = output['output']
                data['metadata']['llm_response'] = output['llm_response']
                data['metadata']['runtime'] = output['runtime']
                break

    matched_traces = []
    matched_scores = []
    un_matched_traces = []
    un_matched_scores = []
    for data in data_dict:
        input = utils.parse_code_messages(data['messages'])
        try:
            output = json.loads(data['metadata']['prediction'])
        except:
            output = None
        trace_input = utils.extract_trace(input)
        if output is None:
            score = 0
        else:
            try:
                trace_output = utils.extract_trace(output)
                intersections = compute_intersection_2(trace_input, trace_output)
                score = len(intersections) / len(trace_input)
            except:
                score = 0
        if score >= 0.5:
            matched_traces.append(trace_input)
            matched_scores.append(score)
        else:
            un_matched_traces.append(trace_input)
            un_matched_scores.append(score)

    average_matched_score = sum(matched_scores) / len(matched_scores)
    average_un_matched_score = sum(un_matched_scores) / len(un_matched_scores)

    data = {
        'average_matched_score': average_matched_score,
        'average_un_matched_score': average_un_matched_score,
        'matched_traces': matched_traces,
        'un_matched_traces': un_matched_traces,
    }

    file_path = os.path.join(f"./eval/{args.save_dir}", "eval.json")
    with open(file_path, "w") as json_file:
        json.dump(data, json_file, indent=4)

if __name__ == "__main__":
    main()

[FILEPATH] BaiPengJack-DTA-data-3bc788f/seed.py [/FILEPATH]
import random
import os
import numpy as np
import torch

def main():
    random_seeds = list(range(0, 100))
    torch_seeds = list(range(0, 100))
    np_seeds = list(range(0, 100))
    random.shuffle(random_seeds)
    random.shuffle(torch_seeds)
    random.shuffle(np_seeds)
    sweep_dir = "./seeds"
    os.makedirs(sweep_dir, exist_ok=True)
    sweep_file_path = os.path.join(sweep_dir, "sweep_seeds.txt")
    random_seeds_file_path = os.path.join(sweep_dir, "sweep_random_seeds.txt")
    torch_seeds_file_path = os.path.join(sweep_dir, "sweep_torch_seeds.txt")
    np_seeds_file_path = os.path.join(sweep_dir, "sweep_np_seeds.txt")
    with open(sweep_file_path, "w") as f:
        for seed in random_seeds:
            f.write(f"{seed}\n")
    with open(random_seeds_file_path, "w") as f:
        for seed in random_seeds:
            f.write(f"{seed}\n")
    with open(torch_seeds_file_path, "w") as f:
        for seed in torch_seeds:
            f.write(f"{seed}\n")
    with open(np_seeds_file_path, "w") as f:
        for seed in np_seeds:
            f.write(f"{seed}\n")
    print("Seeds generated successfully.")

if __name__ == "__main__":
    main()

[FILEPATH] BaiPengJack-DTA-data-3bc788f/attack.py [/FILEPATH]
import argparse
import json
import logging
import os
import re
import sys
import time
import subprocess
from collections import Counter

import openai
import litellm
import torch
import numpy as np
import random
import torch.nn.functional as F

from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, AutoModelForSeq2SeqLM, StoppingCriteria, StoppingCriteriaList, StoppingCriteriaList
from datasets import load_dataset

# from tqdm import tqdm
# import tiktoken
# from transformers import pipeline
# from sklearn.model_selection import train_test_split

import utils
import decoder

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def prompt_evaluation_func(prompts, examples, args):
    results = {}
    for example, prompt in zip(examples, prompts):
        stop_words = set(["\n\n[OUTPUT]", "[OUTPUT]"])
        try:
            llm_response = get_completion(prompt, args)
            llm_response = utils.clean_llm_response(llm_response, stop_words=stop_words)
            runtime = 1
            results[(example, prompt)] = (llm_response, runtime)
        except:
            llm_response = " "
            runtime = 2
            results[(example, prompt)] = (llm_response, runtime)
    return results

def attack_func(args):
    """A demonstration of running an attack on LLMs by our framework.

    Args:
        model (str): The model used for attack.
        initial_prompt (str): The initial prompt used for attack. Defaults to None.
        initial_prompt_path (str): The path to the file containing the initial prompt. Defaults to None.
        dataset_name (str): The name of the dataset used for attack. Defaults to "gh-actions-data-collection".
        dataset_config_name (str): The config name of the dataset used for attack. Defaults to "data".
        dataset_split (str): The split of the dataset used for attack. Defaults to "train[::1]".
        attack_epochs (int): The number of attack epochs. Defaults to 10.
        save_dir (str): The directory to save the results. Defaults to "results".
        llm_api_key (str): The API key for the LLM. Defaults to None.
        cache_dir (str): The directory to cache the model. Defaults to None.
    Returns:
        None.
    """
    # Parse arguments
    print("args:", args)
    args = utils.config_parser(args, "prompt")

    # Create output directory
    os.makedirs(os.path.join("./output", str(args.save_dir)), exist_ok=True)

    # Load the dataset
    data_dict = load_dataset(args.dataset_name, args.dataset_config_name, split=args.dataset_split)

    # Load the model and the tokenizer
    print(f"Loading the model {args.model}")
    tokenizer, model = utils.load_model_and_tokenizer(args.model)

    # Get prompts from the data_dict
    prompts = []
    for data in data_dict:
        code = utils.get_python_code(data['messages'])
        if code is None:
            continue
        prompt = utils.get_prompt(code, data['metadata']['input'], args.prompt)
        if prompt is None:
            continue
        prompts.append(prompt)
    
    # Save prompts
    if args.save_prompts:
        print("Saving prompts to the output file")
        with open(os.path.join("./output", str(args.save_dir), "prompts.txt"), "w") as f:
            for prompt in prompts:
                f.write(prompt)
                f.write("\n\n")

    # Save sample inputs
    print("Saving sample inputs to the output file")
    with open(os.path.join("./output", str(args.save_dir), "sample_inputs.json"), "w") as f:
        for data in data_dict:
            f.write(json.dumps(data['metadata']['input']))
            f.write("\n")

    # Evaluate the initial prompt
    print("Evaluating the initial prompt")
    results = prompt_evaluation_func(prompts, data_dict, args)
    if args.sweep:
        print("Executing sweep")
        best_prompts = utils.save_and_sweep(args, data_dict, results, prompts, tokenizer, model)
    else:
        print("Generating prompt with grad attack")
        best_prompts = utils.grad_attack(args, data_dict, results, prompts, tokenizer, model)
    
    # Save the best prompts to a file
    print("Saving the best prompts to the output file")
    with open(os.path.join("./output", str(args.save_dir), "best_prompts.txt"), "w") as f:
        for prompt in best_prompts:
            f.write(prompt)
            f.write("\n\n")

    # Save the results to a file
    print("Saving the results to the output file")
    file_path = os.path.join("./output", str(args.save_dir), "results.json")
    with open(file_path, "w") as f:
        for key, value in results.items():
            output_dict = {'prompt': key[1], 'output': value[0], 'runtime': value[1]}
            f.write(json.dumps(output_dict))
            f.write("\n")

def get_completion(prompt, args):
    response = decoder.generate_text(prompt, args.model, args.cache