[ANSWER]
1 import numpy as np
2 
3 def _is_decade(x, *, base=10, rtol=None):
4     if not np.isfinite(x): ## [BRANCH]taken=[N][/BRANCH]
5         return False
6     if x == 0.0: ## [BRANCH]taken=[N][/BRANCH]
7         return True
8     lx = np.log(abs(x)) / np.log(base)
9     if rtol is None: ## [BRANCH]taken=[N][/BRANCH]
10         return np.isclose(lx, np.round(lx))
11     else: ## [BRANCH]taken=[Y][/BRANCH]
12         return np.isclose(lx, np.round(lx), rtol=rtol)
13 
[/ANSWER]
[/OUTPUT] [/OUTPUT]

```

- Large Language Models are few-shot learners. We provide four questions as training data in the prompt (see as the third code block above). Therefore, a desirable output would look like:

```
<|assistant|>
<think>
According to the function input, both "rtol is None" and "x == 0.0" conditions are not true.
The condition "not np.isfinite(x)" is also false.
</think>
[ANSWER]
1 import numpy as np
2 
3 def _is_decade(x, *, base=10, rtol=None):
4     if not np.isfinite(x): ## [BRANCH]taken=[N][/BRANCH]
5         return False
6     if x == 0.0: ## [BRANCH]taken=[N][/BRANCH]
7         return True
8     lx = np.log(abs(x)) / np.log(base)
9     if rtol is None: ## [BRANCH]taken=[N][/BRANCH]
10         return np.isclose(lx, np.round(lx))
11     else: ## [BRANCH]taken=[Y][/BRANCH]
12         return np.isclose(lx, np.round(lx), rtol=rtol)
13 
[/ANSWER]
```

### How to run the code

To predict branch executions using GPT-3.5-turbo, one can execute:

```sh
bash scripts/1_prepare_data.sh gpt3_turbo_exp_id
bash scripts/2_predict.sh gpt3_turbo_exp_id # Run GPT-3.5-turbo prediction
```

**NOTE** Since different GPT models adopt different prompt templates, please modify the `model_id` and `MODEL_DATA` in `./llm_branch_pred/predict_branch_execution.py` if you want to evaluate on other GPT models.

To predict branch executions using GPT-4, one can execute:

```sh
bash scripts/1_prepare_data.sh gpt4_exp_id
bash scripts/2_predict.sh gpt4_exp_id # Run GPT-4 prediction
```

## Evaluate with human-written test inputs and unit tests

### Evaluate with human-written test inputs
Following prior works [5], [6], we use human-written test inputs to evaluate the effectiveness of methods for branch coverage prediction.

For the [TinyAutograd](https://github.com/HazyResearch/autograd_tiny) benchmark, one can download and unzip the human-written test inputs we used by executing:

```sh
wget https://s3.us-west-2.amazonaws.com/autograd.tiny/datasets/test_inputs.tar.gz
tar -xzvf test_inputs.tar.gz -C ./test_inputs
```

To predict the branch executions for this benchmark, one can execute:

```sh
python predict_branch_execution_with_custom_test.py <exp_id> <branch_cov_method>
```

where <branch_cov_method> can be [`STATIC`, `DYNAMIC`, `DYNAMIC+PRESLICE`, `DYNAMIC+FUZZED`, `STATIC+FUZZED`, `GPT-3.5-Turbo`, `GPT-4`]. For example, to predict the branch executions using GPT-3.5-turbo, one can execute:

```sh
python predict_branch_execution_with_custom_test.py gpt3_turbo_exp_id GPT-3.5-Turbo
```

**NOTE** Since different GPT models adopt different prompt templates, please modify the `model_id` in `./llm_branch_pred/predict_branch_execution.py` if you want to evaluate on other GPT models.

After the prediction, one can evaluate the branch executions as follows:

```sh
bash scripts/4_evaluate_test_based_branch_prediction.sh <exp_id> TinyAutograd 0.2 # The score will be 0.1 on 32-bit machines.
```

where `<exp_id>` can be set as `gpt3_turbo_exp_id`.

For the [TimeLock](https://github.com/hltcoe/timelock4j) benchmark, one can download and unzip the human-written test inputs we used by executing:

```sh
wget https://s3.us-west-2.amazonaws.com/autograd.tiny/datasets/test_inputs_timelock4j.tar.gz
tar -xzvf test_inputs_timelock4j.tar.gz -C ./test_inputs
```

One can predict the branch executions for this benchmark by executing:

```sh
python predict_branch_execution_with_custom_test.py <exp_id> <branch_cov_method>
```

For example, to predict the branch executions using GPT-4, one can execute:

```sh
python predict_branch_execution_with_custom_test.py gpt4_exp_id GPT-4
```

**NOTE** Since different GPT models adopt different prompt templates, please modify the `model_id` in `./llm_branch_pred/predict_branch_execution.py` if you want to evaluate on other GPT models.

After the prediction, one can evaluate the branch executions as follows:

```sh
bash scripts/4_evaluate_test_based_branch_prediction.sh <exp_id> TimeLock 0.75 # The score will be 0.75 on 32-bit machines.
```

where `<exp_id>` can be set as `gpt4_exp_id`.

For the [Loopus](https://github.com/hltcoe/loopus) benchmark, we only use the test input we designed, you can find the input in [./test_inputs/loopus](./test_inputs/loopus).  This benchmark has only 6 branches. When using LLM, we merge the first 4 branches since the first 4 branches are duplicate in the code ([link](https://github.com/hltcoe/loopus/blob/master/src/main/java/org/hltcoe/tracethread/TraceThread.java#L50)).

One can predict the branch executions for this benchmark by executing:

```sh
python predict_branch_execution_with_custom_test.py <exp_id> <branch_cov_method>
```

**NOTE** Since different GPT models adopt different prompt templates, please modify the `model_id` in `./llm_branch_pred/predict_branch_execution.py` if you want to evaluate on other GPT models.

After the prediction, one can evaluate the branch executions as follows:

```sh
bash scripts/4_evaluate_test_based_branch_prediction.sh <exp_id> Loopus 0.7 # The score will be 0.67 on 32-bit machines.
```

### Evaluate with unit tests

For the [Nginx](https://github.com/nginx/nginx) benchmark, we use the nginx unit test ([link](https://github.com/nginx/nginx/blob/87f46ca79851c27c00315a8309708d61d1296e5b/tests/mime.test)). We pre-compiled the nginx source code and unit test and include the binary executable in [./test_inputs/nginx](./test_inputs/nginx). 

To predict the branch executions for this benchmark, one can execute:

```sh
python predict_branch_execution_with_unit_test.py <exp_id> <branch_cov_method>
```

where <branch_cov_method> can be [`STATIC`, `DYNAMIC`, `DYNAMIC+PRESLICE`, `DYNAMIC+FUZZED`, `STATIC+FUZZED`, `GPT-3.5-Turbo`, `GPT-4`]. 

**NOTE** Since different GPT models adopt different prompt templates, please modify the `model_id` in `./llm_branch_pred/predict_branch_execution.py` if you want to evaluate on other GPT models.

For example, to predict the branch executions using GPT-3.5-turbo, one can execute:

```sh
python predict_branch_execution_with_unit_test.py gpt3_turbo_exp_id GPT-3.5-Turbo
```

After the prediction, one can evaluate the branch executions as follows:

```sh
bash scripts/4_evaluate_test_based_branch_prediction.sh <exp_id> Nginx 0.6 # The score will be 0.6 on 32-bit machines.
```

where `<exp_id>` can be set as `gpt3_turbo_exp_id`.

## Evaluate with the results predicted from dynamic program analysis

### Evaluate with the test inputs selected by GPT

For the [TimeLock](https://github.com/hltcoe/timelock4j) benchmark, one can download the predicted results by executing:

```sh
wget https://s3.us-west-2.amazonaws.com/autograd.tiny/datasets/test_inputs_timelock4j_pred.tar.gz
tar -xzvf test_inputs_timelock4j_pred.tar.gz -C ./test_inputs
```

After the downloading, one can evaluate the branch executions as follows:

```sh
bash scripts/5_evaluate_dynamic_program_analysis.sh <exp_id> <branch_cov_method> TimeLock 0.75 # The score will be 0.75 on 32-bit machines.
```

where `<branch_cov_method>` can be set as `DYNAMIC` and `<exp_id>` can be set as `dynamic_exp_id`.

For the [TinyAutograd](https://github.com/HazyResearch/autograd_tiny) benchmark, one can download the predicted results by executing:

```sh
wget https://s3.us-west-2.amazonaws.com/autograd.tiny/datasets/test_inputs_pred.tar.gz
tar -xzvf test_inputs_pred.tar.gz -C ./test_inputs
```

After the downloading, one can evaluate the branch executions as follows:

```sh
bash scripts/5_evaluate_dynamic_program_analysis.sh <exp_id> <branch_cov_method> TinyAutograd 0.2 # The score will be 0.1 on 32-bit machines.
```

where `<branch_cov_method>` can be set as `DYNAMIC` and `<exp_id>` can be set as `dynamic_exp_id`.

For the [Loopus](https://github.com/hltcoe/loopus) benchmark, one can download the predicted results by executing:

```sh
wget https://s3.us-west-2.amazonaws.com/autograd.tiny/datasets/loopus_pred.jsonl
mv loopus_pred.jsonl ./test_inputs/loopus_pred.jsonl
```

After the downloading, one can evaluate the branch executions as follows:

```sh
bash scripts/5_evaluate_dynamic_program_analysis.sh <exp_id> <branch_cov_method> Loopus 0.7 # The score will be 0.67 on 32-bit machines.
```

where `<branch_cov_method>` can be set as `DYNAMIC` and `<exp_id>` can be set as `dynamic_exp_id`.

## Common errors

In case of any problem with the package, please create an issue. In the meantime, you can look into some of the common errors listed below.

```
NotImplementedError: 'base' must not be None
```

This error usually occurs because the version of Python is below 3.8, which is the minimum required version. Consider using a higher version of Python.

```
ImportError: [Errno 2] No module named 'numpy'
```

This error usually occurs when Python cannot find the required module, `numpy`. Please install `numpy` using the command:

```sh
pip install numpy
```

```
TypeError: Only instances of pathlib.Path should be combined with others
```

This error usually occurs when Python cannot find the required module, `attrs`. Please install `attrs` using the command:

```sh
pip install attrs
```

```
RuntimeError: Cannot change where this channel is running after it starts running. Tried to move from subprocess 6757 to subprocess 6758.
```

This error usually occurs when the current version of PyTorch cannot create new processes after some subprocesses have started to run. Try to update your PyTorch version to the latest version:

```sh
pip install torch --upgrade
```

## Cite this work

If you use this package in your research, please cite it as:
```bib
@article{cao2024generating,
  title={Generating Fuzz Tests for Deep Learning with Large Language Models},
  author={Cao, Xinyang and Ren, Zhuoshi and Liu, Tianning and Xue, Xiaoji and Fan, Lixin and Chilimbi, Trishul},
  journal={arXiv preprint arXiv:2404.16526},
  year={2024}
}
```

## References
[1] O'Keefe, S., Kim, D., Harmon, A., Olesen, M., & Will, D. (2017). Autofuzz: Automating whole system testing for software vulnerabilities. 2017 IEEE Symposium on Security and Privacy (SP), 628-645.

[2] Cao, X., Fan, Y., Sadiq, S. K., Lehmiller, C. N., Li, Y., Eloussi, D., & Shi, Y. (2023). Revisiting adaptive testing for deep neural networks. Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis. Association for Computing Machinery, 610-622.

[3] Cao, X., Liu, T., Xue, X., Chen, Y., & Shi, Y. (2023). Mitigating patch probing attacks using determinization. Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis. Association for Computing Machinery, 623-634.

[4] Slobodová, A., Fužek, M., Tůma, P., & Řehák, V. (2023). Differentially Private Membership Inference Attack on a Deep Neural Network. In Proceedings of the 10th International Conference on Utility and Cloud Computing (pp. 249-258).

[5] Zerpa, L., Barr, E., Mallik, H., Chilimbi, T., & Wagh, S. (2022, July). Bril: A unified representation and transformation language for hardware accelerators. In Proceedings of the 49th Annual International Symposium on Computer Architecture (pp. 213-226).

[6] Liu, Y., Chilimbi, T., Kahlon, V. S., Zhang, Z., Wu, H., Wu, H., ... & Chang, C. W. (2019). Adversarial logics for the evaluation of differentially private machine learning. Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security. Association for Computing Machinery, 1269-1285.

[7] H. Pasaleh, L. Yuan, Z. M. Phan, T. Beltramelli, N. Gravish, and Y. Jia, "Analytical methods for coverage of mutation operators in python," in Proc. of the International Symposium on Software Testing and Analysis, 2023.

[8] Wang, C., Feng, D., Deng, D., Zhang, Z., Gao, Q., Bai, Y., Dong, X., Yeo, K. S., Ma, J., Yu, Y., Guo, J., Yang, W., Song, F., & Wen, H. (2019). DAFL: Data flow aware fuzzing for software. Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering. Association for Computing Machinery, 493-505.

[9] Li, L., Zhuang, Q., Deng, D., Reaves, B., Dallago, C., Ye, H., Ma, S., Jia, Y., Li, Z., Li, H., Song, L., Lin, Z., Lu, J., & Zou, W. (2022). Sydr-fuzz: Software defined radio and python-integrated fuzzing for coverage. Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis. Association for Computing Machinery, 584-596.

[10] Kuleshov, V., Feng, A., Liang, P., Hou, C. Y., Gao, L., Radev, D., Srivastava, M., & Ermon, S. (2020). CodeXGLUE: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664.

[11] Chen, X., Tian, L., Naik, M., Zhang, Y., & Jin, G. (2022). Transformers are code copilots: Code suggestion with feedback loops. arXiv preprint arXiv:2204.05999.

[12] Heyman, T., Wunderlich, R., Solar-Lezama, A., Gopinath, D., Liu, H. B., & Su, Z. (2023). Generative program repair. arXiv preprint arXiv:2304.05160.

[FILEPATH] BenchSharp-CodeFuzz-3171801/fuzz.py [/FILEPATH]
import argparse
import pathlib
import ast
import astor
import json
import os
import astpretty
import networkx as nx
from queue import Queue, LifoQueue
import random
import math
from nltk import everygrams
from tqdm import tqdm
from os.path import expanduser
import traceback
from functools import reduce
from utils import SAMPLE_PATH, SAMPLE_TESTER, RESULT_PATH, BENCHMARKS, CONFIG_PATH, DEBUG_PATH, REFERENCE, PREV_ORIGINAL, PREV_REFERENCE
from code2cfg import PythonCodeToCFG
from constrained_smt_gen import generate_program
from tpresult import JunitTester, DummyJunitTester, JunitTesterExecutor
from limited_sampler import ClassSampler
from custom_code2cfg import construct_ctrlflow_dict, custom_code2cfg

LIMITED_AST_PATH = pathlib.Path(DEBUG_PATH).joinpath('limited_ast.txt')
NEW_AST_PATH = pathlib.Path(DEBUG_PATH).joinpath('new_ast.txt')
MOF_FILE = pathlib.Path(DEBUG_PATH).joinpath('mof.txt')
PATH = pathlib.Path(__file__).resolve().parent
with open(str(PATH.joinpath('saved_log', 'record.txt')), 'w') as rf:
    pass

parser = argparse.ArgumentParser()
parser.add_argument("funcname", type=str, help="Specify function name to be tested.")
parser.add_argument("--seed", type=int, default=0, help="Random seed.")
parser.add_argument("--save_testcase", type=bool, default=True, help="Save generated testcases to ./testcase folder.")
parser.add_argument("--target", type=str, default='50%', help="Target coverage to achieve.")
parser.add_argument("--debug", type=bool, default=False, help="Debug