[/ANSWER]

[FILEPATH] amanmishras2-Learned_to_Branch-d102ab7/logic/OCT/function/run_logic.py [/FILEPATH]
import csv
import math
from collections import Counter

from tqdm import tqdm

from autotrace import AutoTrace
from reasoning import Reasoning

debug_output = False

dataset = "/home/scratch.shakilat.ami2/mshmimu/MoE_Auto_Trace/OCT_data/parseeot.csv"


def read_data(path: str) -> list:
    with open(path, "r") as csvfile:
        reader = csv.reader(csvfile)
        return [row for row in reader]


def convert_types(pretask: list) -> list:
    pretask1 = pretask
    for i in range(len(pretask[0])):
        if pretask[0][i] == "None":
            pretask1[0][i] = None
        elif pretask[0][i] == "S.true":
            pretask1[0][i] = "S.true"
        elif pretask[0][i] == "S.false":
            pretask1[0][i] = "S.false"
        elif pretask[0][i] == "inf":
            pretask1[0][i] = math.inf
        elif pretask[0][i] == "-inf":
            pretask1[0][i] = -math.inf
        else:
            try:
                pretask1[0][i] = int(pretask[0][i])
            except ValueError:
                try:
                    pretask1[0][i] = float(pretask[0][i])
                except ValueError:
                    pretask1[0][i] = pretask[0][i]
    return pretask1


def predict(branch_annotation, answer):

    if answer == "Y":
        value, count = Counter(branch_annotation).most_common(1)[0]
        return value
    else:
        for branch in branch_annotation:
            if "N" in branch:
                return "N"
            else:
                return "Y"


def main(dataset):
    _autoTrace = AutoTrace(debug_output)
    _reasoning = Reasoning()
    result = {}
    csv_data = read_data(dataset)
    correct = 0
    wrong = 0
    correct_0 = 0
    correct_1 = 0
    correct_2 = 0
    correct_3 = 0
    correct_4 = 0
    correct_5 = 0
    correct_6 = 0
    correct_7 = 0
    wrong_0 = 0
    wrong_1 = 0
    wrong_2 = 0
    wrong_3 = 0
    wrong_4 = 0
    wrong_5 = 0
    wrong_6 = 0
    wrong_7 = 0
    attempt_0 = 0
    attempt_1 = 0
    attempt_2 = 0
    attempt_3 = 0
    attempt_4 = 0
    attempt_5 = 0
    attempt_6 = 0
    attempt_7 = 0
    attempt_8 = 0
    attempt_9 = 0
    attempt_10 = 0
    attempt_11 = 0

    for j in tqdm(range(1, 17)):
        if j == 6 or j == 13:
            continue
        counter = 0
        attempt = 0
        max_attempt = 11
        while attempt < max_attempt:
            task_id = str(j).zfill(2) + "_" + str(counter).zfill(4)
            attempt += 1
            if attempt == 1:
                attempt_0 += 1
            elif attempt == 2:
                attempt_1 += 1
            elif attempt == 3:
                attempt_2 += 1
            elif attempt == 4:
                attempt_3 += 1
            elif attempt == 5:
                attempt_4 += 1
            elif attempt == 6:
                attempt_5 += 1
            elif attempt == 7:
                attempt_6 += 1
            elif attempt == 8:
                attempt_7 += 1
            elif attempt == 9:
                attempt_8 += 1
            elif attempt == 10:
                attempt_9 += 1
            elif attempt == 11:
                attempt_10 += 1
            else:
                attempt_11 += 1

            correct_result = csv_data[counter]

            code = correct_result[3].replace("\\t", "")
            function_name = correct_result[4]
            input = correct_result[5]
            if len(correct_result) == 7:
                output = correct_result[6]
            else:
                output = "[]"

            autoTrace_prompt = _autoTrace.generate_prompt(
                j, code, function_name, input
            )
            autoTrace_resp = _autoTrace.get_autoTrace(
                prompt=autoTrace_prompt, task_id=task_id, temperature=0
            )
            if autoTrace_resp == "ERROR":
                wrong += 1
                continue
            autoTrace_resp = _autoTrace.annotate_branches(autoTrace_resp, task_id)
            if not autoTrace_resp:
                wrong += 1
                continue

            reasoning_prompt = _reasoning.generate_reasoning_prompt(
                j, code, input, output, autoTrace_resp
            )
            if len(autoTrace_resp) > 2000:
                # print(autoTrace_resp)
                raise ValueError("code too long")
            print(f"Number of branches: {len(autoTrace_resp)}")
            print(f"Number of attempts: {attempt}")
            print(f"Number of tokens: {len(autoTrace_prompt)}")
            print(f"Number of branches: {len(autoTrace_resp)}")

            gpt_resp = _reasoning.get_reasoning(prompt=reasoning_prompt, temperature=0)
            if gpt_resp == "ERROR":
                wrong += 1
                continue

            correct_pred = _autoTrace.add_branch_output(correct_result)
            correct_pred = convert_types(correct_pred)
            predicted_branches = _autoTrace.add_branch_output(gpt_resp)
            predicted_branches = convert_types(predicted_branches)

            if len(predicted_branches) == 0:
                wrong += 1
                continue

            for i in range(len(correct_pred)):
                correct_pred[i].sort()
                predicted_branches[i].sort()
                # compare two arrays ignoring None or other values in between
                if correct_pred[i] == predicted_branches[i] or (
                    "None" in predicted_branches[i]
                    and correct_pred[i]
                    == [x for x in predicted_branches[i] if x != "None"]
                ):
                    correct += 1
                    if j == 1:
                        correct_0 += 1
                    elif j == 2:
                        correct_1 += 1
                    elif j == 3:
                        correct_2 += 1
                    elif j == 4:
                        correct_3 += 1
                    elif j == 5:
                        correct_4 += 1
                    elif j == 7:
                        correct_5 += 1
                    elif j == 8:
                        correct_6 += 1
                    elif j == 9:
                        correct_7 += 1
                    else:
                        correct_1 += 1
                else:
                    wrong += 1
                    break
            if correct_pred != predicted_branches:
                break
            else:
                counter += 1
                correct_result[3] = code
                correct_result[4] = function_name
                correct_result[5] = input
                if len(correct_result) == 7:
                    correct_result[6] = output
                else:
                    correct_result.append(output)
                if len(correct_result) == 8:
                    correct_result[7] = gpt_resp[2]
                else:
                    correct_result.append(gpt_resp[2])
                result[counter - 1] = correct_result

        print(f"Task: {j}, correct: {correct}")
        print(
            f"Task: {j}, correct rate: {correct / (correct + wrong)}, correct: {correct}, wrong: {wrong}"
        )

    print(f"Total correct: {correct}")
    print(
        f"Total correct rate: {correct / (correct + wrong)}, correct: {correct}, wrong: {wrong}"
    )
    # save to csv
    with open("logic_result.csv", "w") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["branch", "pred_0", "pred_1", "pred_2", "pred_3", "pred_4"])
        writer.writerow(["0_0", result.get(0, ["0_0", [], [], [], [], []])[6:]])
        writer.writerow(["0_1", result.get(1, ["0_1", [], [], [], [], []])[6:]])
        writer.writerow(["0_2", result.get(2, ["0_2", [], [], [], [], []])[6:]])
        writer.writerow(["0_3", result.get(3, ["0_3", [], [], [], [], []])[6:]])
        writer.writerow(["0_4", result.get(4, ["0_4", [], [], [], [], []])[6:]])
        writer.writerow(["1_0", result.get(5, ["1_0", [], [], [], [], []])[6:]])
        writer.writerow(["1_1", result.get(6, ["1_1", [], [], [], [], []])[6:]])
        writer.writerow(["1_2", result.get(7, ["1_2", [], [], [], [], []])[6:]])
        writer.writerow(["2_0", result.get(8, ["2_0", [], [], [], [], []])[6:]])
        writer.writerow(["2_1", result.get(9, ["2_1", [], [], [], [], []])[6:]])
        writer.writerow(["2_2", result.get(10, ["2_2", [], [], [], [], []])[6:]])
        writer.writerow(["3_0", result.get(11, ["3_0", [], [], [], [], []])[6:]])
        writer.writerow(["3_1", result.get(12, ["3_1", [], [], [], [], []])[6:]])
        writer.writerow(["3_2", result.get(13, ["3_2", [], [], [], [], []])[6:]])
        writer.writerow(["4_0", result.get(14, ["4_0", [], [], [], [], []])[6:]])
        writer.writerow(["4_1", result.get(15, ["4_1", [], [], [], [], []])[6:]])
        writer.writerow(["4_2", result.get(16, ["4_2", [], [], [], [], []])[6:]])
        writer.writerow(["4_3", result.get(17, ["4_3", [], [], [], [], []])[6:]])
        writer.writerow(["4_4", result.get(18, ["4_4", [], [], [], [], []])[6:]])
        writer.writerow(["5_0", result.get(19, ["5_0", [], [], [], [], []])[6:]])
        writer.writerow(["5_1", result.get(20, ["5_1", [], [], [], [], []])[6:]])
        writer.writerow(["5_2", result.get(21, ["5_2", [], [], [], [], []])[6:]])
        writer.writerow(["6_0", result.get(22, ["6_0", [], [], [], [], []])[6:]])
        writer.writerow(["6_1", result.get(23, ["6_1", [], [], [], [], []])[6:]])
        writer.writerow(["6_2", result.get(24, ["6_2", [], [], [], [], []])[6:]])
        writer.writerow(["7_0", result.get(25, ["7_0", [], [], [], [], []])[6:]])
        writer.writerow(["7_1", result.get(26, ["7_1", [], [], [], [], []])[6:]])
        writer.writerow(["7_2", result.get(27, ["7_2", [], [], [], [], []])[6:]])
        writer.writerow(["8_0", result.get(28, ["8_0", [], [], [], [], []])[6:]])
        writer.writerow(["8_1", result.get(29, ["8_1", [], [], [], [], []])[6:]])
        writer.writerow(["8_2", result.get(30, ["8_2", [], [], [], [], []])[6:]])
        writer.writerow(["9_0", result.get(31, ["9_0", [], [], [], [], []])[6:]])
        writer.writerow(["9_1", result.get(32, ["9_1", [], [], [], [], []])[6:]])
        writer.writerow(["10_0", result.get(33, ["10_0", [], [], [], [], []])[6:]])
        writer.writerow(["10_1", result.get(34, ["10_1", [], [], [], [], []])[6:]])
        writer.writerow(["11_0", result.get(35, ["11_0", [], [], [], [], []])[6:]])
        writer.writerow(["11_1", result.get(36, ["11_1", [], [], [], [], []])[6:]])
        writer.writerow(["12_0", result.get(37, ["12_0", [], [], [], [], []])[6:]])
        writer.writerow(["12_1", result.get(38, ["12_1", [], [], [], [], []])[6:]])
        writer.writerow(["13_0", result.get(39, ["13_0", [], [], [], [], []])[6:]])
        writer.writerow(["13_1", result.get(40, ["13_1", [], [], [], [], []])[6:]])
        writer.writerow(["14_0", result.get(41, ["14_0", [], [], [], [], []])[6:]])
        writer.writerow(["15_0", result.get(42, ["15_0", [], [], [], [], []])[6:]])
        writer.writerow(["16_0", result.get(43, ["16_0", [], [], [], [], []])[6:]])


if __name__ == "__main__":
    main(dataset)

[FILEPATH] amanmishras2-Learned_to_Branch-d102ab7/logic/OCT/autotrace.py [/FILEPATH]
import csv
import json
import os
import openai
import traceback

debug_output = False
FUNCTION = "get_answer_from_o1_preview"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


class AutoTrace:
    def __init__(self, debug_output):
        self.debug_output = debug_output
        self.api_key = OPENAI_API_KEY

    def _validate_json(self, json_data):
        try:
            json.loads(json_data)
        except json.JSONDecodeError as e:
            raise ValueError("Invalid JSON: {}".format(e))

    def _safe_response(self, response):
        try:
            return response
        except Exception as e:
            print(f"Error in processing response: {e}")
            return "ERROR"

    def read_data(self, path: str) -> list:
        with open(path, "r") as csvfile:
            reader = csv.reader(csvfile)
            return [row for row in reader]

    def get_self_invoke(self, prompt: str) -> str:
        print(f"Generating AutoTrace using function {FUNCTION}...")

        response = openai.chat.completions.create(
            model="o1-preview",
            messages=[
                {
                    "role": "user",
                    "content": prompt,
                }
            ],
        )
        return response.choices[0].message.content

    def get_autoTrace(self, prompt: str, task_id: str, temperature: float = 0) -> str:
        print("Generating AutoTrace using GPT-4o...")
        while True:
            try:
                if FUNCTION == "get_answer_from_o1_preview":
                    response = self.get_self_invoke(prompt=prompt)
                response = self._safe_response(response)
                print(f"AutoTrace response: {response}")
                return response
            except Exception as e:
                print(f"Error: {e}. Retrying...")
                print(traceback.format_exc())
                if temperature >= 1.5:
                    break
                temperature += 0.5

    def generate_prompt(self, j: int, code: str, function_name: str, input: str) -> str:
        new_prompt = f"""Here is the python code which needs to be debugged:
[PYTHON]
{code}
[/PYTHON]

functions: {function_name}
Method Inputs: {input}

Detect all the branches in the code. To do that annotate the code with comment tags. Assume that each function may or may not be called in the code. When you are predicting branch output, you need to assume that function call will always be successful if it is being called in the code. 

Branch output is predicted using '?' and '=' as placeholders. The model should annotate the branches with a comment. Here is the format for comment:
[BRANCH]output=?=[/BRANCH]

The format of branch annotation is as follows:

if condition:
 # [BRANCH]output=?=[/BRANCH]
  # code

elif condition2:
 # [BRANCH]output=?=[/BRANCH]
  # code

else:
 # [BRANCH]output=?=[/BRANCH]
  # code

Do not assume that functions are being called in the code. Predict output for each branch in the code. Do not predict the output for the function calls in the code. This is very important! 

Branches without any subbranches should be annotated like following: # [BRANCH]output=?=[/BRANCH]

When loops are annotated, the model should ensure that they are annotated correctly. For example, for loop, it should be annotated with # [BRANCH]output=?=N[/BRANCH] if it is not executed, or # [BRANCH]output=?=Y_1[/BRANCH] if it is executed once, or # [BRANCH]output=?=Y_2[/BRANCH] if it is executed twice, and so on. The model should not annotate the loop with # [BRANCH]output=?=[/BRANCH].

Your only task is to annotate branches and subbranches in the code, and not the classes, functions, or modules.
Do not include any function calls inside the branches. The function calls should be included in the input. 
Do not change the function body, arguments, or include any new lines, only annotate the branches. 
Do not include any comments in the output.
Do not change the input or code."""
        return new_prompt

    def replace_branch(self, data: str, start: str, end: str, output: str) -> str:
        pos_start = data.find(start)
        pos_end = data.find(end)
        new_data = data[:pos_start] + output + data[pos_end:]
        return new_data

    def annotate_branches(self, resp: str, task_id: str):
       