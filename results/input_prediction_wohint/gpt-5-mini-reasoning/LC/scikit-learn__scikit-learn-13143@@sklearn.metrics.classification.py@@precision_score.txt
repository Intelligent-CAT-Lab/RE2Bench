[THOUGHT]
We need inputs to precision_score that produce precision = 1.0.

precision_score returns the precision value computed as tp / (tp + fp) for the positive class when average='binary' (the default). To get precision 1.0 we need zero false positives (fp = 0) and at least one true positive (tp > 0).

A minimal valid choice is:
- y_true = [1, 1]
- y_pred = [1, 1]

Here tp = 2, fp = 0, so precision = 2 / (2 + 0) = 1.0. These inputs are consistent with the checks in precision_recall_fscore_support/_check_targets and multilabel_confusion_matrix, so the function returns 1.0.
[/THOUGHT]

[INPUT]
```
{
    "y_true": [1, 1],
    "y_pred": [1, 1]
}
```