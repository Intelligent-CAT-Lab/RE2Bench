{"types": {}, "values": {"inputs": {"self": {"input": "content", "encoding": "utf-8", "decode_error": "strict", "strip_accents": "ascii", "preprocessor": null, "tokenizer": "<function split_tokenize at 0x70e60f77ae80>", "analyzer": "word", "lowercase": true, "token_pattern": "(?u)\\b\\w\\w+\\b", "stop_words": null, "max_df": 1.0, "min_df": 1, "max_features": null, "ngram_range": [1, 1], "vocabulary": null, "binary": false, "dtype": "<class 'numpy.int64'>"}, "args": {}, "kwargs": {}}, "return": "functools.partial(<function _analyze at 0x70e6145868e0>, ngrams=<bound method _VectorizerMixin._word_ngrams of CountVectorizer(strip_accents='ascii',\n                tokenizer=<function split_tokenize at 0x70e60f77ae80>)>, tokenizer=<function split_tokenize at 0x70e60f77ae80>, preprocessor=functools.partial(<function _preprocess at 0x70e614586840>, accent_function=<function strip_accents_ascii at 0x70e614587060>, lower=True), decoder=<bound method _VectorizerMixin.decode of CountVectorizer(strip_accents='ascii',\n                tokenizer=<function split_tokenize at 0x70e60f77ae80>)>, stop_words=None)"}, "name": "build_analyzer"}
