{"types": {}, "values": {"inputs": {"self": {"input": "content", "encoding": "utf-8", "decode_error": "strict", "strip_accents": "ascii", "preprocessor": null, "tokenizer": "<function split_tokenize at 0x70e60f77ae80>", "analyzer": "word", "lowercase": true, "token_pattern": "(?u)\\b\\w\\w+\\b", "stop_words": null, "max_df": 1.0, "min_df": 1, "max_features": null, "ngram_range": [1, 1], "vocabulary": null, "binary": false, "dtype": "<class 'numpy.int64'>"}, "args": {}, "kwargs": {}}, "return": "<function split_tokenize at 0x70e60f77ae80>"}, "name": "build_tokenizer"}
