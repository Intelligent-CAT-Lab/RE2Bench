{
    "scikit-learn.sklearn.feature_extraction.text._make_int_array": "def _make_int_array():\n    \"\"\"Construct an array.array of a type suitable for scipy.sparse indices.\"\"\"\n    return array.array(str(\"i\"))",
    "scikit-learn.sklearn.feature_extraction.text.build_analyzer": "def build_analyzer(self):\n    \"\"\"Return a callable to process input data.\n\n    The callable handles preprocessing, tokenization, and n-grams generation.\n\n    Returns\n    -------\n    analyzer: callable\n        A function to handle preprocessing, tokenization\n        and n-grams generation.\n    \"\"\"\n\n    if callable(self.analyzer):\n        return partial(_analyze, analyzer=self.analyzer, decoder=self.decode)\n\n    preprocess = self.build_preprocessor()\n\n    if self.analyzer == \"char\":\n        return partial(\n            _analyze,\n            ngrams=self._char_ngrams,\n            preprocessor=preprocess,\n            decoder=self.decode,\n        )\n\n    elif self.analyzer == \"char_wb\":\n        return partial(\n            _analyze,\n            ngrams=self._char_wb_ngrams,\n            preprocessor=preprocess,\n            decoder=self.decode,\n        )\n\n    elif self.analyzer == \"word\":\n        stop_words = self.get_stop_words()\n        tokenize = self.build_tokenizer()\n        self._check_stop_words_consistency(stop_words, preprocess, tokenize)\n        return partial(\n            _analyze,\n            ngrams=self._word_ngrams,\n            tokenizer=tokenize,\n            preprocessor=preprocess,\n            decoder=self.decode,\n            stop_words=stop_words,\n        )\n\n    else:\n        raise ValueError(\n            \"%s is not a valid tokenization scheme/analyzer\" % self.analyzer\n        )",
    "scikit-learn.sklearn.feature_extraction.text._analyze": "def _analyze(\n    doc,\n    analyzer=None,\n    tokenizer=None,\n    ngrams=None,\n    preprocessor=None,\n    decoder=None,\n    stop_words=None,\n):\n    \"\"\"Chain together an optional series of text processing steps to go from\n    a single document to ngrams, with or without tokenizing or preprocessing.\n\n    If analyzer is used, only the decoder argument is used, as the analyzer is\n    intended to replace the preprocessor, tokenizer, and ngrams steps.\n\n    Parameters\n    ----------\n    analyzer: callable, default=None\n    tokenizer: callable, default=None\n    ngrams: callable, default=None\n    preprocessor: callable, default=None\n    decoder: callable, default=None\n    stop_words: list, default=None\n\n    Returns\n    -------\n    ngrams: list\n        A sequence of tokens, possibly with pairs, triples, etc.\n    \"\"\"\n\n    if decoder is not None:\n        doc = decoder(doc)\n    if analyzer is not None:\n        doc = analyzer(doc)\n    else:\n        if preprocessor is not None:\n            doc = preprocessor(doc)\n        if tokenizer is not None:\n            doc = tokenizer(doc)\n        if ngrams is not None:\n            if stop_words is not None:\n                doc = ngrams(doc, stop_words)\n            else:\n                doc = ngrams(doc)\n    return doc"
}