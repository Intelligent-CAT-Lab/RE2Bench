{
    "scikit-learn.sklearn.feature_selection._mutual_info._compute_mi_cc": "def _compute_mi_cc(x, y, n_neighbors):\n    \"\"\"Compute mutual information between two continuous variables.\n\n    Parameters\n    ----------\n    x, y : ndarray, shape (n_samples,)\n        Samples of two continuous random variables, must have an identical\n        shape.\n\n    n_neighbors : int\n        Number of nearest neighbors to search for each point, see [1]_.\n\n    Returns\n    -------\n    mi : float\n        Estimated mutual information in nat units. If it turned out to be\n        negative it is replaced by 0.\n\n    Notes\n    -----\n    True mutual information can't be negative. If its estimate by a numerical\n    method is negative, it means (providing the method is adequate) that the\n    mutual information is close to 0 and replacing it by 0 is a reasonable\n    strategy.\n\n    References\n    ----------\n    .. [1] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\n           information\". Phys. Rev. E 69, 2004.\n    \"\"\"\n    n_samples = x.size\n\n    x = x.reshape((-1, 1))\n    y = y.reshape((-1, 1))\n    xy = np.hstack((x, y))\n\n    # Here we rely on NearestNeighbors to select the fastest algorithm.\n    nn = NearestNeighbors(metric=\"chebyshev\", n_neighbors=n_neighbors)\n\n    nn.fit(xy)\n    radius = nn.kneighbors()[0]\n    radius = np.nextafter(radius[:, -1], 0)\n\n    # KDTree is explicitly fit to allow for the querying of number of\n    # neighbors within a specified radius\n    kd = KDTree(x, metric=\"chebyshev\")\n    nx = kd.query_radius(x, radius, count_only=True, return_distance=False)\n    nx = np.array(nx) - 1.0\n\n    kd = KDTree(y, metric=\"chebyshev\")\n    ny = kd.query_radius(y, radius, count_only=True, return_distance=False)\n    ny = np.array(ny) - 1.0\n\n    mi = (\n        digamma(n_samples)\n        + digamma(n_neighbors)\n        - np.mean(digamma(nx + 1))\n        - np.mean(digamma(ny + 1))\n    )\n\n    return max(0, mi)",
    "scikit-learn.sklearn.feature_selection._mutual_info._compute_mi_cd": "def _compute_mi_cd(c, d, n_neighbors):\n    \"\"\"Compute mutual information between continuous and discrete variables.\n\n    Parameters\n    ----------\n    c : ndarray, shape (n_samples,)\n        Samples of a continuous random variable.\n\n    d : ndarray, shape (n_samples,)\n        Samples of a discrete random variable.\n\n    n_neighbors : int\n        Number of nearest neighbors to search for each point, see [1]_.\n\n    Returns\n    -------\n    mi : float\n        Estimated mutual information in nat units. If it turned out to be\n        negative it is replaced by 0.\n\n    Notes\n    -----\n    True mutual information can't be negative. If its estimate by a numerical\n    method is negative, it means (providing the method is adequate) that the\n    mutual information is close to 0 and replacing it by 0 is a reasonable\n    strategy.\n\n    References\n    ----------\n    .. [1] B. C. Ross \"Mutual Information between Discrete and Continuous\n       Data Sets\". PLoS ONE 9(2), 2014.\n    \"\"\"\n    n_samples = c.shape[0]\n    c = c.reshape((-1, 1))\n\n    radius = np.empty(n_samples)\n    label_counts = np.empty(n_samples)\n    k_all = np.empty(n_samples)\n    nn = NearestNeighbors()\n    for label in np.unique(d):\n        mask = d == label\n        count = np.sum(mask)\n        if count > 1:\n            k = min(n_neighbors, count - 1)\n            nn.set_params(n_neighbors=k)\n            nn.fit(c[mask])\n            r = nn.kneighbors()[0]\n            radius[mask] = np.nextafter(r[:, -1], 0)\n            k_all[mask] = k\n        label_counts[mask] = count\n\n    # Ignore points with unique labels.\n    mask = label_counts > 1\n    n_samples = np.sum(mask)\n    label_counts = label_counts[mask]\n    k_all = k_all[mask]\n    c = c[mask]\n    radius = radius[mask]\n\n    kd = KDTree(c)\n    m_all = kd.query_radius(c, radius, count_only=True, return_distance=False)\n    m_all = np.array(m_all)\n\n    mi = (\n        digamma(n_samples)\n        + np.mean(digamma(k_all))\n        - np.mean(digamma(label_counts))\n        - np.mean(digamma(m_all))\n    )\n\n    return max(0, mi)",
    "scikit-learn.sklearn.utils._param_validation.wrapper": "@functools.wraps(func)\ndef wrapper(*args, **kwargs):\n    global_skip_validation = get_config()[\"skip_parameter_validation\"]\n    if global_skip_validation:\n        return func(*args, **kwargs)\n\n    func_sig = signature(func)\n\n    # Map *args/**kwargs to the function signature\n    params = func_sig.bind(*args, **kwargs)\n    params.apply_defaults()\n\n    # ignore self/cls and positional/keyword markers\n    to_ignore = [\n        p.name\n        for p in func_sig.parameters.values()\n        if p.kind in (p.VAR_POSITIONAL, p.VAR_KEYWORD)\n    ]\n    to_ignore += [\"self\", \"cls\"]\n    params = {k: v for k, v in params.arguments.items() if k not in to_ignore}\n\n    validate_parameter_constraints(\n        parameter_constraints, params, caller_name=func.__qualname__\n    )\n\n    try:\n        with config_context(\n            skip_parameter_validation=(\n                prefer_skip_nested_validation or global_skip_validation\n            )\n        ):\n            return func(*args, **kwargs)\n    except InvalidParameterError as e:\n        # When the function is just a wrapper around an estimator, we allow\n        # the function to delegate validation to the estimator, but we replace\n        # the name of the estimator by the name of the function in the error\n        # message to avoid confusion.\n        msg = re.sub(\n            r\"parameter of \\w+ must be\",\n            f\"parameter of {func.__qualname__} must be\",\n            str(e),\n        )\n        raise InvalidParameterError(msg) from e"
}