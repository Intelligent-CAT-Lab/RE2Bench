{
    "scikit-learn.sklearn.decomposition._nmf._check_w_h": "def _check_w_h(self, X, W, H, update_H):\n    \"\"\"Check W and H, or initialize them.\"\"\"\n    n_samples, n_features = X.shape\n\n    if self.init == \"custom\" and update_H:\n        _check_init(H, (self._n_components, n_features), \"NMF (input H)\")\n        _check_init(W, (n_samples, self._n_components), \"NMF (input W)\")\n        if self._n_components == \"auto\":\n            self._n_components = H.shape[0]\n\n        if H.dtype != X.dtype or W.dtype != X.dtype:\n            raise TypeError(\n                \"H and W should have the same dtype as X. Got \"\n                \"H.dtype = {} and W.dtype = {}.\".format(H.dtype, W.dtype)\n            )\n\n    elif not update_H:\n        if W is not None:\n            warnings.warn(\n                \"When update_H=False, the provided initial W is not used.\",\n                RuntimeWarning,\n            )\n\n        _check_init(H, (self._n_components, n_features), \"NMF (input H)\")\n        if self._n_components == \"auto\":\n            self._n_components = H.shape[0]\n\n        if H.dtype != X.dtype:\n            raise TypeError(\n                \"H should have the same dtype as X. Got H.dtype = {}.\".format(\n                    H.dtype\n                )\n            )\n\n        # 'mu' solver should not be initialized by zeros\n        if self.solver == \"mu\":\n            avg = np.sqrt(X.mean() / self._n_components)\n            W = np.full((n_samples, self._n_components), avg, dtype=X.dtype)\n        else:\n            W = np.zeros((n_samples, self._n_components), dtype=X.dtype)\n\n    else:\n        if W is not None or H is not None:\n            warnings.warn(\n                (\n                    \"When init!='custom', provided W or H are ignored. Set \"\n                    \" init='custom' to use them as initialization.\"\n                ),\n                RuntimeWarning,\n            )\n\n        if self._n_components == \"auto\":\n            self._n_components = X.shape[1]\n\n        W, H = _initialize_nmf(\n            X, self._n_components, init=self.init, random_state=self.random_state\n        )\n\n    return W, H",
    "scikit-learn.sklearn.decomposition._nmf._check_params": "def _check_params(self, X):\n    super()._check_params(X)\n\n    # batch_size\n    self._batch_size = min(self.batch_size, X.shape[0])\n\n    # forget_factor\n    self._rho = self.forget_factor ** (self._batch_size / X.shape[0])\n\n    # gamma for Maximization-Minimization (MM) algorithm [Fevotte 2011]\n    if self._beta_loss < 1:\n        self._gamma = 1.0 / (2.0 - self._beta_loss)\n    elif self._beta_loss > 2:\n        self._gamma = 1.0 / (self._beta_loss - 1.0)\n    else:\n        self._gamma = 1.0\n\n    # transform_max_iter\n    self._transform_max_iter = (\n        self.max_iter\n        if self.transform_max_iter is None\n        else self.transform_max_iter\n    )\n\n    return self",
    "scikit-learn.sklearn.decomposition._nmf._solve_W": "def _solve_W(self, X, H, max_iter):\n    \"\"\"Minimize the objective function w.r.t W.\n\n    Update W with H being fixed, until convergence. This is the heart\n    of `transform` but it's also used during `fit` when doing fresh restarts.\n    \"\"\"\n    avg = np.sqrt(X.mean() / self._n_components)\n    W = np.full((X.shape[0], self._n_components), avg, dtype=X.dtype)\n    W_buffer = W.copy()\n\n    # Get scaled regularization terms. Done for each minibatch to take into account\n    # variable sizes of minibatches.\n    l1_reg_W, _, l2_reg_W, _ = self._compute_regularization(X)\n\n    for _ in range(max_iter):\n        W, *_ = _multiplicative_update_w(\n            X, W, H, self._beta_loss, l1_reg_W, l2_reg_W, self._gamma\n        )\n\n        W_diff = linalg.norm(W - W_buffer) / linalg.norm(W)\n        if self.tol > 0 and W_diff <= self.tol:\n            break\n\n        W_buffer[:] = W\n\n    return W",
    "scikit-learn.sklearn.decomposition._nmf._minibatch_step": "def _minibatch_step(self, X, W, H, update_H):\n    \"\"\"Perform the update of W and H for one minibatch.\"\"\"\n    batch_size = X.shape[0]\n\n    # get scaled regularization terms. Done for each minibatch to take into account\n    # variable sizes of minibatches.\n    l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H = self._compute_regularization(X)\n\n    # update W\n    if self.fresh_restarts or W is None:\n        W = self._solve_W(X, H, self.fresh_restarts_max_iter)\n    else:\n        W, *_ = _multiplicative_update_w(\n            X, W, H, self._beta_loss, l1_reg_W, l2_reg_W, self._gamma\n        )\n\n    # necessary for stability with beta_loss < 1\n    if self._beta_loss < 1:\n        W[W < np.finfo(np.float64).eps] = 0.0\n\n    batch_cost = (\n        _beta_divergence(X, W, H, self._beta_loss)\n        + l1_reg_W * W.sum()\n        + l1_reg_H * H.sum()\n        + l2_reg_W * (W**2).sum()\n        + l2_reg_H * (H**2).sum()\n    ) / batch_size\n\n    # update H (only at fit or fit_transform)\n    if update_H:\n        H[:] = _multiplicative_update_h(\n            X,\n            W,\n            H,\n            beta_loss=self._beta_loss,\n            l1_reg_H=l1_reg_H,\n            l2_reg_H=l2_reg_H,\n            gamma=self._gamma,\n            A=self._components_numerator,\n            B=self._components_denominator,\n            rho=self._rho,\n        )\n\n        # necessary for stability with beta_loss < 1\n        if self._beta_loss <= 1:\n            H[H < np.finfo(np.float64).eps] = 0.0\n\n    return batch_cost",
    "scikit-learn.sklearn.decomposition._nmf._minibatch_convergence": "def _minibatch_convergence(\n    self, X, batch_cost, H, H_buffer, n_samples, step, n_steps\n):\n    \"\"\"Helper function to encapsulate the early stopping logic\"\"\"\n    batch_size = X.shape[0]\n\n    # counts steps starting from 1 for user friendly verbose mode.\n    step = step + 1\n\n    # Ignore first iteration because H is not updated yet.\n    if step == 1:\n        if self.verbose:\n            print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\n        return False\n\n    # Compute an Exponentially Weighted Average of the cost function to\n    # monitor the convergence while discarding minibatch-local stochastic\n    # variability: https://en.wikipedia.org/wiki/Moving_average\n    if self._ewa_cost is None:\n        self._ewa_cost = batch_cost\n    else:\n        alpha = batch_size / (n_samples + 1)\n        alpha = min(alpha, 1)\n        self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\n\n    # Log progress to be able to monitor convergence\n    if self.verbose:\n        print(\n            f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\n            f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\n        )\n\n    # Early stopping based on change of H\n    H_diff = linalg.norm(H - H_buffer) / linalg.norm(H)\n    if self.tol > 0 and H_diff <= self.tol:\n        if self.verbose:\n            print(f\"Converged (small H change) at step {step}/{n_steps}\")\n        return True\n\n    # Early stopping heuristic due to lack of improvement on smoothed\n    # cost function\n    if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\n        self._no_improvement = 0\n        self._ewa_cost_min = self._ewa_cost\n    else:\n        self._no_improvement += 1\n\n    if (\n        self.max_no_improvement is not None\n        and self._no_improvement >= self.max_no_improvement\n    ):\n        if self.verbose:\n            print(\n                \"Converged (lack of improvement in objective function) \"\n                f\"at step {step}/{n_steps}\"\n            )\n        return True\n\n    return False",
    "scikit-learn.sklearn.utils._chunking.gen_batches": "@validate_params(\n    {\n        \"n\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"min_batch_size\": [Interval(Integral, 0, None, closed=\"left\")],\n    },\n    prefer_skip_nested_validation=True,\n)\ndef gen_batches(n, batch_size, *, min_batch_size=0):\n    \"\"\"Generator to create slices containing `batch_size` elements from 0 to `n`.\n\n    The last slice may contain less than `batch_size` elements, when\n    `batch_size` does not divide `n`.\n\n    Parameters\n    ----------\n    n : int\n        Size of the sequence.\n    batch_size : int\n        Number of elements in each batch.\n    min_batch_size : int, default=0\n        Minimum number of elements in each batch.\n\n    Yields\n    ------\n    slice of `batch_size` elements\n\n    See Also\n    --------\n    gen_even_slices: Generator to create n_packs slices going up to n.\n\n    Examples\n    --------\n    >>> from sklearn.utils import gen_batches\n    >>> list(gen_batches(7, 3))\n    [slice(0, 3, None), slice(3, 6, None), slice(6, 7, None)]\n    >>> list(gen_batches(6, 3))\n    [slice(0, 3, None), slice(3, 6, None)]\n    >>> list(gen_batches(2, 3))\n    [slice(0, 2, None)]\n    >>> list(gen_batches(7, 3, min_batch_size=0))\n    [slice(0, 3, None), slice(3, 6, None), slice(6, 7, None)]\n    >>> list(gen_batches(7, 3, min_batch_size=2))\n    [slice(0, 3, None), slice(3, 7, None)]\n    \"\"\"\n    start = 0\n    for _ in range(int(n // batch_size)):\n        end = start + batch_size\n        if end + min_batch_size > n:\n            continue\n        yield slice(start, end)\n        start = end\n    if start < n:\n        yield slice(start, n)",
    "scikit-learn.sklearn.utils._param_validation.wrapper": "@functools.wraps(func)\ndef wrapper(*args, **kwargs):\n    global_skip_validation = get_config()[\"skip_parameter_validation\"]\n    if global_skip_validation:\n        return func(*args, **kwargs)\n\n    func_sig = signature(func)\n\n    # Map *args/**kwargs to the function signature\n    params = func_sig.bind(*args, **kwargs)\n    params.apply_defaults()\n\n    # ignore self/cls and positional/keyword markers\n    to_ignore = [\n        p.name\n        for p in func_sig.parameters.values()\n        if p.kind in (p.VAR_POSITIONAL, p.VAR_KEYWORD)\n    ]\n    to_ignore += [\"self\", \"cls\"]\n    params = {k: v for k, v in params.arguments.items() if k not in to_ignore}\n\n    validate_parameter_constraints(\n        parameter_constraints, params, caller_name=func.__qualname__\n    )\n\n    try:\n        with config_context(\n            skip_parameter_validation=(\n                prefer_skip_nested_validation or global_skip_validation\n            )\n        ):\n            return func(*args, **kwargs)\n    except InvalidParameterError as e:\n        # When the function is just a wrapper around an estimator, we allow\n        # the function to delegate validation to the estimator, but we replace\n        # the name of the estimator by the name of the function in the error\n        # message to avoid confusion.\n        msg = re.sub(\n            r\"parameter of \\w+ must be\",\n            f\"parameter of {func.__qualname__} must be\",\n            str(e),\n        )\n        raise InvalidParameterError(msg) from e",
    "scikit-learn.sklearn.utils.validation.check_non_negative": "def check_non_negative(X, whom):\n    \"\"\"\n    Check if there is any negative value in an array.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}\n        Input data.\n\n    whom : str\n        Who passed X to this function.\n    \"\"\"\n    xp, _ = get_namespace(X)\n    # avoid X.min() on sparse matrix since it also sorts the indices\n    if sp.issparse(X):\n        if X.format in [\"lil\", \"dok\"]:\n            X = X.tocsr()\n        if X.data.size == 0:\n            X_min = 0\n        else:\n            X_min = X.data.min()\n    else:\n        X_min = xp.min(X)\n\n    if X_min < 0:\n        raise ValueError(f\"Negative values in data passed to {whom}.\")"
}