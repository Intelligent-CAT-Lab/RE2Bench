{
    "scikit-learn.sklearn.decomposition._nmf._compute_regularization": "def _compute_regularization(self, X):\n    \"\"\"Compute scaled regularization terms.\"\"\"\n    n_samples, n_features = X.shape\n    alpha_W = self.alpha_W\n    alpha_H = self.alpha_W if self.alpha_H == \"same\" else self.alpha_H\n\n    l1_reg_W = n_features * alpha_W * self.l1_ratio\n    l1_reg_H = n_samples * alpha_H * self.l1_ratio\n    l2_reg_W = n_features * alpha_W * (1.0 - self.l1_ratio)\n    l2_reg_H = n_samples * alpha_H * (1.0 - self.l1_ratio)\n\n    return l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H",
    "scikit-learn.sklearn.decomposition._nmf._multiplicative_update_w": "def _multiplicative_update_w(\n    X,\n    W,\n    H,\n    beta_loss,\n    l1_reg_W,\n    l2_reg_W,\n    gamma,\n    H_sum=None,\n    HHt=None,\n    XHt=None,\n    update_H=True,\n):\n    \"\"\"Update W in Multiplicative Update NMF.\"\"\"\n    if beta_loss == 2:\n        # Numerator\n        if XHt is None:\n            XHt = safe_sparse_dot(X, H.T)\n        if update_H:\n            # avoid a copy of XHt, which will be re-computed (update_H=True)\n            numerator = XHt\n        else:\n            # preserve the XHt, which is not re-computed (update_H=False)\n            numerator = XHt.copy()\n\n        # Denominator\n        if HHt is None:\n            HHt = np.dot(H, H.T)\n        denominator = np.dot(W, HHt)\n\n    else:\n        # Numerator\n        # if X is sparse, compute WH only where X is non zero\n        WH_safe_X = _special_sparse_dot(W, H, X)\n        if sp.issparse(X):\n            WH_safe_X_data = WH_safe_X.data\n            X_data = X.data\n        else:\n            WH_safe_X_data = WH_safe_X\n            X_data = X\n            # copy used in the Denominator\n            WH = WH_safe_X.copy()\n            if beta_loss - 1.0 < 0:\n                WH[WH < EPSILON] = EPSILON\n\n        # to avoid taking a negative power of zero\n        if beta_loss - 2.0 < 0:\n            WH_safe_X_data[WH_safe_X_data < EPSILON] = EPSILON\n\n        if beta_loss == 1:\n            np.divide(X_data, WH_safe_X_data, out=WH_safe_X_data)\n        elif beta_loss == 0:\n            # speeds up computation time\n            # refer to /numpy/numpy/issues/9363\n            WH_safe_X_data **= -1\n            WH_safe_X_data **= 2\n            # element-wise multiplication\n            WH_safe_X_data *= X_data\n        else:\n            WH_safe_X_data **= beta_loss - 2\n            # element-wise multiplication\n            WH_safe_X_data *= X_data\n\n        # here numerator = dot(X * (dot(W, H) ** (beta_loss - 2)), H.T)\n        numerator = safe_sparse_dot(WH_safe_X, H.T)\n\n        # Denominator\n        if beta_loss == 1:\n            if H_sum is None:\n                H_sum = np.sum(H, axis=1)  # shape(n_components, )\n            denominator = H_sum[np.newaxis, :]\n\n        else:\n            # computation of WHHt = dot(dot(W, H) ** beta_loss - 1, H.T)\n            if sp.issparse(X):\n                # memory efficient computation\n                # (compute row by row, avoiding the dense matrix WH)\n                WHHt = np.empty(W.shape)\n                for i in range(X.shape[0]):\n                    WHi = np.dot(W[i, :], H)\n                    if beta_loss - 1 < 0:\n                        WHi[WHi < EPSILON] = EPSILON\n                    WHi **= beta_loss - 1\n                    WHHt[i, :] = np.dot(WHi, H.T)\n            else:\n                WH **= beta_loss - 1\n                WHHt = np.dot(WH, H.T)\n            denominator = WHHt\n\n    # Add L1 and L2 regularization\n    if l1_reg_W > 0:\n        denominator += l1_reg_W\n    if l2_reg_W > 0:\n        denominator = denominator + l2_reg_W * W\n    denominator[denominator == 0] = EPSILON\n\n    numerator /= denominator\n    delta_W = numerator\n\n    # gamma is in ]0, 1]\n    if gamma != 1:\n        delta_W **= gamma\n\n    W *= delta_W\n\n    return W, H_sum, HHt, XHt"
}