{
    "scikit-learn.sklearn.impute._base._transform_indicator": "def _transform_indicator(self, X):\n    \"\"\"Compute the indicator mask.'\n\n    Note that X must be the original data as passed to the imputer before\n    any imputation, since imputation may be done inplace in some cases.\n    \"\"\"\n    if self.add_indicator:\n        if not hasattr(self, \"indicator_\"):\n            raise ValueError(\n                \"Make sure to call _fit_indicator before _transform_indicator\"\n            )\n        return self.indicator_.transform(X)",
    "scikit-learn.sklearn.impute._base._concatenate_indicator": "def _concatenate_indicator(self, X_imputed, X_indicator):\n    \"\"\"Concatenate indicator mask with the imputed data.\"\"\"\n    if not self.add_indicator:\n        return X_imputed\n\n    if sp.issparse(X_imputed):\n        # sp.hstack may result in different formats between sparse arrays and\n        # matrices; specify the format to keep consistent behavior\n        hstack = partial(sp.hstack, format=X_imputed.format)\n    else:\n        hstack = np.hstack\n\n    if X_indicator is None:\n        raise ValueError(\n            \"Data from the missing indicator are not provided. Call \"\n            \"_fit_indicator and _transform_indicator in the imputer \"\n            \"implementation.\"\n        )\n\n    return hstack((X_imputed, X_indicator))",
    "scikit-learn.sklearn.impute._base._validate_input": "def _validate_input(self, X, in_fit):\n    if self.strategy in (\"most_frequent\", \"constant\"):\n        # If input is a list of strings, dtype = object.\n        # Otherwise ValueError is raised in SimpleImputer\n        # with strategy='most_frequent' or 'constant'\n        # because the list is converted to Unicode numpy array\n        if isinstance(X, list) and any(\n            isinstance(elem, str) for row in X for elem in row\n        ):\n            dtype = object\n        else:\n            dtype = None\n    else:\n        dtype = FLOAT_DTYPES\n\n    if not in_fit and self._fit_dtype.kind == \"O\":\n        # Use object dtype if fitted on object dtypes\n        dtype = self._fit_dtype\n\n    if is_pandas_na(self.missing_values) or is_scalar_nan(self.missing_values):\n        ensure_all_finite = \"allow-nan\"\n    else:\n        ensure_all_finite = True\n\n    try:\n        X = validate_data(\n            self,\n            X,\n            reset=in_fit,\n            accept_sparse=\"csc\",\n            dtype=dtype,\n            force_writeable=True if not in_fit else None,\n            ensure_all_finite=ensure_all_finite,\n            copy=self.copy,\n        )\n    except ValueError as ve:\n        if \"could not convert\" in str(ve):\n            new_ve = ValueError(\n                \"Cannot use {} strategy with non-numeric data:\\n{}\".format(\n                    self.strategy, ve\n                )\n            )\n            raise new_ve from None\n        else:\n            raise ve\n\n    if in_fit:\n        # Use the dtype seen in `fit` for non-`fit` conversion\n        self._fit_dtype = X.dtype\n\n    _check_inputs_dtype(X, self.missing_values)\n    if X.dtype.kind not in (\"i\", \"u\", \"f\", \"O\"):\n        raise ValueError(\n            \"SimpleImputer does not support data with dtype \"\n            \"{0}. Please provide either a numeric array (with\"\n            \" a floating point or integer dtype) or \"\n            \"categorical data represented either as an array \"\n            \"with integer dtype or an array of string values \"\n            \"with an object dtype.\".format(X.dtype)\n        )\n\n    if sp.issparse(X) and self.missing_values == 0:\n        # missing_values = 0 not allowed with sparse data as it would\n        # force densification\n        raise ValueError(\n            \"Imputation not possible when missing_values \"\n            \"== 0 and input is sparse. Provide a dense \"\n            \"array instead.\"\n        )\n\n    if self.strategy == \"constant\":\n        if in_fit and self.fill_value is not None:\n            fill_value_dtype = type(self.fill_value)\n            err_msg = (\n                f\"fill_value={self.fill_value!r} (of type {fill_value_dtype!r}) \"\n                f\"cannot be cast to the input data that is {X.dtype!r}. \"\n                \"If fill_value is a Python scalar, instead pass  a numpy scalar \"\n                \"(e.g. fill_value=np.uint8(0) if your data is of type np.uint8). \"\n                \"Make sure that both dtypes are of the same kind.\"\n            )\n        elif not in_fit:\n            fill_value_dtype = self._fill_dtype\n            err_msg = (\n                f\"The dtype of the filling value (i.e. {fill_value_dtype!r}) \"\n                f\"cannot be cast to the input data that is {X.dtype!r}. \"\n                \"Make sure that the dtypes of the input data are of the same kind \"\n                \"between fit and transform.\"\n            )\n        else:\n            # By default, fill_value=None, and the replacement is always\n            # compatible with the input data\n            fill_value_dtype = X.dtype\n\n        # Make sure we can safely cast fill_value dtype to the input data dtype\n        if not np.can_cast(fill_value_dtype, X.dtype, casting=\"same_kind\"):\n            raise ValueError(err_msg)\n\n    return X",
    "scikit-learn.sklearn.utils._mask._get_mask": "def _get_mask(X, value_to_mask):\n    \"\"\"Compute the boolean mask X == value_to_mask.\n\n    Parameters\n    ----------\n    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        Input data, where ``n_samples`` is the number of samples and\n        ``n_features`` is the number of features.\n\n    value_to_mask : {int, float}\n        The value which is to be masked in X.\n\n    Returns\n    -------\n    X_mask : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        Missing mask.\n    \"\"\"\n    if not sp.issparse(X):\n        # For all cases apart of a sparse input where we need to reconstruct\n        # a sparse output\n        return _get_dense_mask(X, value_to_mask)\n\n    Xt = _get_dense_mask(X.data, value_to_mask)\n\n    sparse_constructor = sp.csr_matrix if X.format == \"csr\" else sp.csc_matrix\n    Xt_sparse = sparse_constructor(\n        (Xt, X.indices.copy(), X.indptr.copy()), shape=X.shape, dtype=bool\n    )\n\n    return Xt_sparse",
    "scikit-learn.sklearn.utils.validation.check_is_fitted": "def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):\n    \"\"\"Perform is_fitted validation for estimator.\n\n    Checks if the estimator is fitted by verifying the presence of\n    fitted attributes (ending with a trailing underscore) and otherwise\n    raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to\n    specify if the estimator is fitted or not. See\n    :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\n    for an example on how to use the API.\n\n    If no `attributes` are passed, this function will pass if an estimator is stateless.\n    An estimator can indicate it's stateless by setting the `requires_fit` tag. See\n    :ref:`estimator_tags` for more information. Note that the `requires_fit` tag\n    is ignored if `attributes` are passed.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Raises\n    ------\n    TypeError\n        If the estimator is a class or not an estimator instance\n\n    NotFittedError\n        If the attributes are not found.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.utils.validation import check_is_fitted\n    >>> from sklearn.exceptions import NotFittedError\n    >>> lr = LogisticRegression()\n    >>> try:\n    ...     check_is_fitted(lr)\n    ... except NotFittedError as exc:\n    ...     print(f\"Model is not fitted yet.\")\n    Model is not fitted yet.\n    >>> lr.fit([[1, 2], [1, 3]], [1, 0])\n    LogisticRegression()\n    >>> check_is_fitted(lr)\n    \"\"\"\n    if isclass(estimator):\n        raise TypeError(\"{} is a class, not an instance.\".format(estimator))\n    if msg is None:\n        msg = (\n            \"This %(name)s instance is not fitted yet. Call 'fit' with \"\n            \"appropriate arguments before using this estimator.\"\n        )\n\n    if not hasattr(estimator, \"fit\"):\n        raise TypeError(\"%s is not an estimator instance.\" % (estimator))\n\n    tags = get_tags(estimator)\n\n    if not tags.requires_fit and attributes is None:\n        return\n\n    if not _is_fitted(estimator, attributes, all_or_any):\n        raise NotFittedError(msg % {\"name\": type(estimator).__name__})"
}