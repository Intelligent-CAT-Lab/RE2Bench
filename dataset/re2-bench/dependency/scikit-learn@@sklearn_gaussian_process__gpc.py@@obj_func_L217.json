{
    "scikit-learn.sklearn.gaussian_process._gpc.log_marginal_likelihood": "def log_marginal_likelihood(\n    self, theta=None, eval_gradient=False, clone_kernel=True\n):\n    \"\"\"Returns log-marginal likelihood of theta for training data.\n\n    Parameters\n    ----------\n    theta : array-like of shape (n_kernel_params,), default=None\n        Kernel hyperparameters for which the log-marginal likelihood is\n        evaluated. If None, the precomputed log_marginal_likelihood\n        of ``self.kernel_.theta`` is returned.\n\n    eval_gradient : bool, default=False\n        If True, the gradient of the log-marginal likelihood with respect\n        to the kernel hyperparameters at position theta is returned\n        additionally. If True, theta must not be None.\n\n    clone_kernel : bool, default=True\n        If True, the kernel attribute is copied. If False, the kernel\n        attribute is modified, but may result in a performance improvement.\n\n    Returns\n    -------\n    log_likelihood : float\n        Log-marginal likelihood of theta for training data.\n\n    log_likelihood_gradient : ndarray of shape (n_kernel_params,), \\\n            optional\n        Gradient of the log-marginal likelihood with respect to the kernel\n        hyperparameters at position theta.\n        Only returned when `eval_gradient` is True.\n    \"\"\"\n    if theta is None:\n        if eval_gradient:\n            raise ValueError(\"Gradient can only be evaluated for theta!=None\")\n        return self.log_marginal_likelihood_value_\n\n    if clone_kernel:\n        kernel = self.kernel_.clone_with_theta(theta)\n    else:\n        kernel = self.kernel_\n        kernel.theta = theta\n\n    if eval_gradient:\n        K, K_gradient = kernel(self.X_train_, eval_gradient=True)\n    else:\n        K = kernel(self.X_train_)\n\n    # Compute log-marginal-likelihood Z and also store some temporaries\n    # which can be reused for computing Z's gradient\n    Z, (pi, W_sr, L, b, a) = self._posterior_mode(K, return_temporaries=True)\n\n    if not eval_gradient:\n        return Z\n\n    # Compute gradient based on Algorithm 5.1 of GPML\n    d_Z = np.empty(theta.shape[0])\n    # XXX: Get rid of the np.diag() in the next line\n    R = W_sr[:, np.newaxis] * cho_solve((L, True), np.diag(W_sr))  # Line 7\n    C = solve(L, W_sr[:, np.newaxis] * K)  # Line 8\n    # Line 9: (use einsum to compute np.diag(C.T.dot(C))))\n    s_2 = (\n        -0.5\n        * (np.diag(K) - np.einsum(\"ij, ij -> j\", C, C))\n        * (pi * (1 - pi) * (1 - 2 * pi))\n    )  # third derivative\n\n    for j in range(d_Z.shape[0]):\n        C = K_gradient[:, :, j]  # Line 11\n        # Line 12: (R.T.ravel().dot(C.ravel()) = np.trace(R.dot(C)))\n        s_1 = 0.5 * a.T.dot(C).dot(a) - 0.5 * R.T.ravel().dot(C.ravel())\n\n        b = C.dot(self.y_train_ - pi)  # Line 13\n        s_3 = b - K.dot(R.dot(b))  # Line 14\n\n        d_Z[j] = s_1 + s_2.T.dot(s_3)  # Line 15\n\n    return Z, d_Z"
}