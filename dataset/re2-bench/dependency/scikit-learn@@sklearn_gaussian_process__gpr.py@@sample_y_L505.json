{
    "scikit-learn.sklearn.gaussian_process._gpr.predict": "def predict(self, X, return_std=False, return_cov=False):\n    \"\"\"Predict using the Gaussian process regression model.\n\n    We can also predict based on an unfitted model by using the GP prior.\n    In addition to the mean of the predictive distribution, optionally also\n    returns its standard deviation (`return_std=True`) or covariance\n    (`return_cov=True`). Note that at most one of the two can be requested.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features) or list of object\n        Query points where the GP is evaluated.\n\n    return_std : bool, default=False\n        If True, the standard-deviation of the predictive distribution at\n        the query points is returned along with the mean.\n\n    return_cov : bool, default=False\n        If True, the covariance of the joint predictive distribution at\n        the query points is returned along with the mean.\n\n    Returns\n    -------\n    y_mean : ndarray of shape (n_samples,) or (n_samples, n_targets)\n        Mean of predictive distribution at query points.\n\n    y_std : ndarray of shape (n_samples,) or (n_samples, n_targets), optional\n        Standard deviation of predictive distribution at query points.\n        Only returned when `return_std` is True.\n\n    y_cov : ndarray of shape (n_samples, n_samples) or \\\n            (n_samples, n_samples, n_targets), optional\n        Covariance of joint predictive distribution at query points.\n        Only returned when `return_cov` is True.\n    \"\"\"\n    if return_std and return_cov:\n        raise RuntimeError(\n            \"At most one of return_std or return_cov can be requested.\"\n        )\n\n    if self.kernel is None or self.kernel.requires_vector_input:\n        dtype, ensure_2d = \"numeric\", True\n    else:\n        dtype, ensure_2d = None, False\n\n    X = validate_data(self, X, ensure_2d=ensure_2d, dtype=dtype, reset=False)\n\n    if not hasattr(self, \"X_train_\"):  # Unfitted;predict based on GP prior\n        if self.kernel is None:\n            kernel = C(1.0, constant_value_bounds=\"fixed\") * RBF(\n                1.0, length_scale_bounds=\"fixed\"\n            )\n        else:\n            kernel = self.kernel\n\n        n_targets = self.n_targets if self.n_targets is not None else 1\n        y_mean = np.zeros(shape=(X.shape[0], n_targets)).squeeze()\n\n        if return_cov:\n            y_cov = kernel(X)\n            if n_targets > 1:\n                y_cov = np.repeat(\n                    np.expand_dims(y_cov, -1), repeats=n_targets, axis=-1\n                )\n            return y_mean, y_cov\n        elif return_std:\n            y_var = kernel.diag(X)\n            if n_targets > 1:\n                y_var = np.repeat(\n                    np.expand_dims(y_var, -1), repeats=n_targets, axis=-1\n                )\n            return y_mean, np.sqrt(y_var)\n        else:\n            return y_mean\n    else:  # Predict based on GP posterior\n        # Alg 2.1, page 19, line 4 -> f*_bar = K(X_test, X_train) . alpha\n        K_trans = self.kernel_(X, self.X_train_)\n        y_mean = K_trans @ self.alpha_\n\n        # undo normalisation\n        y_mean = self._y_train_std * y_mean + self._y_train_mean\n\n        # if y_mean has shape (n_samples, 1), reshape to (n_samples,)\n        if y_mean.ndim > 1 and y_mean.shape[1] == 1:\n            y_mean = np.squeeze(y_mean, axis=1)\n\n        if not return_cov and not return_std:\n            return y_mean\n\n        # Alg 2.1, page 19, line 5 -> v = L \\ K(X_test, X_train)^T\n        V = solve_triangular(\n            self.L_, K_trans.T, lower=GPR_CHOLESKY_LOWER, check_finite=False\n        )\n\n        if return_cov:\n            # Alg 2.1, page 19, line 6 -> K(X_test, X_test) - v^T. v\n            y_cov = self.kernel_(X) - V.T @ V\n\n            # undo normalisation\n            y_cov = np.outer(y_cov, self._y_train_std**2).reshape(*y_cov.shape, -1)\n            # if y_cov has shape (n_samples, n_samples, 1), reshape to\n            # (n_samples, n_samples)\n            if y_cov.shape[2] == 1:\n                y_cov = np.squeeze(y_cov, axis=2)\n\n            return y_mean, y_cov\n        else:  # return_std\n            # Compute variance of predictive distribution\n            # Use einsum to avoid explicitly forming the large matrix\n            # V^T @ V just to extract its diagonal afterward.\n            y_var = self.kernel_.diag(X).copy()\n            y_var -= np.einsum(\"ij,ji->i\", V.T, V)\n\n            # Check if any of the variances is negative because of\n            # numerical issues. If yes: set the variance to 0.\n            y_var_negative = y_var < 0\n            if np.any(y_var_negative):\n                warnings.warn(\n                    \"Predicted variances smaller than 0. \"\n                    \"Setting those variances to 0.\"\n                )\n                y_var[y_var_negative] = 0.0\n\n            # undo normalisation\n            y_var = np.outer(y_var, self._y_train_std**2).reshape(*y_var.shape, -1)\n\n            # if y_var has shape (n_samples, 1), reshape to (n_samples,)\n            if y_var.shape[1] == 1:\n                y_var = np.squeeze(y_var, axis=1)\n\n            return y_mean, np.sqrt(y_var)",
    "scikit-learn.sklearn.gaussian_process._gpr.<listcomp>": "y_samples = [\n    rng.multivariate_normal(\n        y_mean[:, target], y_cov[..., target], n_samples\n    ).T[:, np.newaxis]\n    for target in range(y_mean.shape[1])\n",
    "scikit-learn.sklearn.utils.validation.check_random_state": "def check_random_state(seed):\n    \"\"\"Turn seed into an np.random.RandomState instance.\n\n    Parameters\n    ----------\n    seed : None, int or instance of RandomState\n        If seed is None, return the RandomState singleton used by np.random.\n        If seed is an int, return a new RandomState instance seeded with seed.\n        If seed is already a RandomState instance, return it.\n        Otherwise raise ValueError.\n\n    Returns\n    -------\n    :class:`numpy:numpy.random.RandomState`\n        The random state object based on `seed` parameter.\n\n    Examples\n    --------\n    >>> from sklearn.utils.validation import check_random_state\n    >>> check_random_state(42)\n    RandomState(MT19937) at 0x...\n    \"\"\"\n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, numbers.Integral):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState):\n        return seed\n    raise ValueError(\n        \"%r cannot be used to seed a numpy.random.RandomState instance\" % seed\n    )"
}