{
    "scikit-learn.sklearn.decomposition._dict_learning._update_inner_stats": "def _update_inner_stats(self, X, code, batch_size, step):\n    \"\"\"Update the inner stats inplace.\"\"\"\n    if step < batch_size - 1:\n        theta = (step + 1) * batch_size\n    else:\n        theta = batch_size**2 + step + 1 - batch_size\n    beta = (theta + 1 - batch_size) / (theta + 1)\n\n    self._A *= beta\n    self._A += code.T @ code / batch_size\n    self._B *= beta\n    self._B += X.T @ code / batch_size",
    "scikit-learn.sklearn.decomposition._dict_learning._sparse_encode": "def _sparse_encode(\n    X,\n    dictionary,\n    *,\n    gram=None,\n    cov=None,\n    algorithm=\"lasso_lars\",\n    n_nonzero_coefs=None,\n    alpha=None,\n    copy_cov=True,\n    init=None,\n    max_iter=1000,\n    n_jobs=None,\n    verbose=0,\n    positive=False,\n):\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\n\n    n_samples, n_features = X.shape\n    n_components = dictionary.shape[0]\n\n    if algorithm in (\"lars\", \"omp\"):\n        regularization = n_nonzero_coefs\n        if regularization is None:\n            regularization = min(max(n_features / 10, 1), n_components)\n    else:\n        regularization = alpha\n        if regularization is None:\n            regularization = 1.0\n\n    if gram is None and algorithm != \"threshold\":\n        gram = np.dot(dictionary, dictionary.T).astype(X.dtype, copy=False)\n\n    if cov is None and algorithm != \"lasso_cd\":\n        copy_cov = False\n        cov = np.dot(dictionary, X.T)\n\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\n        code = _sparse_encode_precomputed(\n            X,\n            dictionary,\n            gram=gram,\n            cov=cov,\n            algorithm=algorithm,\n            regularization=regularization,\n            copy_cov=copy_cov,\n            init=init,\n            max_iter=max_iter,\n            verbose=verbose,\n            positive=positive,\n        )\n        return code\n\n    # Enter parallel code block\n    n_samples = X.shape[0]\n    n_components = dictionary.shape[0]\n    code = np.empty((n_samples, n_components))\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\n\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\n        delayed(_sparse_encode_precomputed)(\n            X[this_slice],\n            dictionary,\n            gram=gram,\n            cov=cov[:, this_slice] if cov is not None else None,\n            algorithm=algorithm,\n            regularization=regularization,\n            copy_cov=copy_cov,\n            init=init[this_slice] if init is not None else None,\n            max_iter=max_iter,\n            verbose=verbose,\n            positive=positive,\n        )\n        for this_slice in slices\n    )\n    for this_slice, this_view in zip(slices, code_views):\n        code[this_slice] = this_view\n    return code",
    "scikit-learn.sklearn.decomposition._dict_learning._update_dict": "def _update_dict(\n    dictionary,\n    Y,\n    code,\n    A=None,\n    B=None,\n    verbose=False,\n    random_state=None,\n    positive=False,\n):\n    \"\"\"Update the dense dictionary factor in place.\n\n    Parameters\n    ----------\n    dictionary : ndarray of shape (n_components, n_features)\n        Value of the dictionary at the previous iteration.\n\n    Y : ndarray of shape (n_samples, n_features)\n        Data matrix.\n\n    code : ndarray of shape (n_samples, n_components)\n        Sparse coding of the data against which to optimize the dictionary.\n\n    A : ndarray of shape (n_components, n_components), default=None\n        Together with `B`, sufficient stats of the online model to update the\n        dictionary.\n\n    B : ndarray of shape (n_features, n_components), default=None\n        Together with `A`, sufficient stats of the online model to update the\n        dictionary.\n\n    verbose: bool, default=False\n        Degree of output the procedure will print.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for randomly initializing the dictionary. Pass an int for\n        reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    positive : bool, default=False\n        Whether to enforce positivity when finding the dictionary.\n\n        .. versionadded:: 0.20\n    \"\"\"\n    n_samples, n_components = code.shape\n    random_state = check_random_state(random_state)\n\n    if A is None:\n        A = code.T @ code\n    if B is None:\n        B = Y.T @ code\n\n    n_unused = 0\n\n    for k in range(n_components):\n        if A[k, k] > 1e-6:\n            # 1e-6 is arbitrary but consistent with the spams implementation\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\n        else:\n            # kth atom is almost never used -> sample a new one from the data\n            newd = Y[random_state.choice(n_samples)]\n\n            # add small noise to avoid making the sparse coding ill conditioned\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\n            noise = random_state.normal(0, noise_level, size=len(newd))\n\n            dictionary[k] = newd + noise\n            code[:, k] = 0\n            n_unused += 1\n\n        if positive:\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\n\n        # Projection on the constraint set ||V_k|| <= 1\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\n\n    if verbose and n_unused > 0:\n        print(f\"{n_unused} unused atoms resampled.\")"
}