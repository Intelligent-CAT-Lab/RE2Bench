{
    "scikit-learn.sklearn.compose._column_transformer.__init__": "def __init__(\n    self,\n    transformers,\n    *,\n    remainder=\"drop\",\n    sparse_threshold=0.3,\n    n_jobs=None,\n    transformer_weights=None,\n    verbose=False,\n    verbose_feature_names_out=True,\n    force_int_remainder_cols=\"deprecated\",\n):\n    self.transformers = transformers\n    self.remainder = remainder\n    self.sparse_threshold = sparse_threshold\n    self.n_jobs = n_jobs\n    self.transformer_weights = transformer_weights\n    self.verbose = verbose\n    self.verbose_feature_names_out = verbose_feature_names_out\n    self.force_int_remainder_cols = force_int_remainder_cols",
    "scikit-learn.sklearn.compose._column_transformer.set_output": "def set_output(self, *, transform=None):\n    \"\"\"Set the output container when `\"transform\"` and `\"fit_transform\"` are called.\n\n    Calling `set_output` will set the output of all estimators in `transformers`\n    and `transformers_`.\n\n    Parameters\n    ----------\n    transform : {\"default\", \"pandas\", \"polars\"}, default=None\n        Configure output of `transform` and `fit_transform`.\n\n        - `\"default\"`: Default output format of a transformer\n        - `\"pandas\"`: DataFrame output\n        - `\"polars\"`: Polars output\n        - `None`: Transform configuration is unchanged\n\n        .. versionadded:: 1.4\n            `\"polars\"` option was added.\n\n    Returns\n    -------\n    self : estimator instance\n        Estimator instance.\n    \"\"\"\n    super().set_output(transform=transform)\n\n    transformers = (\n        trans\n        for _, trans, _ in chain(\n            self.transformers, getattr(self, \"transformers_\", [])\n        )\n        if trans not in {\"passthrough\", \"drop\"}\n    )\n    for trans in transformers:\n        _safe_set_output(trans, transform=transform)\n\n    if self.remainder not in {\"passthrough\", \"drop\"}:\n        _safe_set_output(self.remainder, transform=transform)\n\n    return self",
    "scikit-learn.sklearn.ensemble._hist_gradient_boosting.gradient_boosting._check_categories": "def _check_categories(self):\n    \"\"\"Check categories found by the preprocessor and return their encoded values.\n\n    Returns a list of length ``self.n_features_in_``, with one entry per\n    input feature.\n\n    For non-categorical features, the corresponding entry is ``None``.\n\n    For categorical features, the corresponding entry is an array\n    containing the categories as encoded by the preprocessor (an\n    ``OrdinalEncoder``), excluding missing values. The entry is therefore\n    ``np.arange(n_categories)`` where ``n_categories`` is the number of\n    unique values in the considered feature column, after removing missing\n    values.\n\n    If ``n_categories > self.max_bins`` for any feature, a ``ValueError``\n    is raised.\n    \"\"\"\n    encoder = self._preprocessor.named_transformers_[\"encoder\"]\n    known_categories = [None] * self._preprocessor.n_features_in_\n    categorical_column_indices = np.arange(self._preprocessor.n_features_in_)[\n        self._preprocessor.output_indices_[\"encoder\"]\n    ]\n    for feature_idx, categories in zip(\n        categorical_column_indices, encoder.categories_\n    ):\n        # OrdinalEncoder always puts np.nan as the last category if the\n        # training data has missing values. Here we remove it because it is\n        # already added by the _BinMapper.\n        if len(categories) and is_scalar_nan(categories[-1]):\n            categories = categories[:-1]\n        if categories.size > self.max_bins:\n            try:\n                feature_name = repr(encoder.feature_names_in_[feature_idx])\n            except AttributeError:\n                feature_name = f\"at index {feature_idx}\"\n            raise ValueError(\n                f\"Categorical feature {feature_name} is expected to \"\n                f\"have a cardinality <= {self.max_bins} but actually \"\n                f\"has a cardinality of {categories.size}.\"\n            )\n        known_categories[feature_idx] = np.arange(len(categories), dtype=X_DTYPE)\n    return known_categories",
    "scikit-learn.sklearn.ensemble._hist_gradient_boosting.gradient_boosting._check_categorical_features": "def _check_categorical_features(self, X):\n    \"\"\"Check and validate categorical features in X\n\n    Parameters\n    ----------\n    X : {array-like, pandas DataFrame} of shape (n_samples, n_features)\n        Input data.\n\n    Return\n    ------\n    is_categorical : ndarray of shape (n_features,) or None, dtype=bool\n        Indicates whether a feature is categorical. If no feature is\n        categorical, this is None.\n    \"\"\"\n    # Special code for pandas because of a bug in recent pandas, which is\n    # fixed in main and maybe included in 2.2.1, see\n    # https://github.com/pandas-dev/pandas/pull/57173.\n    # Also pandas versions < 1.5.1 do not support the dataframe interchange\n    if _is_pandas_df(X):\n        X_is_dataframe = True\n        categorical_columns_mask = np.asarray(X.dtypes == \"category\")\n    elif hasattr(X, \"__dataframe__\"):\n        X_is_dataframe = True\n        categorical_columns_mask = np.asarray(\n            [\n                c.dtype[0].name == \"CATEGORICAL\"\n                for c in X.__dataframe__().get_columns()\n            ]\n        )\n    else:\n        X_is_dataframe = False\n        categorical_columns_mask = None\n\n    categorical_features = self.categorical_features\n\n    categorical_by_dtype = (\n        isinstance(categorical_features, str)\n        and categorical_features == \"from_dtype\"\n    )\n    no_categorical_dtype = categorical_features is None or (\n        categorical_by_dtype and not X_is_dataframe\n    )\n\n    if no_categorical_dtype:\n        return None\n\n    use_pandas_categorical = categorical_by_dtype and X_is_dataframe\n    if use_pandas_categorical:\n        categorical_features = categorical_columns_mask\n    else:\n        categorical_features = np.asarray(categorical_features)\n\n    if categorical_features.size == 0:\n        return None\n\n    if categorical_features.dtype.kind not in (\"i\", \"b\", \"U\", \"O\"):\n        raise ValueError(\n            \"categorical_features must be an array-like of bool, int or \"\n            f\"str, got: {categorical_features.dtype.name}.\"\n        )\n\n    if categorical_features.dtype.kind == \"O\":\n        types = set(type(f) for f in categorical_features)\n        if types != {str}:\n            raise ValueError(\n                \"categorical_features must be an array-like of bool, int or \"\n                f\"str, got: {', '.join(sorted(t.__name__ for t in types))}.\"\n            )\n\n    n_features = X.shape[1]\n    # At this point `validate_data` was not called yet because we use the original\n    # dtypes to discover the categorical features. Thus `feature_names_in_`\n    # is not defined yet.\n    feature_names_in_ = getattr(X, \"columns\", None)\n\n    if categorical_features.dtype.kind in (\"U\", \"O\"):\n        # check for feature names\n        if feature_names_in_ is None:\n            raise ValueError(\n                \"categorical_features should be passed as an array of \"\n                \"integers or as a boolean mask when the model is fitted \"\n                \"on data without feature names.\"\n            )\n        is_categorical = np.zeros(n_features, dtype=bool)\n        feature_names = list(feature_names_in_)\n        for feature_name in categorical_features:\n            try:\n                is_categorical[feature_names.index(feature_name)] = True\n            except ValueError as e:\n                raise ValueError(\n                    f\"categorical_features has an item value '{feature_name}' \"\n                    \"which is not a valid feature name of the training \"\n                    f\"data. Observed feature names: {feature_names}\"\n                ) from e\n    elif categorical_features.dtype.kind == \"i\":\n        # check for categorical features as indices\n        if (\n            np.max(categorical_features) >= n_features\n            or np.min(categorical_features) < 0\n        ):\n            raise ValueError(\n                \"categorical_features set as integer \"\n                \"indices must be in [0, n_features - 1]\"\n            )\n        is_categorical = np.zeros(n_features, dtype=bool)\n        is_categorical[categorical_features] = True\n    else:\n        if categorical_features.shape[0] != n_features:\n            raise ValueError(\n                \"categorical_features set as a boolean mask \"\n                \"must have shape (n_features,), got: \"\n                f\"{categorical_features.shape}\"\n            )\n        is_categorical = categorical_features\n\n    if not np.any(is_categorical):\n        return None\n    return is_categorical",
    "scikit-learn.sklearn.preprocessing._encoders.__init__": "def __init__(\n    self,\n    *,\n    categories=\"auto\",\n    dtype=np.float64,\n    handle_unknown=\"error\",\n    unknown_value=None,\n    encoded_missing_value=np.nan,\n    min_frequency=None,\n    max_categories=None,\n):\n    self.categories = categories\n    self.dtype = dtype\n    self.handle_unknown = handle_unknown\n    self.unknown_value = unknown_value\n    self.encoded_missing_value = encoded_missing_value\n    self.min_frequency = min_frequency\n    self.max_categories = max_categories",
    "scikit-learn.sklearn.preprocessing._function_transformer.__init__": "def __init__(\n    self,\n    func=None,\n    inverse_func=None,\n    *,\n    validate=False,\n    accept_sparse=False,\n    check_inverse=True,\n    feature_names_out=None,\n    kw_args=None,\n    inv_kw_args=None,\n):\n    self.func = func\n    self.inverse_func = inverse_func\n    self.validate = validate\n    self.accept_sparse = accept_sparse\n    self.check_inverse = check_inverse\n    self.feature_names_out = feature_names_out\n    self.kw_args = kw_args\n    self.inv_kw_args = inv_kw_args",
    "scikit-learn.sklearn.utils._set_output.wrapped": "@wraps(f)\ndef wrapped(self, X, *args, **kwargs):\n    data_to_wrap = f(self, X, *args, **kwargs)\n    if isinstance(data_to_wrap, tuple):\n        # only wrap the first output for cross decomposition\n        return_tuple = (\n            _wrap_data_with_container(method, data_to_wrap[0], X, self),\n            *data_to_wrap[1:],\n        )\n        # Support for namedtuples `_make` is a documented API for namedtuples:\n        # https://docs.python.org/3/library/collections.html#collections.somenamedtuple._make\n        if hasattr(type(data_to_wrap), \"_make\"):\n            return type(data_to_wrap)._make(return_tuple)\n        return return_tuple\n\n    return _wrap_data_with_container(method, data_to_wrap, X, self)",
    "scikit-learn.sklearn.utils.validation.validate_data": "def validate_data(\n    _estimator,\n    /,\n    X=\"no_validation\",\n    y=\"no_validation\",\n    reset=True,\n    validate_separately=False,\n    skip_check_array=False,\n    **check_params,\n):\n    \"\"\"Validate input data and set or check feature names and counts of the input.\n\n    This helper function should be used in an estimator that requires input\n    validation. This mutates the estimator and sets the `n_features_in_` and\n    `feature_names_in_` attributes if `reset=True`.\n\n    .. versionadded:: 1.6\n\n    Parameters\n    ----------\n    _estimator : estimator instance\n        The estimator to validate the input for.\n\n    X : {array-like, sparse matrix, dataframe} of shape \\\n            (n_samples, n_features), default='no validation'\n        The input samples.\n        If `'no_validation'`, no validation is performed on `X`. This is\n        useful for meta-estimator which can delegate input validation to\n        their underlying estimator(s). In that case `y` must be passed and\n        the only accepted `check_params` are `multi_output` and\n        `y_numeric`.\n\n    y : array-like of shape (n_samples,), default='no_validation'\n        The targets.\n\n        - If `None`, :func:`~sklearn.utils.check_array` is called on `X`. If\n          the estimator's `requires_y` tag is True, then an error will be raised.\n        - If `'no_validation'`, :func:`~sklearn.utils.check_array` is called\n          on `X` and the estimator's `requires_y` tag is ignored. This is a default\n          placeholder and is never meant to be explicitly set. In that case `X` must be\n          passed.\n        - Otherwise, only `y` with `_check_y` or both `X` and `y` are checked with\n          either :func:`~sklearn.utils.check_array` or\n          :func:`~sklearn.utils.check_X_y` depending on `validate_separately`.\n\n    reset : bool, default=True\n        Whether to reset the `n_features_in_` attribute.\n        If False, the input will be checked for consistency with data\n        provided when reset was last True.\n\n        .. note::\n\n           It is recommended to call `reset=True` in `fit` and in the first\n           call to `partial_fit`. All other methods that validate `X`\n           should set `reset=False`.\n\n    validate_separately : False or tuple of dicts, default=False\n        Only used if `y` is not `None`.\n        If `False`, call :func:`~sklearn.utils.check_X_y`. Else, it must be a tuple of\n        kwargs to be used for calling :func:`~sklearn.utils.check_array` on `X` and `y`\n        respectively.\n\n        `estimator=self` is automatically added to these dicts to generate\n        more informative error message in case of invalid input data.\n\n    skip_check_array : bool, default=False\n        If `True`, `X` and `y` are unchanged and only `feature_names_in_` and\n        `n_features_in_` are checked. Otherwise, :func:`~sklearn.utils.check_array`\n        is called on `X` and `y`.\n\n    **check_params : kwargs\n        Parameters passed to :func:`~sklearn.utils.check_array` or\n        :func:`~sklearn.utils.check_X_y`. Ignored if validate_separately\n        is not False.\n\n        `estimator=self` is automatically added to these params to generate\n        more informative error message in case of invalid input data.\n\n    Returns\n    -------\n    out : {ndarray, sparse matrix} or tuple of these\n        The validated input. A tuple is returned if both `X` and `y` are\n        validated.\n    \"\"\"\n    _check_feature_names(_estimator, X, reset=reset)\n    tags = get_tags(_estimator)\n    if y is None and tags.target_tags.required:\n        raise ValueError(\n            f\"This {_estimator.__class__.__name__} estimator \"\n            \"requires y to be passed, but the target y is None.\"\n        )\n\n    no_val_X = isinstance(X, str) and X == \"no_validation\"\n    no_val_y = y is None or (isinstance(y, str) and y == \"no_validation\")\n\n    if no_val_X and no_val_y:\n        raise ValueError(\"Validation should be done on X, y or both.\")\n\n    default_check_params = {\"estimator\": _estimator}\n    check_params = {**default_check_params, **check_params}\n\n    if skip_check_array:\n        if not no_val_X and no_val_y:\n            out = X\n        elif no_val_X and not no_val_y:\n            out = y\n        else:\n            out = X, y\n    elif not no_val_X and no_val_y:\n        out = check_array(X, input_name=\"X\", **check_params)\n    elif no_val_X and not no_val_y:\n        out = _check_y(y, **check_params)\n    else:\n        if validate_separately:\n            # We need this because some estimators validate X and y\n            # separately, and in general, separately calling check_array()\n            # on X and y isn't equivalent to just calling check_X_y()\n            # :(\n            check_X_params, check_y_params = validate_separately\n            if \"estimator\" not in check_X_params:\n                check_X_params = {**default_check_params, **check_X_params}\n            X = check_array(X, input_name=\"X\", **check_X_params)\n            if \"estimator\" not in check_y_params:\n                check_y_params = {**default_check_params, **check_y_params}\n            y = check_array(y, input_name=\"y\", **check_y_params)\n        else:\n            X, y = check_X_y(X, y, **check_params)\n        out = X, y\n\n    if not no_val_X and check_params.get(\"ensure_2d\", True):\n        _check_n_features(_estimator, X, reset=reset)\n\n    return out"
}