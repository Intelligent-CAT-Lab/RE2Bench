{
    "scikit-learn.sklearn.externals.array_api_compat.numpy._aliases.asarray": "def asarray(\n    obj: Array | complex | NestedSequence[complex] | SupportsBufferProtocol,\n    /,\n    *,\n    dtype: DType | None = None,\n    device: Device | None = None,\n    copy: _Copy | None = None,\n    **kwargs: Any,\n) -> Array:\n    \"\"\"\n    Array API compatibility wrapper for asarray().\n\n    See the corresponding documentation in the array library and/or the array API\n    specification for more details.\n    \"\"\"\n    _helpers._check_device(np, device)\n\n    if copy is None:\n        copy = np._CopyMode.IF_NEEDED\n    elif copy is False:\n        copy = np._CopyMode.NEVER\n    elif copy is True:\n        copy = np._CopyMode.ALWAYS\n\n    return np.array(obj, copy=copy, dtype=dtype, **kwargs)  # pyright: ignore",
    "scikit-learn.sklearn.utils._arpack._init_arpack_v0": "def _init_arpack_v0(size, random_state):\n    \"\"\"Initialize the starting vector for iteration in ARPACK functions.\n\n    Initialize an ndarray with values sampled from the uniform distribution on\n    [-1, 1]. This initialization model has been chosen to be consistent with\n    the ARPACK one as another initialization can lead to convergence issues.\n\n    Parameters\n    ----------\n    size : int\n        The size of the eigenvalue vector to be initialized.\n\n    random_state : int, RandomState instance or None, default=None\n        The seed of the pseudo random number generator used to generate a\n        uniform distribution. If int, random_state is the seed used by the\n        random number generator; If RandomState instance, random_state is the\n        random number generator; If None, the random number generator is the\n        RandomState instance used by `np.random`.\n\n    Returns\n    -------\n    v0 : ndarray of shape (size,)\n        The initialized vector.\n    \"\"\"\n    random_state = check_random_state(random_state)\n    v0 = random_state.uniform(-1, 1, size)\n    return v0",
    "scikit-learn.sklearn.utils.extmath._randomized_svd": "def _randomized_svd(\n    M,\n    n_components,\n    *,\n    n_oversamples=10,\n    n_iter=\"auto\",\n    power_iteration_normalizer=\"auto\",\n    transpose=\"auto\",\n    flip_sign=True,\n    random_state=None,\n    svd_lapack_driver=\"gesdd\",\n):\n    \"\"\"Body of randomized_svd without input validation.\"\"\"\n    xp, is_array_api_compliant = get_namespace(M)\n\n    if sparse.issparse(M) and M.format in (\"lil\", \"dok\"):\n        warnings.warn(\n            \"Calculating SVD of a {} is expensive. \"\n            \"csr_matrix is more efficient.\".format(type(M).__name__),\n            sparse.SparseEfficiencyWarning,\n        )\n\n    random_state = check_random_state(random_state)\n    n_random = n_components + n_oversamples\n    n_samples, n_features = M.shape\n\n    if n_iter == \"auto\":\n        # Checks if the number of iterations is explicitly specified\n        # Adjust n_iter. 7 was found a good compromise for PCA. See #5299\n        n_iter = 7 if n_components < 0.1 * min(M.shape) else 4\n\n    if transpose == \"auto\":\n        transpose = n_samples < n_features\n    if transpose:\n        # this implementation is a bit faster with smaller shape[1]\n        M = M.T\n\n    Q = _randomized_range_finder(\n        M,\n        size=n_random,\n        n_iter=n_iter,\n        power_iteration_normalizer=power_iteration_normalizer,\n        random_state=random_state,\n    )\n\n    # project M to the (k + p) dimensional space using the basis vectors\n    B = Q.T @ M\n\n    # compute the SVD on the thin matrix: (k + p) wide\n    if is_array_api_compliant:\n        Uhat, s, Vt = xp.linalg.svd(B, full_matrices=False)\n    else:\n        # When array_api_dispatch is disabled, rely on scipy.linalg\n        # instead of numpy.linalg to avoid introducing a behavior change w.r.t.\n        # previous versions of scikit-learn.\n        Uhat, s, Vt = linalg.svd(\n            B, full_matrices=False, lapack_driver=svd_lapack_driver\n        )\n    del B\n    U = Q @ Uhat\n\n    if flip_sign:\n        if not transpose:\n            U, Vt = svd_flip(U, Vt)\n        else:\n            # In case of transpose u_based_decision=false\n            # to actually flip based on u and not v.\n            U, Vt = svd_flip(U, Vt, u_based_decision=False)\n\n    if transpose:\n        # transpose back the results according to the input convention\n        return Vt[:n_components, :].T, s[:n_components], U[:, :n_components].T\n    else:\n        return U[:, :n_components], s[:n_components], Vt[:n_components, :]",
    "scikit-learn.sklearn.utils.extmath.svd_flip": "def svd_flip(u, v, u_based_decision=True):\n    \"\"\"Sign correction to ensure deterministic output from SVD.\n\n    Adjusts the columns of u and the rows of v such that the loadings in the\n    columns in u that are largest in absolute value are always positive.\n\n    If u_based_decision is False, then the same sign correction is applied to\n    so that the rows in v that are largest in absolute value are always\n    positive.\n\n    Parameters\n    ----------\n    u : ndarray\n        Parameters u and v are the output of `linalg.svd` or\n        :func:`~sklearn.utils.extmath.randomized_svd`, with matching inner\n        dimensions so one can compute `np.dot(u * s, v)`.\n        u can be None if `u_based_decision` is False.\n\n    v : ndarray\n        Parameters u and v are the output of `linalg.svd` or\n        :func:`~sklearn.utils.extmath.randomized_svd`, with matching inner\n        dimensions so one can compute `np.dot(u * s, v)`. The input v should\n        really be called vt to be consistent with scipy's output.\n        v can be None if `u_based_decision` is True.\n\n    u_based_decision : bool, default=True\n        If True, use the columns of u as the basis for sign flipping.\n        Otherwise, use the rows of v. The choice of which variable to base the\n        decision on is generally algorithm dependent.\n\n    Returns\n    -------\n    u_adjusted : ndarray\n        Array u with adjusted columns and the same dimensions as u.\n\n    v_adjusted : ndarray\n        Array v with adjusted rows and the same dimensions as v.\n    \"\"\"\n    xp, _ = get_namespace(*[a for a in [u, v] if a is not None])\n\n    if u_based_decision:\n        # columns of u, rows of v, or equivalently rows of u.T and v\n        max_abs_u_cols = xp.argmax(xp.abs(u.T), axis=1)\n        shift = xp.arange(u.T.shape[0], device=device(u))\n        indices = max_abs_u_cols + shift * u.T.shape[1]\n        signs = xp.sign(xp.take(xp.reshape(u.T, (-1,)), indices, axis=0))\n        u *= signs[np.newaxis, :]\n        if v is not None:\n            v *= signs[:, np.newaxis]\n    else:\n        # rows of v, columns of u\n        max_abs_v_rows = xp.argmax(xp.abs(v), axis=1)\n        shift = xp.arange(v.shape[0], device=device(v))\n        indices = max_abs_v_rows + shift * v.shape[1]\n        signs = xp.sign(xp.take(xp.reshape(v, (-1,)), indices, axis=0))\n        if u is not None:\n            u *= signs[np.newaxis, :]\n        v *= signs[:, np.newaxis]\n    return u, v",
    "scikit-learn.sklearn.utils.sparsefuncs.mean_variance_axis": "def mean_variance_axis(X, axis, weights=None, return_sum_weights=False):\n    \"\"\"Compute mean and variance along an axis on a CSR or CSC matrix.\n\n    Parameters\n    ----------\n    X : sparse matrix of shape (n_samples, n_features)\n        Input data. It can be of CSR or CSC format.\n\n    axis : {0, 1}\n        Axis along which the axis should be computed.\n\n    weights : ndarray of shape (n_samples,) or (n_features,), default=None\n        If axis is set to 0 shape is (n_samples,) or\n        if axis is set to 1 shape is (n_features,).\n        If it is set to None, then samples are equally weighted.\n\n        .. versionadded:: 0.24\n\n    return_sum_weights : bool, default=False\n        If True, returns the sum of weights seen for each feature\n        if `axis=0` or each sample if `axis=1`.\n\n        .. versionadded:: 0.24\n\n    Returns\n    -------\n\n    means : ndarray of shape (n_features,), dtype=floating\n        Feature-wise means.\n\n    variances : ndarray of shape (n_features,), dtype=floating\n        Feature-wise variances.\n\n    sum_weights : ndarray of shape (n_features,), dtype=floating\n        Returned if `return_sum_weights` is `True`.\n\n    Examples\n    --------\n    >>> from sklearn.utils import sparsefuncs\n    >>> from scipy import sparse\n    >>> import numpy as np\n    >>> indptr = np.array([0, 3, 4, 4, 4])\n    >>> indices = np.array([0, 1, 2, 2])\n    >>> data = np.array([8, 1, 2, 5])\n    >>> scale = np.array([2, 3, 2])\n    >>> csr = sparse.csr_matrix((data, indices, indptr))\n    >>> csr.todense()\n    matrix([[8, 1, 2],\n            [0, 0, 5],\n            [0, 0, 0],\n            [0, 0, 0]])\n    >>> sparsefuncs.mean_variance_axis(csr, axis=0)\n    (array([2.  , 0.25, 1.75]), array([12.    ,  0.1875,  4.1875]))\n    \"\"\"\n    _raise_error_wrong_axis(axis)\n\n    if sp.issparse(X) and X.format == \"csr\":\n        if axis == 0:\n            return _csr_mean_var_axis0(\n                X, weights=weights, return_sum_weights=return_sum_weights\n            )\n        else:\n            return _csc_mean_var_axis0(\n                X.T, weights=weights, return_sum_weights=return_sum_weights\n            )\n    elif sp.issparse(X) and X.format == \"csc\":\n        if axis == 0:\n            return _csc_mean_var_axis0(\n                X, weights=weights, return_sum_weights=return_sum_weights\n            )\n        else:\n            return _csr_mean_var_axis0(\n                X.T, weights=weights, return_sum_weights=return_sum_weights\n            )\n    else:\n        _raise_typeerror(X)",
    "scikit-learn.sklearn.utils.sparsefuncs._implicit_column_offset": "def _implicit_column_offset(X, offset):\n    \"\"\"Create an implicitly offset linear operator.\n\n    This is used by PCA on sparse data to avoid densifying the whole data\n    matrix.\n\n    Params\n    ------\n        X : sparse matrix of shape (n_samples, n_features)\n        offset : ndarray of shape (n_features,)\n\n    Returns\n    -------\n    centered : LinearOperator\n    \"\"\"\n    offset = offset[None, :]\n    XT = X.T\n    return LinearOperator(\n        matvec=lambda x: X @ x - offset @ x,\n        matmat=lambda x: X @ x - offset @ x,\n        rmatvec=lambda x: XT @ x - (offset * x.sum()),\n        rmatmat=lambda x: XT @ x - offset.T @ x.sum(axis=0)[None, :],\n        dtype=X.dtype,\n        shape=X.shape,\n    )",
    "scikit-learn.sklearn.utils.validation.check_random_state": "def check_random_state(seed):\n    \"\"\"Turn seed into an np.random.RandomState instance.\n\n    Parameters\n    ----------\n    seed : None, int or instance of RandomState\n        If seed is None, return the RandomState singleton used by np.random.\n        If seed is an int, return a new RandomState instance seeded with seed.\n        If seed is already a RandomState instance, return it.\n        Otherwise raise ValueError.\n\n    Returns\n    -------\n    :class:`numpy:numpy.random.RandomState`\n        The random state object based on `seed` parameter.\n\n    Examples\n    --------\n    >>> from sklearn.utils.validation import check_random_state\n    >>> check_random_state(42)\n    RandomState(MT19937) at 0x...\n    \"\"\"\n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, numbers.Integral):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState):\n        return seed\n    raise ValueError(\n        \"%r cannot be used to seed a numpy.random.RandomState instance\" % seed\n    )"
}