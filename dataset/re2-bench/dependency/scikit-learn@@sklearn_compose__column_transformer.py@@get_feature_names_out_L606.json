{
    "scikit-learn.sklearn.compose._column_transformer._iter": "def _iter(self, fitted, column_as_labels, skip_drop, skip_empty_columns):\n    \"\"\"\n    Generate (name, trans, columns, weight) tuples.\n\n\n    Parameters\n    ----------\n    fitted : bool\n        If True, use the fitted transformers (``self.transformers_``) to\n        iterate through transformers, else use the transformers passed by\n        the user (``self.transformers``).\n\n    column_as_labels : bool\n        If True, columns are returned as string labels. If False, columns\n        are returned as they were given by the user. This can only be True\n        if the ``ColumnTransformer`` is already fitted.\n\n    skip_drop : bool\n        If True, 'drop' transformers are filtered out.\n\n    skip_empty_columns : bool\n        If True, transformers with empty selected columns are filtered out.\n\n    Yields\n    ------\n    A generator of tuples containing:\n        - name : the name of the transformer\n        - transformer : the transformer object\n        - columns : the columns for that transformer\n        - weight : the weight of the transformer\n    \"\"\"\n    if fitted:\n        transformers = self.transformers_\n    else:\n        # interleave the validated column specifiers\n        transformers = [\n            (name, trans, column)\n            for (name, trans, _), column in zip(self.transformers, self._columns)\n        ]\n        # add transformer tuple for remainder\n        if self._remainder[2]:\n            transformers = chain(transformers, [self._remainder])\n\n    get_weight = (self.transformer_weights or {}).get\n\n    for name, trans, columns in transformers:\n        if skip_drop and trans == \"drop\":\n            continue\n        if skip_empty_columns and _is_empty_column_selection(columns):\n            continue\n\n        if column_as_labels:\n            # Convert all columns to using their string labels\n            columns_is_scalar = np.isscalar(columns)\n\n            indices = self._transformer_to_input_indices[name]\n            columns = self.feature_names_in_[indices]\n\n            if columns_is_scalar:\n                # selection is done with one dimension\n                columns = columns[0]\n\n        yield (name, trans, columns, get_weight(name))",
    "scikit-learn.sklearn.compose._column_transformer._get_feature_name_out_for_transformer": "def _get_feature_name_out_for_transformer(self, name, trans, feature_names_in):\n    \"\"\"Gets feature names of transformer.\n\n    Used in conjunction with self._iter(fitted=True) in get_feature_names_out.\n    \"\"\"\n    column_indices = self._transformer_to_input_indices[name]\n    names = feature_names_in[column_indices]\n    # An actual transformer\n    if not hasattr(trans, \"get_feature_names_out\"):\n        raise AttributeError(\n            f\"Transformer {name} (type {type(trans).__name__}) does \"\n            \"not provide get_feature_names_out.\"\n        )\n    return trans.get_feature_names_out(names)",
    "scikit-learn.sklearn.compose._column_transformer._add_prefix_for_feature_names_out": "def _add_prefix_for_feature_names_out(self, transformer_with_feature_names_out):\n    \"\"\"Add prefix for feature names out that includes the transformer names.\n\n    Parameters\n    ----------\n    transformer_with_feature_names_out : list of tuples of (str, array-like of str)\n        The tuple consistent of the transformer's name and its feature names out.\n\n    Returns\n    -------\n    feature_names_out : ndarray of shape (n_features,), dtype=str\n        Transformed feature names.\n    \"\"\"\n    feature_names_out_callable = None\n    if callable(self.verbose_feature_names_out):\n        feature_names_out_callable = self.verbose_feature_names_out\n    elif isinstance(self.verbose_feature_names_out, str):\n        feature_names_out_callable = partial(\n            _feature_names_out_with_str_format,\n            str_format=self.verbose_feature_names_out,\n        )\n    elif self.verbose_feature_names_out is True:\n        feature_names_out_callable = partial(\n            _feature_names_out_with_str_format,\n            str_format=\"{transformer_name}__{feature_name}\",\n        )\n\n    if feature_names_out_callable is not None:\n        # Prefix the feature names out with the transformers name\n        names = list(\n            chain.from_iterable(\n                (feature_names_out_callable(name, i) for i in feature_names_out)\n                for name, feature_names_out in transformer_with_feature_names_out\n            )\n        )\n        return np.asarray(names, dtype=object)\n\n    # verbose_feature_names_out is False\n    # Check that names are all unique without a prefix\n    feature_names_count = Counter(\n        chain.from_iterable(s for _, s in transformer_with_feature_names_out)\n    )\n    top_6_overlap = [\n        name for name, count in feature_names_count.most_common(6) if count > 1\n    ]\n    top_6_overlap.sort()\n    if top_6_overlap:\n        if len(top_6_overlap) == 6:\n            # There are more than 5 overlapping names, we only show the 5\n            # of the feature names\n            names_repr = str(top_6_overlap[:5])[:-1] + \", ...]\"\n        else:\n            names_repr = str(top_6_overlap)\n        raise ValueError(\n            f\"Output feature names: {names_repr} are not unique. Please set \"\n            \"verbose_feature_names_out=True to add prefixes to feature names\"\n        )\n\n    return np.concatenate(\n        [name for _, name in transformer_with_feature_names_out],\n    )",
    "scikit-learn.sklearn.utils.validation.check_is_fitted": "def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):\n    \"\"\"Perform is_fitted validation for estimator.\n\n    Checks if the estimator is fitted by verifying the presence of\n    fitted attributes (ending with a trailing underscore) and otherwise\n    raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to\n    specify if the estimator is fitted or not. See\n    :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\n    for an example on how to use the API.\n\n    If no `attributes` are passed, this function will pass if an estimator is stateless.\n    An estimator can indicate it's stateless by setting the `requires_fit` tag. See\n    :ref:`estimator_tags` for more information. Note that the `requires_fit` tag\n    is ignored if `attributes` are passed.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Raises\n    ------\n    TypeError\n        If the estimator is a class or not an estimator instance\n\n    NotFittedError\n        If the attributes are not found.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.utils.validation import check_is_fitted\n    >>> from sklearn.exceptions import NotFittedError\n    >>> lr = LogisticRegression()\n    >>> try:\n    ...     check_is_fitted(lr)\n    ... except NotFittedError as exc:\n    ...     print(f\"Model is not fitted yet.\")\n    Model is not fitted yet.\n    >>> lr.fit([[1, 2], [1, 3]], [1, 0])\n    LogisticRegression()\n    >>> check_is_fitted(lr)\n    \"\"\"\n    if isclass(estimator):\n        raise TypeError(\"{} is a class, not an instance.\".format(estimator))\n    if msg is None:\n        msg = (\n            \"This %(name)s instance is not fitted yet. Call 'fit' with \"\n            \"appropriate arguments before using this estimator.\"\n        )\n\n    if not hasattr(estimator, \"fit\"):\n        raise TypeError(\"%s is not an estimator instance.\" % (estimator))\n\n    tags = get_tags(estimator)\n\n    if not tags.requires_fit and attributes is None:\n        return\n\n    if not _is_fitted(estimator, attributes, all_or_any):\n        raise NotFittedError(msg % {\"name\": type(estimator).__name__})",
    "scikit-learn.sklearn.utils.validation._check_feature_names_in": "def _check_feature_names_in(estimator, input_features=None, *, generate_names=True):\n    \"\"\"Check `input_features` and generate names if needed.\n\n    Commonly used in :term:`get_feature_names_out`.\n\n    Parameters\n    ----------\n    input_features : array-like of str or None, default=None\n        Input features.\n\n        - If `input_features` is `None`, then `feature_names_in_` is\n          used as feature names in. If `feature_names_in_` is not defined,\n          then the following input feature names are generated:\n          `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n        - If `input_features` is an array-like, then `input_features` must\n          match `feature_names_in_` if `feature_names_in_` is defined.\n\n    generate_names : bool, default=True\n        Whether to generate names when `input_features` is `None` and\n        `estimator.feature_names_in_` is not defined. This is useful for transformers\n        that validates `input_features` but do not require them in\n        :term:`get_feature_names_out` e.g. `PCA`.\n\n    Returns\n    -------\n    feature_names_in : ndarray of str or `None`\n        Feature names in.\n    \"\"\"\n\n    feature_names_in_ = getattr(estimator, \"feature_names_in_\", None)\n    n_features_in_ = getattr(estimator, \"n_features_in_\", None)\n\n    if input_features is not None:\n        input_features = np.asarray(input_features, dtype=object)\n        if feature_names_in_ is not None and not np.array_equal(\n            feature_names_in_, input_features\n        ):\n            raise ValueError(\"input_features is not equal to feature_names_in_\")\n\n        if n_features_in_ is not None and len(input_features) != n_features_in_:\n            raise ValueError(\n                \"input_features should have length equal to number of \"\n                f\"features ({n_features_in_}), got {len(input_features)}\"\n            )\n        return input_features\n\n    if feature_names_in_ is not None:\n        return feature_names_in_\n\n    if not generate_names:\n        return\n\n    # Generates feature names if `n_features_in_` is defined\n    if n_features_in_ is None:\n        raise ValueError(\"Unable to generate feature names without n_features_in_\")\n\n    return np.asarray([f\"x{i}\" for i in range(n_features_in_)], dtype=object)"
}