{
    "scikit-learn.sklearn.base.__sklearn_tags__": "def __sklearn_tags__(self):\n    tags = super().__sklearn_tags__()\n    tags.transformer_tags = TransformerTags()\n    return tags",
    "scikit-learn.sklearn.base.is_classifier": "def is_classifier(estimator):\n    \"\"\"Return True if the given estimator is (probably) a classifier.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator object to test.\n\n    Returns\n    -------\n    out : bool\n        True if estimator is a classifier and False otherwise.\n\n    Examples\n    --------\n    >>> from sklearn.base import is_classifier\n    >>> from sklearn.cluster import KMeans\n    >>> from sklearn.svm import SVC, SVR\n    >>> classifier = SVC()\n    >>> regressor = SVR()\n    >>> kmeans = KMeans()\n    >>> is_classifier(classifier)\n    True\n    >>> is_classifier(regressor)\n    False\n    >>> is_classifier(kmeans)\n    False\n    \"\"\"\n    return get_tags(estimator).estimator_type == \"classifier\"",
    "scikit-learn.sklearn.neighbors._base.__sklearn_tags__": "def __sklearn_tags__(self):\n    tags = super().__sklearn_tags__()\n    tags.input_tags.allow_nan = self.metric == \"nan_euclidean\"\n    return tags",
    "scikit-learn.sklearn.neighbors._base._check_precomputed": "def _check_precomputed(X):\n    \"\"\"Check precomputed distance matrix.\n\n    If the precomputed distance matrix is sparse, it checks that the non-zero\n    entries are sorted by distances. If not, the matrix is copied and sorted.\n\n    Parameters\n    ----------\n    X : {sparse matrix, array-like}, (n_samples, n_samples)\n        Distance matrix to other samples. X may be a sparse matrix, in which\n        case only non-zero elements may be considered neighbors.\n\n    Returns\n    -------\n    X : {sparse matrix, array-like}, (n_samples, n_samples)\n        Distance matrix to other samples. X may be a sparse matrix, in which\n        case only non-zero elements may be considered neighbors.\n    \"\"\"\n    if not issparse(X):\n        X = check_array(X, ensure_non_negative=True, input_name=\"X\")\n        return X\n    else:\n        graph = X\n\n    if graph.format not in (\"csr\", \"csc\", \"coo\", \"lil\"):\n        raise TypeError(\n            \"Sparse matrix in {!r} format is not supported due to \"\n            \"its handling of explicit zeros\".format(graph.format)\n        )\n    copied = graph.format != \"csr\"\n    graph = check_array(\n        graph,\n        accept_sparse=\"csr\",\n        ensure_non_negative=True,\n        input_name=\"precomputed distance matrix\",\n    )\n    graph = sort_graph_by_row_values(graph, copy=not copied, warn_when_not_sorted=True)\n\n    return graph",
    "scikit-learn.sklearn.neighbors._base._check_algorithm_metric": "def _check_algorithm_metric(self):\n    if self.algorithm == \"auto\":\n        if self.metric == \"precomputed\":\n            alg_check = \"brute\"\n        elif (\n            callable(self.metric)\n            or self.metric in VALID_METRICS[\"ball_tree\"]\n            or isinstance(self.metric, DistanceMetric)\n        ):\n            alg_check = \"ball_tree\"\n        else:\n            alg_check = \"brute\"\n    else:\n        alg_check = self.algorithm\n\n    if callable(self.metric):\n        if self.algorithm == \"kd_tree\":\n            # callable metric is only valid for brute force and ball_tree\n            raise ValueError(\n                \"kd_tree does not support callable metric '%s'\"\n                \"Function call overhead will result\"\n                \"in very poor performance.\" % self.metric\n            )\n    elif self.metric not in VALID_METRICS[alg_check] and not isinstance(\n        self.metric, DistanceMetric\n    ):\n        raise ValueError(\n            \"Metric '%s' not valid. Use \"\n            \"sorted(sklearn.neighbors.VALID_METRICS['%s']) \"\n            \"to get valid options. \"\n            \"Metric can also be a callable function.\" % (self.metric, alg_check)\n        )\n\n    if self.metric_params is not None and \"p\" in self.metric_params:\n        if self.p is not None:\n            warnings.warn(\n                (\n                    \"Parameter p is found in metric_params. \"\n                    \"The corresponding parameter from __init__ \"\n                    \"is ignored.\"\n                ),\n                SyntaxWarning,\n                stacklevel=3,\n            )",
    "scikit-learn.sklearn.neighbors._classification.__sklearn_tags__": "def __sklearn_tags__(self):\n    tags = super().__sklearn_tags__()\n    tags.classifier_tags.multi_label = True\n    return tags",
    "scikit-learn.sklearn.neighbors._regression.__sklearn_tags__": "def __sklearn_tags__(self):\n    tags = super().__sklearn_tags__()\n    # For cross-validation routines to split data correctly\n    tags.input_tags.pairwise = self.metric == \"precomputed\"\n    return tags",
    "scikit-learn.sklearn.utils._tags.get_tags": "def get_tags(estimator) -> Tags:\n    \"\"\"Get estimator tags.\n\n    :class:`~sklearn.BaseEstimator` provides the estimator tags machinery.\n\n    For scikit-learn built-in estimators, we should still rely on\n    `self.__sklearn_tags__()`. `get_tags(est)` should be used when we\n    are not sure where `est` comes from: typically\n    `get_tags(self.estimator)` where `self` is a meta-estimator, or in\n    the common checks.\n\n    .. versionadded:: 1.6\n\n    Parameters\n    ----------\n    estimator : estimator object\n        The estimator from which to get the tag.\n\n    Returns\n    -------\n    tags : :class:`~.sklearn.utils.Tags`\n        The estimator tags.\n    \"\"\"\n\n    try:\n        tags = estimator.__sklearn_tags__()\n    except AttributeError as exc:\n        if \"object has no attribute '__sklearn_tags__'\" in str(exc):\n            # Happens when `__sklearn_tags__` is implemented by calling\n            # `super().__sklearn_tags__()` but there is no `__sklearn_tags__`\n            # method in the base class. Typically happens when only inheriting\n            # from Mixins.\n\n            raise AttributeError(\n                f\"The following error was raised: {exc}. It seems that \"\n                \"there are no classes that implement `__sklearn_tags__` \"\n                \"in the MRO and/or all classes in the MRO call \"\n                \"`super().__sklearn_tags__()`. Make sure to inherit from \"\n                \"`BaseEstimator` which implements `__sklearn_tags__` (or \"\n                \"alternatively define `__sklearn_tags__` but we don't recommend \"\n                \"this approach). Note that `BaseEstimator` needs to be on the \"\n                \"right side of other Mixins in the inheritance order.\"\n            )\n        else:\n            raise\n\n    return tags",
    "scikit-learn.sklearn.utils.multiclass.check_classification_targets": "def check_classification_targets(y):\n    \"\"\"Ensure that target y is of a non-regression type.\n\n    Only the following target types (as defined in type_of_target) are allowed:\n        'binary', 'multiclass', 'multiclass-multioutput',\n        'multilabel-indicator', 'multilabel-sequences'\n\n    Parameters\n    ----------\n    y : array-like\n        Target values.\n    \"\"\"\n    y_type = type_of_target(y, input_name=\"y\")\n    if y_type not in [\n        \"binary\",\n        \"multiclass\",\n        \"multiclass-multioutput\",\n        \"multilabel-indicator\",\n        \"multilabel-sequences\",\n    ]:\n        raise ValueError(\n            f\"Unknown label type: {y_type}. Maybe you are trying to fit a \"\n            \"classifier, which expects discrete classes on a \"\n            \"regression target with continuous values.\"\n        )\n\n    if \"multiclass\" in y_type:\n        n_samples = _num_samples(y)\n        if n_samples > 20 and cached_unique(y).shape[0] > round(0.5 * n_samples):\n            # Only raise the warning when we have at least 20 samples.\n            warnings.warn(\n                \"The number of unique classes is greater than 50% of the number \"\n                \"of samples. `y` could represent a regression problem, not a \"\n                \"classification problem.\",\n                UserWarning,\n                stacklevel=2,\n            )",
    "scikit-learn.sklearn.utils.validation.validate_data": "def validate_data(\n    _estimator,\n    /,\n    X=\"no_validation\",\n    y=\"no_validation\",\n    reset=True,\n    validate_separately=False,\n    skip_check_array=False,\n    **check_params,\n):\n    \"\"\"Validate input data and set or check feature names and counts of the input.\n\n    This helper function should be used in an estimator that requires input\n    validation. This mutates the estimator and sets the `n_features_in_` and\n    `feature_names_in_` attributes if `reset=True`.\n\n    .. versionadded:: 1.6\n\n    Parameters\n    ----------\n    _estimator : estimator instance\n        The estimator to validate the input for.\n\n    X : {array-like, sparse matrix, dataframe} of shape \\\n            (n_samples, n_features), default='no validation'\n        The input samples.\n        If `'no_validation'`, no validation is performed on `X`. This is\n        useful for meta-estimator which can delegate input validation to\n        their underlying estimator(s). In that case `y` must be passed and\n        the only accepted `check_params` are `multi_output` and\n        `y_numeric`.\n\n    y : array-like of shape (n_samples,), default='no_validation'\n        The targets.\n\n        - If `None`, :func:`~sklearn.utils.check_array` is called on `X`. If\n          the estimator's `requires_y` tag is True, then an error will be raised.\n        - If `'no_validation'`, :func:`~sklearn.utils.check_array` is called\n          on `X` and the estimator's `requires_y` tag is ignored. This is a default\n          placeholder and is never meant to be explicitly set. In that case `X` must be\n          passed.\n        - Otherwise, only `y` with `_check_y` or both `X` and `y` are checked with\n          either :func:`~sklearn.utils.check_array` or\n          :func:`~sklearn.utils.check_X_y` depending on `validate_separately`.\n\n    reset : bool, default=True\n        Whether to reset the `n_features_in_` attribute.\n        If False, the input will be checked for consistency with data\n        provided when reset was last True.\n\n        .. note::\n\n           It is recommended to call `reset=True` in `fit` and in the first\n           call to `partial_fit`. All other methods that validate `X`\n           should set `reset=False`.\n\n    validate_separately : False or tuple of dicts, default=False\n        Only used if `y` is not `None`.\n        If `False`, call :func:`~sklearn.utils.check_X_y`. Else, it must be a tuple of\n        kwargs to be used for calling :func:`~sklearn.utils.check_array` on `X` and `y`\n        respectively.\n\n        `estimator=self` is automatically added to these dicts to generate\n        more informative error message in case of invalid input data.\n\n    skip_check_array : bool, default=False\n        If `True`, `X` and `y` are unchanged and only `feature_names_in_` and\n        `n_features_in_` are checked. Otherwise, :func:`~sklearn.utils.check_array`\n        is called on `X` and `y`.\n\n    **check_params : kwargs\n        Parameters passed to :func:`~sklearn.utils.check_array` or\n        :func:`~sklearn.utils.check_X_y`. Ignored if validate_separately\n        is not False.\n\n        `estimator=self` is automatically added to these params to generate\n        more informative error message in case of invalid input data.\n\n    Returns\n    -------\n    out : {ndarray, sparse matrix} or tuple of these\n        The validated input. A tuple is returned if both `X` and `y` are\n        validated.\n    \"\"\"\n    _check_feature_names(_estimator, X, reset=reset)\n    tags = get_tags(_estimator)\n    if y is None and tags.target_tags.required:\n        raise ValueError(\n            f\"This {_estimator.__class__.__name__} estimator \"\n            \"requires y to be passed, but the target y is None.\"\n        )\n\n    no_val_X = isinstance(X, str) and X == \"no_validation\"\n    no_val_y = y is None or (isinstance(y, str) and y == \"no_validation\")\n\n    if no_val_X and no_val_y:\n        raise ValueError(\"Validation should be done on X, y or both.\")\n\n    default_check_params = {\"estimator\": _estimator}\n    check_params = {**default_check_params, **check_params}\n\n    if skip_check_array:\n        if not no_val_X and no_val_y:\n            out = X\n        elif no_val_X and not no_val_y:\n            out = y\n        else:\n            out = X, y\n    elif not no_val_X and no_val_y:\n        out = check_array(X, input_name=\"X\", **check_params)\n    elif no_val_X and not no_val_y:\n        out = _check_y(y, **check_params)\n    else:\n        if validate_separately:\n            # We need this because some estimators validate X and y\n            # separately, and in general, separately calling check_array()\n            # on X and y isn't equivalent to just calling check_X_y()\n            # :(\n            check_X_params, check_y_params = validate_separately\n            if \"estimator\" not in check_X_params:\n                check_X_params = {**default_check_params, **check_X_params}\n            X = check_array(X, input_name=\"X\", **check_X_params)\n            if \"estimator\" not in check_y_params:\n                check_y_params = {**default_check_params, **check_y_params}\n            y = check_array(y, input_name=\"y\", **check_y_params)\n        else:\n            X, y = check_X_y(X, y, **check_params)\n        out = X, y\n\n    if not no_val_X and check_params.get(\"ensure_2d\", True):\n        _check_n_features(_estimator, X, reset=reset)\n\n    return out",
    "scikit-learn.sklearn.utils.validation.check_array": "def check_array(\n    array,\n    accept_sparse=False,\n    *,\n    accept_large_sparse=True,\n    dtype=\"numeric\",\n    order=None,\n    copy=False,\n    force_writeable=False,\n    ensure_all_finite=True,\n    ensure_non_negative=False,\n    ensure_2d=True,\n    allow_nd=False,\n    ensure_min_samples=1,\n    ensure_min_features=1,\n    estimator=None,\n    input_name=\"\",\n):\n    \"\"\"Input validation on an array, list, sparse matrix or similar.\n\n    By default, the input is checked to be a non-empty 2D array containing\n    only finite values. If the dtype of the array is object, attempt\n    converting to float, raising on failure.\n\n    Parameters\n    ----------\n    array : object\n        Input object to check / convert.\n\n    accept_sparse : str, bool or list/tuple of str, default=False\n        String[s] representing allowed sparse matrix formats, such as 'csc',\n        'csr', etc. If the input is sparse but not in the allowed format,\n        it will be converted to the first listed format. True allows the input\n        to be any format. False means that a sparse matrix input will\n        raise an error.\n\n    accept_large_sparse : bool, default=True\n        If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by\n        accept_sparse, accept_large_sparse=False will cause it to be accepted\n        only if its indices are stored with a 32-bit dtype.\n\n        .. versionadded:: 0.20\n\n    dtype : 'numeric', type, list of type or None, default='numeric'\n        Data type of result. If None, the dtype of the input is preserved.\n        If \"numeric\", dtype is preserved unless array.dtype is object.\n        If dtype is a list of types, conversion on the first type is only\n        performed if the dtype of the input is not in the list.\n\n    order : {'F', 'C'} or None, default=None\n        Whether an array will be forced to be fortran or c-style.\n        When order is None (default), then if copy=False, nothing is ensured\n        about the memory layout of the output array; otherwise (copy=True)\n        the memory layout of the returned array is kept as close as possible\n        to the original array.\n\n    copy : bool, default=False\n        Whether a forced copy will be triggered. If copy=False, a copy might\n        be triggered by a conversion.\n\n    force_writeable : bool, default=False\n        Whether to force the output array to be writeable. If True, the returned array\n        is guaranteed to be writeable, which may require a copy. Otherwise the\n        writeability of the input array is preserved.\n\n        .. versionadded:: 1.6\n\n    ensure_all_finite : bool or 'allow-nan', default=True\n        Whether to raise an error on np.inf, np.nan, pd.NA in array. The\n        possibilities are:\n\n        - True: Force all values of array to be finite.\n        - False: accepts np.inf, np.nan, pd.NA in array.\n        - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n          cannot be infinite.\n\n        .. versionadded:: 1.6\n           `force_all_finite` was renamed to `ensure_all_finite`.\n\n    ensure_non_negative : bool, default=False\n        Make sure the array has only non-negative values. If True, an array that\n        contains negative values will raise a ValueError.\n\n        .. versionadded:: 1.6\n\n    ensure_2d : bool, default=True\n        Whether to raise a value error if array is not 2D.\n\n    allow_nd : bool, default=False\n        Whether to allow array.ndim > 2.\n\n    ensure_min_samples : int, default=1\n        Make sure that the array has a minimum number of samples in its first\n        axis (rows for a 2D array). Setting to 0 disables this check.\n\n    ensure_min_features : int, default=1\n        Make sure that the 2D array has some minimum number of features\n        (columns). The default value of 1 rejects empty datasets.\n        This check is only enforced when the input data has effectively 2\n        dimensions or is originally 1D and ``ensure_2d`` is True. Setting to 0\n        disables this check.\n\n    estimator : str or estimator instance, default=None\n        If passed, include the name of the estimator in warning messages.\n\n    input_name : str, default=\"\"\n        The data name used to construct the error message. In particular\n        if `input_name` is \"X\" and the data has NaN values and\n        allow_nan is False, the error message will link to the imputer\n        documentation.\n\n        .. versionadded:: 1.1.0\n\n    Returns\n    -------\n    array_converted : object\n        The converted and validated array.\n\n    Examples\n    --------\n    >>> from sklearn.utils.validation import check_array\n    >>> X = [[1, 2, 3], [4, 5, 6]]\n    >>> X_checked = check_array(X)\n    >>> X_checked\n    array([[1, 2, 3], [4, 5, 6]])\n    \"\"\"\n    if isinstance(array, np.matrix):\n        raise TypeError(\n            \"np.matrix is not supported. Please convert to a numpy array with \"\n            \"np.asarray. For more information see: \"\n            \"https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\"\n        )\n\n    xp, is_array_api_compliant = get_namespace(array)\n\n    # store reference to original array to check if copy is needed when\n    # function returns\n    array_orig = array\n\n    # store whether originally we wanted numeric dtype\n    dtype_numeric = isinstance(dtype, str) and dtype == \"numeric\"\n\n    dtype_orig = getattr(array, \"dtype\", None)\n    if not is_array_api_compliant and not hasattr(dtype_orig, \"kind\"):\n        # not a data type (e.g. a column named dtype in a pandas DataFrame)\n        dtype_orig = None\n\n    # check if the object contains several dtypes (typically a pandas\n    # DataFrame), and store them. If not, store None.\n    dtypes_orig = None\n    pandas_requires_conversion = False\n    # track if we have a Series-like object to raise a better error message\n    type_if_series = None\n    if hasattr(array, \"dtypes\") and hasattr(array.dtypes, \"__array__\"):\n        # throw warning if columns are sparse. If all columns are sparse, then\n        # array.sparse exists and sparsity will be preserved (later).\n        with suppress(ImportError):\n            from pandas import SparseDtype\n\n            def is_sparse(dtype):\n                return isinstance(dtype, SparseDtype)\n\n            if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n                warnings.warn(\n                    \"pandas.DataFrame with sparse columns found.\"\n                    \"It will be converted to a dense numpy array.\"\n                )\n\n        dtypes_orig = list(array.dtypes)\n        pandas_requires_conversion = any(\n            _pandas_dtype_needs_early_conversion(i) for i in dtypes_orig\n        )\n        if all(isinstance(dtype_iter, np.dtype) for dtype_iter in dtypes_orig):\n            dtype_orig = np.result_type(*dtypes_orig)\n        elif pandas_requires_conversion and any(d == object for d in dtypes_orig):\n            # Force object if any of the dtypes is an object\n            dtype_orig = object\n\n    elif (_is_extension_array_dtype(array) or hasattr(array, \"iloc\")) and hasattr(\n        array, \"dtype\"\n    ):\n        # array is a pandas series\n        type_if_series = type(array)\n        pandas_requires_conversion = _pandas_dtype_needs_early_conversion(array.dtype)\n        if isinstance(array.dtype, np.dtype):\n            dtype_orig = array.dtype\n        else:\n            # Set to None to let array.astype work out the best dtype\n            dtype_orig = None\n\n    if dtype_numeric:\n        if (\n            dtype_orig is not None\n            and hasattr(dtype_orig, \"kind\")\n            and dtype_orig.kind == \"O\"\n        ):\n            # if input is object, convert to float.\n            dtype = xp.float64\n        else:\n            dtype = None\n\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            # no dtype conversion required\n            dtype = None\n        else:\n            # dtype conversion required. Let's select the first element of the\n            # list of accepted types.\n            dtype = dtype[0]\n\n    if pandas_requires_conversion:\n        # pandas dataframe requires conversion earlier to handle extension dtypes with\n        # nans\n        # Use the original dtype for conversion if dtype is None\n        new_dtype = dtype_orig if dtype is None else dtype\n        array = array.astype(new_dtype)\n        # Since we converted here, we do not need to convert again later\n        dtype = None\n\n    if ensure_all_finite not in (True, False, \"allow-nan\"):\n        raise ValueError(\n            \"ensure_all_finite should be a bool or 'allow-nan'. Got \"\n            f\"{ensure_all_finite!r} instead.\"\n        )\n\n    if dtype is not None and _is_numpy_namespace(xp):\n        # convert to dtype object to conform to Array API to be use `xp.isdtype` later\n        dtype = np.dtype(dtype)\n\n    estimator_name = _check_estimator_name(estimator)\n    context = \" by %s\" % estimator_name if estimator is not None else \"\"\n\n    # When all dataframe columns are sparse, convert to a sparse array\n    if hasattr(array, \"sparse\") and array.ndim > 1:\n        with suppress(ImportError):\n            from pandas import SparseDtype\n\n            def is_sparse(dtype):\n                return isinstance(dtype, SparseDtype)\n\n            if array.dtypes.apply(is_sparse).all():\n                # DataFrame.sparse only supports `to_coo`\n                array = array.sparse.to_coo()\n                if array.dtype == np.dtype(\"object\"):\n                    unique_dtypes = set([dt.subtype.name for dt in array_orig.dtypes])\n                    if len(unique_dtypes) > 1:\n                        raise ValueError(\n                            \"Pandas DataFrame with mixed sparse extension arrays \"\n                            \"generated a sparse matrix with object dtype which \"\n                            \"can not be converted to a scipy sparse matrix.\"\n                            \"Sparse extension arrays should all have the same \"\n                            \"numeric type.\"\n                        )\n\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(\n            array,\n            accept_sparse=accept_sparse,\n            dtype=dtype,\n            copy=copy,\n            ensure_all_finite=ensure_all_finite,\n            accept_large_sparse=accept_large_sparse,\n            estimator_name=estimator_name,\n            input_name=input_name,\n        )\n        if ensure_2d and array.ndim < 2:\n            raise ValueError(\n                f\"Expected 2D input, got input with shape {array.shape}.\\n\"\n                \"Reshape your data either using array.reshape(-1, 1) if \"\n                \"your data has a single feature or array.reshape(1, -1) \"\n                \"if it contains a single sample.\"\n            )\n    else:\n        # If np.array(..) gives ComplexWarning, then we convert the warning\n        # to an error. This is needed because specifying a non complex\n        # dtype to the function converts complex to real dtype,\n        # thereby passing the test made in the lines following the scope\n        # of warnings context manager.\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter(\"error\", ComplexWarning)\n                if dtype is not None and xp.isdtype(dtype, \"integral\"):\n                    # Conversion float -> int should not contain NaN or\n                    # inf (numpy#14412). We cannot use casting='safe' because\n                    # then conversion float -> int would be disallowed.\n                    array = _asarray_with_order(array, order=order, xp=xp)\n                    if xp.isdtype(array.dtype, (\"real floating\", \"complex floating\")):\n                        _assert_all_finite(\n                            array,\n                            allow_nan=False,\n                            msg_dtype=dtype,\n                            estimator_name=estimator_name,\n                            input_name=input_name,\n                        )\n                    array = xp.astype(array, dtype, copy=False)\n                else:\n                    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            except ComplexWarning as complex_warning:\n                raise ValueError(\n                    \"Complex data not supported\\n{}\\n\".format(array)\n                ) from complex_warning\n\n        # It is possible that the np.array(..) gave no warning. This happens\n        # when no dtype conversion happened, for example dtype = None. The\n        # result is that np.array(..) produces an array of complex dtype\n        # and we need to catch and raise exception for such cases.\n        _ensure_no_complex_data(array)\n\n        if ensure_2d:\n            # If input is scalar raise error\n            if array.ndim == 0:\n                raise ValueError(\n                    \"Expected 2D array, got scalar array instead:\\narray={}.\\n\"\n                    \"Reshape your data either using array.reshape(-1, 1) if \"\n                    \"your data has a single feature or array.reshape(1, -1) \"\n                    \"if it contains a single sample.\".format(array)\n                )\n            # If input is 1D raise error\n            if array.ndim == 1:\n                # If input is a Series-like object (eg. pandas Series or polars Series)\n                if type_if_series is not None:\n                    msg = (\n                        f\"Expected a 2-dimensional container but got {type_if_series} \"\n                        \"instead. Pass a DataFrame containing a single row (i.e. \"\n                        \"single sample) or a single column (i.e. single feature) \"\n                        \"instead.\"\n                    )\n                else:\n                    msg = (\n                        f\"Expected 2D array, got 1D array instead:\\narray={array}.\\n\"\n                        \"Reshape your data either using array.reshape(-1, 1) if \"\n                        \"your data has a single feature or array.reshape(1, -1) \"\n                        \"if it contains a single sample.\"\n                    )\n                raise ValueError(msg)\n\n        if dtype_numeric and hasattr(array.dtype, \"kind\") and array.dtype.kind in \"USV\":\n            raise ValueError(\n                \"dtype='numeric' is not compatible with arrays of bytes/strings.\"\n                \"Convert your data to numeric values explicitly instead.\"\n            )\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError(\n                f\"Found array with dim {array.ndim},\"\n                f\" while dim <= 2 is required{context}.\"\n            )\n\n        if ensure_all_finite:\n            _assert_all_finite(\n                array,\n                input_name=input_name,\n                estimator_name=estimator_name,\n                allow_nan=ensure_all_finite == \"allow-nan\",\n            )\n\n        if copy:\n            if _is_numpy_namespace(xp):\n                # only make a copy if `array` and `array_orig` may share memory`\n                if np.may_share_memory(array, array_orig):\n                    array = _asarray_with_order(\n                        array, dtype=dtype, order=order, copy=True, xp=xp\n                    )\n            else:\n                # always make a copy for non-numpy arrays\n                array = _asarray_with_order(\n                    array, dtype=dtype, order=order, copy=True, xp=xp\n                )\n\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError(\n                \"Found array with %d sample(s) (shape=%s) while a\"\n                \" minimum of %d is required%s.\"\n                % (n_samples, array.shape, ensure_min_samples, context)\n            )\n\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError(\n                \"Found array with %d feature(s) (shape=%s) while\"\n                \" a minimum of %d is required%s.\"\n                % (n_features, array.shape, ensure_min_features, context)\n            )\n\n    if ensure_non_negative:\n        whom = input_name\n        if estimator_name:\n            whom += f\" in {estimator_name}\"\n        check_non_negative(array, whom)\n\n    if force_writeable:\n        # By default, array.copy() creates a C-ordered copy. We set order=K to\n        # preserve the order of the array.\n        copy_params = {\"order\": \"K\"} if not sp.issparse(array) else {}\n\n        array_data = array.data if sp.issparse(array) else array\n        flags = getattr(array_data, \"flags\", None)\n        if not getattr(flags, \"writeable\", True):\n            # This situation can only happen when copy=False, the array is read-only and\n            # a writeable output is requested. This is an ambiguous setting so we chose\n            # to always (except for one specific setting, see below) make a copy to\n            # ensure that the output is writeable, even if avoidable, to not overwrite\n            # the user's data by surprise.\n\n            if _is_pandas_df_or_series(array_orig):\n                try:\n                    # In pandas >= 3, np.asarray(df), called earlier in check_array,\n                    # returns a read-only intermediate array. It can be made writeable\n                    # safely without copy because if the original DataFrame was backed\n                    # by a read-only array, trying to change the flag would raise an\n                    # error, in which case we make a copy.\n                    array_data.flags.writeable = True\n                except ValueError:\n                    array = array.copy(**copy_params)\n            else:\n                array = array.copy(**copy_params)\n\n    return array"
}