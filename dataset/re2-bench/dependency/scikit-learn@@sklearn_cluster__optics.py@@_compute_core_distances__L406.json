{
    "scikit-learn.sklearn.neighbors._base.kneighbors": "def kneighbors(self, X=None, n_neighbors=None, return_distance=True):\n    \"\"\"Find the K-neighbors of a point.\n\n    Returns indices of and distances to the neighbors of each point.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}, shape (n_queries, n_features), \\\n        or (n_queries, n_indexed) if metric == 'precomputed', default=None\n        The query point or points.\n        If not provided, neighbors of each indexed point are returned.\n        In this case, the query point is not considered its own neighbor.\n\n    n_neighbors : int, default=None\n        Number of neighbors required for each sample. The default is the\n        value passed to the constructor.\n\n    return_distance : bool, default=True\n        Whether or not to return the distances.\n\n    Returns\n    -------\n    neigh_dist : ndarray of shape (n_queries, n_neighbors)\n        Array representing the lengths to points, only present if\n        return_distance=True.\n\n    neigh_ind : ndarray of shape (n_queries, n_neighbors)\n        Indices of the nearest points in the population matrix.\n\n    Examples\n    --------\n    In the following example, we construct a NearestNeighbors\n    class from an array representing our data set and ask who's\n    the closest point to [1,1,1]\n\n    >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n    >>> from sklearn.neighbors import NearestNeighbors\n    >>> neigh = NearestNeighbors(n_neighbors=1)\n    >>> neigh.fit(samples)\n    NearestNeighbors(n_neighbors=1)\n    >>> print(neigh.kneighbors([[1., 1., 1.]]))\n    (array([[0.5]]), array([[2]]))\n\n    As you can see, it returns [[0.5]], and [[2]], which means that the\n    element is at distance 0.5 and is the third element of samples\n    (indexes start at 0). You can also query for multiple points:\n\n    >>> X = [[0., 1., 0.], [1., 0., 1.]]\n    >>> neigh.kneighbors(X, return_distance=False)\n    array([[1],\n           [2]]...)\n    \"\"\"\n    check_is_fitted(self)\n\n    if n_neighbors is None:\n        n_neighbors = self.n_neighbors\n    elif n_neighbors <= 0:\n        raise ValueError(\"Expected n_neighbors > 0. Got %d\" % n_neighbors)\n    elif not isinstance(n_neighbors, numbers.Integral):\n        raise TypeError(\n            \"n_neighbors does not take %s value, enter integer value\"\n            % type(n_neighbors)\n        )\n\n    ensure_all_finite = \"allow-nan\" if get_tags(self).input_tags.allow_nan else True\n    query_is_train = X is None\n    if query_is_train:\n        X = self._fit_X\n        # Include an extra neighbor to account for the sample itself being\n        # returned, which is removed later\n        n_neighbors += 1\n    else:\n        if self.metric == \"precomputed\":\n            X = _check_precomputed(X)\n        else:\n            X = validate_data(\n                self,\n                X,\n                ensure_all_finite=ensure_all_finite,\n                accept_sparse=\"csr\",\n                reset=False,\n                order=\"C\",\n            )\n\n    n_samples_fit = self.n_samples_fit_\n    if n_neighbors > n_samples_fit:\n        if query_is_train:\n            n_neighbors -= 1  # ok to modify inplace because an error is raised\n            inequality_str = \"n_neighbors < n_samples_fit\"\n        else:\n            inequality_str = \"n_neighbors <= n_samples_fit\"\n        raise ValueError(\n            f\"Expected {inequality_str}, but \"\n            f\"n_neighbors = {n_neighbors}, n_samples_fit = {n_samples_fit}, \"\n            f\"n_samples = {X.shape[0]}\"  # include n_samples for common tests\n        )\n\n    n_jobs = effective_n_jobs(self.n_jobs)\n    chunked_results = None\n    use_pairwise_distances_reductions = (\n        self._fit_method == \"brute\"\n        and ArgKmin.is_usable_for(\n            X if X is not None else self._fit_X, self._fit_X, self.effective_metric_\n        )\n    )\n    if use_pairwise_distances_reductions:\n        results = ArgKmin.compute(\n            X=X,\n            Y=self._fit_X,\n            k=n_neighbors,\n            metric=self.effective_metric_,\n            metric_kwargs=self.effective_metric_params_,\n            strategy=\"auto\",\n            return_distance=return_distance,\n        )\n\n    elif (\n        self._fit_method == \"brute\" and self.metric == \"precomputed\" and issparse(X)\n    ):\n        results = _kneighbors_from_graph(\n            X, n_neighbors=n_neighbors, return_distance=return_distance\n        )\n\n    elif self._fit_method == \"brute\":\n        # Joblib-based backend, which is used when user-defined callable\n        # are passed for metric.\n\n        # This won't be used in the future once PairwiseDistancesReductions\n        # support:\n        #   - DistanceMetrics which work on supposedly binary data\n        #   - CSR-dense and dense-CSR case if 'euclidean' in metric.\n        reduce_func = partial(\n            self._kneighbors_reduce_func,\n            n_neighbors=n_neighbors,\n            return_distance=return_distance,\n        )\n\n        # for efficiency, use squared euclidean distances\n        if self.effective_metric_ == \"euclidean\":\n            kwds = {\"squared\": True}\n        else:\n            kwds = self.effective_metric_params_\n\n        chunked_results = list(\n            pairwise_distances_chunked(\n                X,\n                self._fit_X,\n                reduce_func=reduce_func,\n                metric=self.effective_metric_,\n                n_jobs=n_jobs,\n                **kwds,\n            )\n        )\n\n    elif self._fit_method in [\"ball_tree\", \"kd_tree\"]:\n        if issparse(X):\n            raise ValueError(\n                \"%s does not work with sparse matrices. Densify the data, \"\n                \"or set algorithm='brute'\" % self._fit_method\n            )\n        chunked_results = Parallel(n_jobs, prefer=\"threads\")(\n            delayed(self._tree.query)(X[s], n_neighbors, return_distance)\n            for s in gen_even_slices(X.shape[0], n_jobs)\n        )\n    else:\n        raise ValueError(\"internal: _fit_method not recognized\")\n\n    if chunked_results is not None:\n        if return_distance:\n            neigh_dist, neigh_ind = zip(*chunked_results)\n            results = np.vstack(neigh_dist), np.vstack(neigh_ind)\n        else:\n            results = np.vstack(chunked_results)\n\n    if not query_is_train:\n        return results\n    else:\n        # If the query data is the same as the indexed data, we would like\n        # to ignore the first nearest neighbor of every sample, i.e\n        # the sample itself.\n        if return_distance:\n            neigh_dist, neigh_ind = results\n        else:\n            neigh_ind = results\n\n        n_queries, _ = X.shape\n        sample_range = np.arange(n_queries)[:, None]\n        sample_mask = neigh_ind != sample_range\n\n        # Corner case: When the number of duplicates are more\n        # than the number of neighbors, the first NN will not\n        # be the sample, but a duplicate.\n        # In that case mask the first duplicate.\n        dup_gr_nbrs = np.all(sample_mask, axis=1)\n        sample_mask[:, 0][dup_gr_nbrs] = False\n        neigh_ind = np.reshape(neigh_ind[sample_mask], (n_queries, n_neighbors - 1))\n\n        if return_distance:\n            neigh_dist = np.reshape(\n                neigh_dist[sample_mask], (n_queries, n_neighbors - 1)\n            )\n            return neigh_dist, neigh_ind\n        return neigh_ind",
    "scikit-learn.sklearn.utils._chunking.get_chunk_n_rows": "def get_chunk_n_rows(row_bytes, *, max_n_rows=None, working_memory=None):\n    \"\"\"Calculate how many rows can be processed within `working_memory`.\n\n    Parameters\n    ----------\n    row_bytes : int\n        The expected number of bytes of memory that will be consumed\n        during the processing of each row.\n    max_n_rows : int, default=None\n        The maximum return value.\n    working_memory : int or float, default=None\n        The number of rows to fit inside this number of MiB will be\n        returned. When None (default), the value of\n        ``sklearn.get_config()['working_memory']`` is used.\n\n    Returns\n    -------\n    int\n        The number of rows which can be processed within `working_memory`.\n\n    Warns\n    -----\n    Issues a UserWarning if `row_bytes exceeds `working_memory` MiB.\n    \"\"\"\n\n    if working_memory is None:\n        working_memory = get_config()[\"working_memory\"]\n\n    chunk_n_rows = int(working_memory * (2**20) // row_bytes)\n    if max_n_rows is not None:\n        chunk_n_rows = min(chunk_n_rows, max_n_rows)\n    if chunk_n_rows < 1:\n        warnings.warn(\n            \"Could not adhere to working_memory config. \"\n            \"Currently %.0fMiB, %.0fMiB required.\"\n            % (working_memory, np.ceil(row_bytes * 2**-20))\n        )\n        chunk_n_rows = 1\n    return chunk_n_rows",
    "scikit-learn.sklearn.utils._chunking.gen_batches": "@validate_params(\n    {\n        \"n\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"min_batch_size\": [Interval(Integral, 0, None, closed=\"left\")],\n    },\n    prefer_skip_nested_validation=True,\n)\ndef gen_batches(n, batch_size, *, min_batch_size=0):\n    \"\"\"Generator to create slices containing `batch_size` elements from 0 to `n`.\n\n    The last slice may contain less than `batch_size` elements, when\n    `batch_size` does not divide `n`.\n\n    Parameters\n    ----------\n    n : int\n        Size of the sequence.\n    batch_size : int\n        Number of elements in each batch.\n    min_batch_size : int, default=0\n        Minimum number of elements in each batch.\n\n    Yields\n    ------\n    slice of `batch_size` elements\n\n    See Also\n    --------\n    gen_even_slices: Generator to create n_packs slices going up to n.\n\n    Examples\n    --------\n    >>> from sklearn.utils import gen_batches\n    >>> list(gen_batches(7, 3))\n    [slice(0, 3, None), slice(3, 6, None), slice(6, 7, None)]\n    >>> list(gen_batches(6, 3))\n    [slice(0, 3, None), slice(3, 6, None)]\n    >>> list(gen_batches(2, 3))\n    [slice(0, 2, None)]\n    >>> list(gen_batches(7, 3, min_batch_size=0))\n    [slice(0, 3, None), slice(3, 6, None), slice(6, 7, None)]\n    >>> list(gen_batches(7, 3, min_batch_size=2))\n    [slice(0, 3, None), slice(3, 7, None)]\n    \"\"\"\n    start = 0\n    for _ in range(int(n // batch_size)):\n        end = start + batch_size\n        if end + min_batch_size > n:\n            continue\n        yield slice(start, end)\n        start = end\n    if start < n:\n        yield slice(start, n)",
    "scikit-learn.sklearn.utils._param_validation.wrapper": "@functools.wraps(func)\ndef wrapper(*args, **kwargs):\n    global_skip_validation = get_config()[\"skip_parameter_validation\"]\n    if global_skip_validation:\n        return func(*args, **kwargs)\n\n    func_sig = signature(func)\n\n    # Map *args/**kwargs to the function signature\n    params = func_sig.bind(*args, **kwargs)\n    params.apply_defaults()\n\n    # ignore self/cls and positional/keyword markers\n    to_ignore = [\n        p.name\n        for p in func_sig.parameters.values()\n        if p.kind in (p.VAR_POSITIONAL, p.VAR_KEYWORD)\n    ]\n    to_ignore += [\"self\", \"cls\"]\n    params = {k: v for k, v in params.arguments.items() if k not in to_ignore}\n\n    validate_parameter_constraints(\n        parameter_constraints, params, caller_name=func.__qualname__\n    )\n\n    try:\n        with config_context(\n            skip_parameter_validation=(\n                prefer_skip_nested_validation or global_skip_validation\n            )\n        ):\n            return func(*args, **kwargs)\n    except InvalidParameterError as e:\n        # When the function is just a wrapper around an estimator, we allow\n        # the function to delegate validation to the estimator, but we replace\n        # the name of the estimator by the name of the function in the error\n        # message to avoid confusion.\n        msg = re.sub(\n            r\"parameter of \\w+ must be\",\n            f\"parameter of {func.__qualname__} must be\",\n            str(e),\n        )\n        raise InvalidParameterError(msg) from e"
}