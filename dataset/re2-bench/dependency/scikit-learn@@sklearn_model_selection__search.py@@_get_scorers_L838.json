{
    "scikit-learn.sklearn.metrics._scorer.__init__": "def __init__(self, *, scorers, raise_exc=True):\n    self._scorers = scorers\n    self._raise_exc = raise_exc",
    "scikit-learn.sklearn.metrics._scorer._check_multimetric_scoring": "def _check_multimetric_scoring(estimator, scoring):\n    \"\"\"Check the scoring parameter in cases when multiple metrics are allowed.\n\n    In addition, multimetric scoring leverages a caching mechanism to not call the same\n    estimator response method multiple times. Hence, the scorer is modified to only use\n    a single response method given a list of response methods and the estimator.\n\n    Parameters\n    ----------\n    estimator : sklearn estimator instance\n        The estimator for which the scoring will be applied.\n\n    scoring : list, tuple or dict\n        Strategy to evaluate the performance of the cross-validated model on\n        the test set.\n\n        The possibilities are:\n\n        - a list or tuple of unique strings;\n        - a callable returning a dictionary where they keys are the metric\n          names and the values are the metric scores;\n        - a dictionary with metric names as keys and callables a values.\n\n        See :ref:`multimetric_grid_search` for an example.\n\n    Returns\n    -------\n    scorers_dict : dict\n        A dict mapping each scorer name to its validated scorer.\n    \"\"\"\n    err_msg_generic = (\n        f\"scoring is invalid (got {scoring!r}). Refer to the \"\n        \"scoring glossary for details: \"\n        \"https://scikit-learn.org/stable/glossary.html#term-scoring\"\n    )\n\n    if isinstance(scoring, (list, tuple, set)):\n        err_msg = (\n            \"The list/tuple elements must be unique strings of predefined scorers. \"\n        )\n        try:\n            keys = set(scoring)\n        except TypeError as e:\n            raise ValueError(err_msg) from e\n\n        if len(keys) != len(scoring):\n            raise ValueError(\n                f\"{err_msg} Duplicate elements were found in\"\n                f\" the given list. {scoring!r}\"\n            )\n        elif len(keys) > 0:\n            if not all(isinstance(k, str) for k in keys):\n                if any(callable(k) for k in keys):\n                    raise ValueError(\n                        f\"{err_msg} One or more of the elements \"\n                        \"were callables. Use a dict of score \"\n                        \"name mapped to the scorer callable. \"\n                        f\"Got {scoring!r}\"\n                    )\n                else:\n                    raise ValueError(\n                        f\"{err_msg} Non-string types were found \"\n                        f\"in the given list. Got {scoring!r}\"\n                    )\n            scorers = {\n                scorer: check_scoring(estimator, scoring=scorer) for scorer in scoring\n            }\n        else:\n            raise ValueError(f\"{err_msg} Empty list was given. {scoring!r}\")\n\n    elif isinstance(scoring, dict):\n        keys = set(scoring)\n        if not all(isinstance(k, str) for k in keys):\n            raise ValueError(\n                \"Non-string types were found in the keys of \"\n                f\"the given dict. scoring={scoring!r}\"\n            )\n        if len(keys) == 0:\n            raise ValueError(f\"An empty dict was passed. {scoring!r}\")\n        scorers = {\n            key: check_scoring(estimator, scoring=scorer)\n            for key, scorer in scoring.items()\n        }\n    else:\n        raise ValueError(err_msg_generic)\n\n    return scorers",
    "scikit-learn.sklearn.model_selection._search._check_refit_for_multimetric": "def _check_refit_for_multimetric(self, scores):\n    \"\"\"Check `refit` is compatible with `scores` is valid\"\"\"\n    multimetric_refit_msg = (\n        \"For multi-metric scoring, the parameter refit must be set to a \"\n        \"scorer key or a callable to refit an estimator with the best \"\n        \"parameter setting on the whole data and make the best_* \"\n        \"attributes available for that metric. If this is not needed, \"\n        f\"refit should be set to False explicitly. {self.refit!r} was \"\n        \"passed.\"\n    )\n\n    valid_refit_dict = isinstance(self.refit, str) and self.refit in scores\n\n    if (\n        self.refit is not False\n        and not valid_refit_dict\n        and not callable(self.refit)\n    ):\n        raise ValueError(multimetric_refit_msg)",
    "scikit-learn.sklearn.utils._param_validation.wrapper": "@functools.wraps(func)\ndef wrapper(*args, **kwargs):\n    global_skip_validation = get_config()[\"skip_parameter_validation\"]\n    if global_skip_validation:\n        return func(*args, **kwargs)\n\n    func_sig = signature(func)\n\n    # Map *args/**kwargs to the function signature\n    params = func_sig.bind(*args, **kwargs)\n    params.apply_defaults()\n\n    # ignore self/cls and positional/keyword markers\n    to_ignore = [\n        p.name\n        for p in func_sig.parameters.values()\n        if p.kind in (p.VAR_POSITIONAL, p.VAR_KEYWORD)\n    ]\n    to_ignore += [\"self\", \"cls\"]\n    params = {k: v for k, v in params.arguments.items() if k not in to_ignore}\n\n    validate_parameter_constraints(\n        parameter_constraints, params, caller_name=func.__qualname__\n    )\n\n    try:\n        with config_context(\n            skip_parameter_validation=(\n                prefer_skip_nested_validation or global_skip_validation\n            )\n        ):\n            return func(*args, **kwargs)\n    except InvalidParameterError as e:\n        # When the function is just a wrapper around an estimator, we allow\n        # the function to delegate validation to the estimator, but we replace\n        # the name of the estimator by the name of the function in the error\n        # message to avoid confusion.\n        msg = re.sub(\n            r\"parameter of \\w+ must be\",\n            f\"parameter of {func.__qualname__} must be\",\n            str(e),\n        )\n        raise InvalidParameterError(msg) from e"
}