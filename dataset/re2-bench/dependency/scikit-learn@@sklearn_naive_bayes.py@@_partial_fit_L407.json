{
    "scikit-learn.sklearn.externals.array_api_compat._internal.wrapped_f": "@wraps(f)\ndef wrapped_f(*args: object, **kwargs: object) -> object:\n    return f(*args, xp=xp, **kwargs)",
    "scikit-learn.sklearn.externals.array_api_compat.numpy._aliases.asarray": "def asarray(\n    obj: Array | complex | NestedSequence[complex] | SupportsBufferProtocol,\n    /,\n    *,\n    dtype: DType | None = None,\n    device: Device | None = None,\n    copy: _Copy | None = None,\n    **kwargs: Any,\n) -> Array:\n    \"\"\"\n    Array API compatibility wrapper for asarray().\n\n    See the corresponding documentation in the array library and/or the array API\n    specification for more details.\n    \"\"\"\n    _helpers._check_device(np, device)\n\n    if copy is None:\n        copy = np._CopyMode.IF_NEEDED\n    elif copy is False:\n        copy = np._CopyMode.NEVER\n    elif copy is True:\n        copy = np._CopyMode.ALWAYS\n\n    return np.array(obj, copy=copy, dtype=dtype, **kwargs)  # pyright: ignore",
    "scikit-learn.sklearn.externals.array_api_extra._delegation.isclose": "def isclose(\n    a: Array | complex,\n    b: Array | complex,\n    *,\n    rtol: float = 1e-05,\n    atol: float = 1e-08,\n    equal_nan: bool = False,\n    xp: ModuleType | None = None,\n) -> Array:\n    \"\"\"\n    Return a boolean array where two arrays are element-wise equal within a tolerance.\n\n    The tolerance values are positive, typically very small numbers. The relative\n    difference ``(rtol * abs(b))`` and the absolute difference `atol` are added together\n    to compare against the absolute difference between `a` and `b`.\n\n    NaNs are treated as equal if they are in the same place and if ``equal_nan=True``.\n    Infs are treated as equal if they are in the same place and of the same sign in both\n    arrays.\n\n    Parameters\n    ----------\n    a, b : Array | int | float | complex | bool\n        Input objects to compare. At least one must be an array.\n    rtol : array_like, optional\n        The relative tolerance parameter (see Notes).\n    atol : array_like, optional\n        The absolute tolerance parameter (see Notes).\n    equal_nan : bool, optional\n        Whether to compare NaN's as equal. If True, NaN's in `a` will be considered\n        equal to NaN's in `b` in the output array.\n    xp : array_namespace, optional\n        The standard-compatible namespace for `a` and `b`. Default: infer.\n\n    Returns\n    -------\n    Array\n        A boolean array of shape broadcasted from `a` and `b`, containing ``True`` where\n        `a` is close to `b`, and ``False`` otherwise.\n\n    Warnings\n    --------\n    The default `atol` is not appropriate for comparing numbers with magnitudes much\n    smaller than one (see notes).\n\n    See Also\n    --------\n    math.isclose : Similar function in stdlib for Python scalars.\n\n    Notes\n    -----\n    For finite values, `isclose` uses the following equation to test whether two\n    floating point values are equivalent::\n\n        absolute(a - b) <= (atol + rtol * absolute(b))\n\n    Unlike the built-in `math.isclose`,\n    the above equation is not symmetric in `a` and `b`,\n    so that ``isclose(a, b)`` might be different from ``isclose(b, a)`` in some rare\n    cases.\n\n    The default value of `atol` is not appropriate when the reference value `b` has\n    magnitude smaller than one. For example, it is unlikely that ``a = 1e-9`` and\n    ``b = 2e-9`` should be considered \"close\", yet ``isclose(1e-9, 2e-9)`` is ``True``\n    with default settings. Be sure to select `atol` for the use case at hand, especially\n    for defining the threshold below which a non-zero value in `a` will be considered\n    \"close\" to a very small or zero value in `b`.\n\n    The comparison of `a` and `b` uses standard broadcasting, which means that `a` and\n    `b` need not have the same shape in order for ``isclose(a, b)`` to evaluate to\n    ``True``.\n\n    `isclose` is not defined for non-numeric data types.\n    ``bool`` is considered a numeric data-type for this purpose.\n    \"\"\"\n    xp = array_namespace(a, b) if xp is None else xp\n\n    if (\n        is_numpy_namespace(xp)\n        or is_cupy_namespace(xp)\n        or is_dask_namespace(xp)\n        or is_jax_namespace(xp)\n    ):\n        return xp.isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan)\n\n    if is_torch_namespace(xp):\n        a, b = asarrays(a, b, xp=xp)  # Array API 2024.12 support\n        return xp.isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan)\n\n    return _funcs.isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan, xp=xp)",
    "scikit-learn.sklearn.naive_bayes._update_mean_variance": "@staticmethod\ndef _update_mean_variance(n_past, mu, var, X, sample_weight=None):\n    \"\"\"Compute online update of Gaussian mean and variance.\n\n    Given starting sample count, mean, and variance, a new set of\n    points X, and optionally sample weights, return the updated mean and\n    variance. (NB - each dimension (column) in X is treated as independent\n    -- you get variance, not covariance).\n\n    Can take scalar mean and variance, or vector mean and variance to\n    simultaneously update a number of independent Gaussians.\n\n    See Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n\n    http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n\n    Parameters\n    ----------\n    n_past : int\n        Number of samples represented in old mean and variance. If sample\n        weights were given, this should contain the sum of sample\n        weights represented in old mean and variance.\n\n    mu : array-like of shape (number of Gaussians,)\n        Means for Gaussians in original set.\n\n    var : array-like of shape (number of Gaussians,)\n        Variances for Gaussians in original set.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Weights applied to individual samples (1. for unweighted).\n\n    Returns\n    -------\n    total_mu : array-like of shape (number of Gaussians,)\n        Updated mean for each Gaussian over the combined set.\n\n    total_var : array-like of shape (number of Gaussians,)\n        Updated variance for each Gaussian over the combined set.\n    \"\"\"\n    xp, _ = get_namespace(X)\n    if X.shape[0] == 0:\n        return mu, var\n\n    # Compute (potentially weighted) mean and variance of new datapoints\n    if sample_weight is not None:\n        n_new = float(xp.sum(sample_weight))\n        if np.isclose(n_new, 0.0):\n            return mu, var\n        new_mu = _average(X, axis=0, weights=sample_weight, xp=xp)\n        new_var = _average((X - new_mu) ** 2, axis=0, weights=sample_weight, xp=xp)\n    else:\n        n_new = X.shape[0]\n        new_var = xp.var(X, axis=0)\n        new_mu = xp.mean(X, axis=0)\n\n    if n_past == 0:\n        return new_mu, new_var\n\n    n_total = float(n_past + n_new)\n\n    # Combine mean of old and new data, taking into consideration\n    # (weighted) number of observations\n    total_mu = (n_new * new_mu + n_past * mu) / n_total\n\n    # Combine variance of old and new data, taking into consideration\n    # (weighted) number of observations. This is achieved by combining\n    # the sum-of-squared-differences (ssd)\n    old_ssd = n_past * var\n    new_ssd = n_new * new_var\n    total_ssd = old_ssd + new_ssd + (n_new * n_past / n_total) * (mu - new_mu) ** 2\n    total_var = total_ssd / n_total\n\n    return total_mu, total_var",
    "scikit-learn.sklearn.utils._array_api.get_namespace": "def get_namespace(*arrays, remove_none=True, remove_types=(str,), xp=None):\n    \"\"\"Get namespace of arrays.\n\n    Introspect `arrays` arguments and return their common Array API compatible\n    namespace object, if any.\n\n    Note that sparse arrays are filtered by default.\n\n    See: https://numpy.org/neps/nep-0047-array-api-standard.html\n\n    If `arrays` are regular numpy arrays, `array_api_compat.numpy` is returned instead.\n\n    Namespace support is not enabled by default. To enabled it call:\n\n      sklearn.set_config(array_api_dispatch=True)\n\n    or:\n\n      with sklearn.config_context(array_api_dispatch=True):\n          # your code here\n\n    Otherwise `array_api_compat.numpy` is\n    always returned irrespective of the fact that arrays implement the\n    `__array_namespace__` protocol or not.\n\n    Note that if no arrays pass the set filters, ``_NUMPY_API_WRAPPER_INSTANCE, False``\n    is returned.\n\n    Parameters\n    ----------\n    *arrays : array objects\n        Array objects.\n\n    remove_none : bool, default=True\n        Whether to ignore None objects passed in arrays.\n\n    remove_types : tuple or list, default=(str,)\n        Types to ignore in the arrays.\n\n    xp : module, default=None\n        Precomputed array namespace module. When passed, typically from a caller\n        that has already performed inspection of its own inputs, skips array\n        namespace inspection.\n\n    Returns\n    -------\n    namespace : module\n        Namespace shared by array objects. If any of the `arrays` are not arrays,\n        the namespace defaults to the NumPy namespace.\n\n    is_array_api_compliant : bool\n        True if the arrays are containers that implement the array API spec (see\n        https://data-apis.org/array-api/latest/index.html).\n        Always False when array_api_dispatch=False.\n    \"\"\"\n    array_api_dispatch = get_config()[\"array_api_dispatch\"]\n    if not array_api_dispatch:\n        if xp is not None:\n            return xp, False\n        else:\n            return np_compat, False\n\n    if xp is not None:\n        return xp, True\n\n    arrays = _remove_non_arrays(\n        *arrays,\n        remove_none=remove_none,\n        remove_types=remove_types,\n    )\n\n    if not arrays:\n        return np_compat, False\n\n    _check_array_api_dispatch(array_api_dispatch)\n\n    namespace, is_array_api_compliant = array_api_compat.get_namespace(*arrays), True\n\n    if namespace.__name__ == \"array_api_strict\" and hasattr(\n        namespace, \"set_array_api_strict_flags\"\n    ):\n        namespace.set_array_api_strict_flags(api_version=\"2024.12\")\n\n    return namespace, is_array_api_compliant",
    "scikit-learn.sklearn.utils._array_api.get_namespace_and_device": "def get_namespace_and_device(\n    *array_list, remove_none=True, remove_types=(str,), xp=None\n):\n    \"\"\"Combination into one single function of `get_namespace` and `device`.\n\n    Parameters\n    ----------\n    *array_list : array objects\n        Array objects.\n    remove_none : bool, default=True\n        Whether to ignore None objects passed in arrays.\n    remove_types : tuple or list, default=(str,)\n        Types to ignore in the arrays.\n    xp : module, default=None\n        Precomputed array namespace module. When passed, typically from a caller\n        that has already performed inspection of its own inputs, skips array\n        namespace inspection.\n\n    Returns\n    -------\n    namespace : module\n        Namespace shared by array objects. If any of the `arrays` are not arrays,\n        the namespace defaults to NumPy.\n    is_array_api_compliant : bool\n        True if the arrays are containers that implement the Array API spec.\n        Always False when array_api_dispatch=False.\n    device : device\n        `device` object (see the \"Device Support\" section of the array API spec).\n    \"\"\"\n    skip_remove_kwargs = dict(remove_none=False, remove_types=[])\n\n    array_list = _remove_non_arrays(\n        *array_list,\n        remove_none=remove_none,\n        remove_types=remove_types,\n    )\n    arrays_device = device(*array_list, **skip_remove_kwargs)\n\n    if xp is None:\n        xp, is_array_api = get_namespace(*array_list, **skip_remove_kwargs)\n    else:\n        xp, is_array_api = xp, True\n\n    if is_array_api:\n        return xp, is_array_api, arrays_device\n    else:\n        return xp, False, arrays_device",
    "scikit-learn.sklearn.utils._array_api._find_matching_floating_dtype": "def _find_matching_floating_dtype(*arrays, xp):\n    \"\"\"Find a suitable floating point dtype when computing with arrays.\n\n    If any of the arrays are floating point, return the dtype with the highest\n    precision by following official type promotion rules:\n\n    https://data-apis.org/array-api/latest/API_specification/type_promotion.html\n\n    If there are no floating point input arrays (all integral inputs for\n    instance), return the default floating point dtype for the namespace.\n    \"\"\"\n    dtyped_arrays = [xp.asarray(a) for a in arrays if hasattr(a, \"dtype\")]\n    floating_dtypes = [\n        a.dtype for a in dtyped_arrays if xp.isdtype(a.dtype, \"real floating\")\n    ]\n    if floating_dtypes:\n        # Return the floating dtype with the highest precision:\n        return xp.result_type(*floating_dtypes)\n\n    # If none of the input arrays have a floating point dtype, they must be all\n    # integer arrays or containers of Python scalars: return the default\n    # floating point dtype for the namespace (implementation specific).\n    return xp.asarray(0.0).dtype",
    "scikit-learn.sklearn.utils._array_api._isin": "def _isin(element, test_elements, xp, assume_unique=False, invert=False):\n    \"\"\"Calculates ``element in test_elements``, broadcasting over `element`\n    only.\n\n    Returns a boolean array of the same shape as `element` that is True\n    where an element of `element` is in `test_elements` and False otherwise.\n    \"\"\"\n    if _is_numpy_namespace(xp):\n        return xp.asarray(\n            numpy.isin(\n                element=element,\n                test_elements=test_elements,\n                assume_unique=assume_unique,\n                invert=invert,\n            )\n        )\n\n    original_element_shape = element.shape\n    element = xp.reshape(element, (-1,))\n    test_elements = xp.reshape(test_elements, (-1,))\n    return xp.reshape(\n        _in1d(\n            ar1=element,\n            ar2=test_elements,\n            xp=xp,\n            assume_unique=assume_unique,\n            invert=invert,\n        ),\n        original_element_shape,\n    )",
    "scikit-learn.sklearn.utils.multiclass._check_partial_fit_first_call": "def _check_partial_fit_first_call(clf, classes=None):\n    \"\"\"Private helper function for factorizing common classes param logic.\n\n    Estimators that implement the ``partial_fit`` API need to be provided with\n    the list of possible classes at the first call to partial_fit.\n\n    Subsequent calls to partial_fit should check that ``classes`` is still\n    consistent with a previous value of ``clf.classes_`` when provided.\n\n    This function returns True if it detects that this was the first call to\n    ``partial_fit`` on ``clf``. In that case the ``classes_`` attribute is also\n    set on ``clf``.\n\n    \"\"\"\n    if getattr(clf, \"classes_\", None) is None and classes is None:\n        raise ValueError(\"classes must be passed on the first call to partial_fit.\")\n\n    elif classes is not None:\n        if getattr(clf, \"classes_\", None) is not None:\n            if not np.array_equal(clf.classes_, unique_labels(classes)):\n                raise ValueError(\n                    \"`classes=%r` is not the same as on last call \"\n                    \"to partial_fit, was: %r\" % (classes, clf.classes_)\n                )\n\n        else:\n            # This is the first call to partial_fit\n            clf.classes_ = unique_labels(classes)\n            return True\n\n    # classes is None and clf.classes_ has already previously been set:\n    # nothing to do\n    return False",
    "scikit-learn.sklearn.utils.validation._check_sample_weight": "def _check_sample_weight(\n    sample_weight,\n    X,\n    *,\n    dtype=None,\n    force_float_dtype=True,\n    ensure_non_negative=False,\n    ensure_same_device=True,\n    copy=False,\n):\n    \"\"\"Validate sample weights.\n\n    Note that passing sample_weight=None will output an array of ones.\n    Therefore, in some cases, you may want to protect the call with:\n    if sample_weight is not None:\n        sample_weight = _check_sample_weight(...)\n\n    Parameters\n    ----------\n    sample_weight : {ndarray, Number or None}, shape (n_samples,)\n        Input sample weights.\n\n    X : {ndarray, list, sparse matrix}\n        Input data.\n\n    dtype : dtype, default=None\n        dtype of the validated `sample_weight`.\n        If None, and `sample_weight` is an array:\n\n            - If `sample_weight.dtype` is one of `{np.float64, np.float32}`,\n              then the dtype is preserved.\n            - Else the output has NumPy's default dtype: `np.float64`.\n\n        If `dtype` is not `{np.float32, np.float64, None}`, then output will\n        be `np.float64`.\n\n    force_float_dtype : bool, default=True\n        Whether `X` should be forced to be float dtype, when `dtype` is a non-float\n        dtype or None.\n\n    ensure_non_negative : bool, default=False,\n        Whether or not the weights are expected to be non-negative.\n\n        .. versionadded:: 1.0\n\n    ensure_same_device : bool, default=True\n        Whether `sample_weight` should be forced to be on the same device as `X`.\n\n    copy : bool, default=False\n        If True, a copy of sample_weight will be created.\n\n    Returns\n    -------\n    sample_weight : ndarray of shape (n_samples,)\n        Validated sample weight. It is guaranteed to be \"C\" contiguous.\n    \"\"\"\n    xp, is_array_api, device = get_namespace_and_device(X, remove_types=(int, float))\n\n    n_samples = _num_samples(X)\n\n    max_float_type = _max_precision_float_dtype(xp, device)\n    float_dtypes = (\n        [xp.float32] if max_float_type == xp.float32 else [xp.float64, xp.float32]\n    )\n    if force_float_dtype and dtype is not None and dtype not in float_dtypes:\n        dtype = max_float_type\n\n    if sample_weight is None:\n        sample_weight = xp.ones(n_samples, dtype=dtype, device=device)\n    elif isinstance(sample_weight, numbers.Number):\n        sample_weight = xp.full(n_samples, sample_weight, dtype=dtype, device=device)\n    else:\n        if force_float_dtype and dtype is None:\n            dtype = float_dtypes\n        if is_array_api and ensure_same_device:\n            sample_weight = xp.asarray(sample_weight, device=device)\n        sample_weight = check_array(\n            sample_weight,\n            accept_sparse=False,\n            ensure_2d=False,\n            dtype=dtype,\n            order=\"C\",\n            copy=copy,\n            input_name=\"sample_weight\",\n        )\n        if sample_weight.ndim != 1:\n            raise ValueError(\n                f\"Sample weights must be 1D array or scalar, got \"\n                f\"{sample_weight.ndim}D array. Expected either a scalar value \"\n                f\"or a 1D array of length {n_samples}.\"\n            )\n\n        if sample_weight.shape != (n_samples,):\n            raise ValueError(\n                \"sample_weight.shape == {}, expected {}!\".format(\n                    sample_weight.shape, (n_samples,)\n                )\n            )\n\n    if ensure_non_negative:\n        check_non_negative(sample_weight, \"`sample_weight`\")\n\n    return sample_weight",
    "scikit-learn.sklearn.utils.validation.validate_data": "def validate_data(\n    _estimator,\n    /,\n    X=\"no_validation\",\n    y=\"no_validation\",\n    reset=True,\n    validate_separately=False,\n    skip_check_array=False,\n    **check_params,\n):\n    \"\"\"Validate input data and set or check feature names and counts of the input.\n\n    This helper function should be used in an estimator that requires input\n    validation. This mutates the estimator and sets the `n_features_in_` and\n    `feature_names_in_` attributes if `reset=True`.\n\n    .. versionadded:: 1.6\n\n    Parameters\n    ----------\n    _estimator : estimator instance\n        The estimator to validate the input for.\n\n    X : {array-like, sparse matrix, dataframe} of shape \\\n            (n_samples, n_features), default='no validation'\n        The input samples.\n        If `'no_validation'`, no validation is performed on `X`. This is\n        useful for meta-estimator which can delegate input validation to\n        their underlying estimator(s). In that case `y` must be passed and\n        the only accepted `check_params` are `multi_output` and\n        `y_numeric`.\n\n    y : array-like of shape (n_samples,), default='no_validation'\n        The targets.\n\n        - If `None`, :func:`~sklearn.utils.check_array` is called on `X`. If\n          the estimator's `requires_y` tag is True, then an error will be raised.\n        - If `'no_validation'`, :func:`~sklearn.utils.check_array` is called\n          on `X` and the estimator's `requires_y` tag is ignored. This is a default\n          placeholder and is never meant to be explicitly set. In that case `X` must be\n          passed.\n        - Otherwise, only `y` with `_check_y` or both `X` and `y` are checked with\n          either :func:`~sklearn.utils.check_array` or\n          :func:`~sklearn.utils.check_X_y` depending on `validate_separately`.\n\n    reset : bool, default=True\n        Whether to reset the `n_features_in_` attribute.\n        If False, the input will be checked for consistency with data\n        provided when reset was last True.\n\n        .. note::\n\n           It is recommended to call `reset=True` in `fit` and in the first\n           call to `partial_fit`. All other methods that validate `X`\n           should set `reset=False`.\n\n    validate_separately : False or tuple of dicts, default=False\n        Only used if `y` is not `None`.\n        If `False`, call :func:`~sklearn.utils.check_X_y`. Else, it must be a tuple of\n        kwargs to be used for calling :func:`~sklearn.utils.check_array` on `X` and `y`\n        respectively.\n\n        `estimator=self` is automatically added to these dicts to generate\n        more informative error message in case of invalid input data.\n\n    skip_check_array : bool, default=False\n        If `True`, `X` and `y` are unchanged and only `feature_names_in_` and\n        `n_features_in_` are checked. Otherwise, :func:`~sklearn.utils.check_array`\n        is called on `X` and `y`.\n\n    **check_params : kwargs\n        Parameters passed to :func:`~sklearn.utils.check_array` or\n        :func:`~sklearn.utils.check_X_y`. Ignored if validate_separately\n        is not False.\n\n        `estimator=self` is automatically added to these params to generate\n        more informative error message in case of invalid input data.\n\n    Returns\n    -------\n    out : {ndarray, sparse matrix} or tuple of these\n        The validated input. A tuple is returned if both `X` and `y` are\n        validated.\n    \"\"\"\n    _check_feature_names(_estimator, X, reset=reset)\n    tags = get_tags(_estimator)\n    if y is None and tags.target_tags.required:\n        raise ValueError(\n            f\"This {_estimator.__class__.__name__} estimator \"\n            \"requires y to be passed, but the target y is None.\"\n        )\n\n    no_val_X = isinstance(X, str) and X == \"no_validation\"\n    no_val_y = y is None or (isinstance(y, str) and y == \"no_validation\")\n\n    if no_val_X and no_val_y:\n        raise ValueError(\"Validation should be done on X, y or both.\")\n\n    default_check_params = {\"estimator\": _estimator}\n    check_params = {**default_check_params, **check_params}\n\n    if skip_check_array:\n        if not no_val_X and no_val_y:\n            out = X\n        elif no_val_X and not no_val_y:\n            out = y\n        else:\n            out = X, y\n    elif not no_val_X and no_val_y:\n        out = check_array(X, input_name=\"X\", **check_params)\n    elif no_val_X and not no_val_y:\n        out = _check_y(y, **check_params)\n    else:\n        if validate_separately:\n            # We need this because some estimators validate X and y\n            # separately, and in general, separately calling check_array()\n            # on X and y isn't equivalent to just calling check_X_y()\n            # :(\n            check_X_params, check_y_params = validate_separately\n            if \"estimator\" not in check_X_params:\n                check_X_params = {**default_check_params, **check_X_params}\n            X = check_array(X, input_name=\"X\", **check_X_params)\n            if \"estimator\" not in check_y_params:\n                check_y_params = {**default_check_params, **check_y_params}\n            y = check_array(y, input_name=\"y\", **check_y_params)\n        else:\n            X, y = check_X_y(X, y, **check_params)\n        out = X, y\n\n    if not no_val_X and check_params.get(\"ensure_2d\", True):\n        _check_n_features(_estimator, X, reset=reset)\n\n    return out"
}