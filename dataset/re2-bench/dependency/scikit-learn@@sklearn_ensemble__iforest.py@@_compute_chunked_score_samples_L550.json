{
    "scikit-learn.sklearn.ensemble._iforest._compute_score_samples": "def _compute_score_samples(self, X, subsample_features):\n    \"\"\"\n    Compute the score of each samples in X going through the extra trees.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix\n        Data matrix.\n\n    subsample_features : bool\n        Whether features should be subsampled.\n\n    Returns\n    -------\n    scores : ndarray of shape (n_samples,)\n        The score of each sample in X.\n    \"\"\"\n    n_samples = X.shape[0]\n\n    depths = np.zeros(n_samples, order=\"f\")\n\n    average_path_length_max_samples = _average_path_length([self._max_samples])\n\n    # Note: we use default n_jobs value, i.e. sequential computation, which\n    # we expect to be more performant that parallelizing for small number\n    # of samples, e.g. < 1k samples. Default n_jobs value can be overridden\n    # by using joblib.parallel_backend context manager around\n    # ._compute_score_samples. Using a higher n_jobs may speed up the\n    # computation of the scores, e.g. for > 1k samples. See\n    # https://github.com/scikit-learn/scikit-learn/pull/28622 for more\n    # details.\n    lock = threading.Lock()\n    Parallel(\n        verbose=self.verbose,\n        require=\"sharedmem\",\n    )(\n        delayed(_parallel_compute_tree_depths)(\n            tree,\n            X,\n            features if subsample_features else None,\n            self._decision_path_lengths[tree_idx],\n            self._average_path_length_per_tree[tree_idx],\n            depths,\n            lock,\n        )\n        for tree_idx, (tree, features) in enumerate(\n            zip(self.estimators_, self.estimators_features_)\n        )\n    )\n\n    denominator = len(self.estimators_) * average_path_length_max_samples\n    scores = 2 ** (\n        # For a single training sample, denominator and depth are 0.\n        # Therefore, we set the score manually to 1.\n        -np.divide(\n            depths, denominator, out=np.ones_like(depths), where=denominator != 0\n        )\n    )\n    return scores",
    "scikit-learn.sklearn.utils._chunking.get_chunk_n_rows": "def get_chunk_n_rows(row_bytes, *, max_n_rows=None, working_memory=None):\n    \"\"\"Calculate how many rows can be processed within `working_memory`.\n\n    Parameters\n    ----------\n    row_bytes : int\n        The expected number of bytes of memory that will be consumed\n        during the processing of each row.\n    max_n_rows : int, default=None\n        The maximum return value.\n    working_memory : int or float, default=None\n        The number of rows to fit inside this number of MiB will be\n        returned. When None (default), the value of\n        ``sklearn.get_config()['working_memory']`` is used.\n\n    Returns\n    -------\n    int\n        The number of rows which can be processed within `working_memory`.\n\n    Warns\n    -----\n    Issues a UserWarning if `row_bytes exceeds `working_memory` MiB.\n    \"\"\"\n\n    if working_memory is None:\n        working_memory = get_config()[\"working_memory\"]\n\n    chunk_n_rows = int(working_memory * (2**20) // row_bytes)\n    if max_n_rows is not None:\n        chunk_n_rows = min(chunk_n_rows, max_n_rows)\n    if chunk_n_rows < 1:\n        warnings.warn(\n            \"Could not adhere to working_memory config. \"\n            \"Currently %.0fMiB, %.0fMiB required.\"\n            % (working_memory, np.ceil(row_bytes * 2**-20))\n        )\n        chunk_n_rows = 1\n    return chunk_n_rows",
    "scikit-learn.sklearn.utils._chunking.gen_batches": "@validate_params(\n    {\n        \"n\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"min_batch_size\": [Interval(Integral, 0, None, closed=\"left\")],\n    },\n    prefer_skip_nested_validation=True,\n)\ndef gen_batches(n, batch_size, *, min_batch_size=0):\n    \"\"\"Generator to create slices containing `batch_size` elements from 0 to `n`.\n\n    The last slice may contain less than `batch_size` elements, when\n    `batch_size` does not divide `n`.\n\n    Parameters\n    ----------\n    n : int\n        Size of the sequence.\n    batch_size : int\n        Number of elements in each batch.\n    min_batch_size : int, default=0\n        Minimum number of elements in each batch.\n\n    Yields\n    ------\n    slice of `batch_size` elements\n\n    See Also\n    --------\n    gen_even_slices: Generator to create n_packs slices going up to n.\n\n    Examples\n    --------\n    >>> from sklearn.utils import gen_batches\n    >>> list(gen_batches(7, 3))\n    [slice(0, 3, None), slice(3, 6, None), slice(6, 7, None)]\n    >>> list(gen_batches(6, 3))\n    [slice(0, 3, None), slice(3, 6, None)]\n    >>> list(gen_batches(2, 3))\n    [slice(0, 2, None)]\n    >>> list(gen_batches(7, 3, min_batch_size=0))\n    [slice(0, 3, None), slice(3, 6, None), slice(6, 7, None)]\n    >>> list(gen_batches(7, 3, min_batch_size=2))\n    [slice(0, 3, None), slice(3, 7, None)]\n    \"\"\"\n    start = 0\n    for _ in range(int(n // batch_size)):\n        end = start + batch_size\n        if end + min_batch_size > n:\n            continue\n        yield slice(start, end)\n        start = end\n    if start < n:\n        yield slice(start, n)",
    "scikit-learn.sklearn.utils._param_validation.wrapper": "@functools.wraps(func)\ndef wrapper(*args, **kwargs):\n    global_skip_validation = get_config()[\"skip_parameter_validation\"]\n    if global_skip_validation:\n        return func(*args, **kwargs)\n\n    func_sig = signature(func)\n\n    # Map *args/**kwargs to the function signature\n    params = func_sig.bind(*args, **kwargs)\n    params.apply_defaults()\n\n    # ignore self/cls and positional/keyword markers\n    to_ignore = [\n        p.name\n        for p in func_sig.parameters.values()\n        if p.kind in (p.VAR_POSITIONAL, p.VAR_KEYWORD)\n    ]\n    to_ignore += [\"self\", \"cls\"]\n    params = {k: v for k, v in params.arguments.items() if k not in to_ignore}\n\n    validate_parameter_constraints(\n        parameter_constraints, params, caller_name=func.__qualname__\n    )\n\n    try:\n        with config_context(\n            skip_parameter_validation=(\n                prefer_skip_nested_validation or global_skip_validation\n            )\n        ):\n            return func(*args, **kwargs)\n    except InvalidParameterError as e:\n        # When the function is just a wrapper around an estimator, we allow\n        # the function to delegate validation to the estimator, but we replace\n        # the name of the estimator by the name of the function in the error\n        # message to avoid confusion.\n        msg = re.sub(\n            r\"parameter of \\w+ must be\",\n            f\"parameter of {func.__qualname__} must be\",\n            str(e),\n        )\n        raise InvalidParameterError(msg) from e",
    "scikit-learn.sklearn.utils.validation._num_samples": "def _num_samples(x):\n    \"\"\"Return number of samples in array-like x.\"\"\"\n    message = \"Expected sequence or array-like, got %s\" % type(x)\n    if hasattr(x, \"fit\") and callable(x.fit):\n        # Don't get num_samples from an ensembles length!\n        raise TypeError(message)\n\n    if _use_interchange_protocol(x):\n        return x.__dataframe__().num_rows()\n\n    if not hasattr(x, \"__len__\") and not hasattr(x, \"shape\"):\n        if hasattr(x, \"__array__\"):\n            xp, _ = get_namespace(x)\n            x = xp.asarray(x)\n        else:\n            raise TypeError(message)\n\n    if hasattr(x, \"shape\") and x.shape is not None:\n        if len(x.shape) == 0:\n            raise TypeError(\n                \"Input should have at least 1 dimension i.e. satisfy \"\n                f\"`len(x.shape) > 0`, got scalar `{x!r}` instead.\"\n            )\n        # Check that shape is returning an integer or default to len\n        # Dask dataframes may not return numeric shape[0] value\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n\n    try:\n        return len(x)\n    except TypeError as type_error:\n        raise TypeError(message) from type_error"
}