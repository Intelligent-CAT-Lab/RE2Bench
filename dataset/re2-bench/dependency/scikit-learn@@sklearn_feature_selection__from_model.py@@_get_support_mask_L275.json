{
    "scikit-learn.sklearn.feature_selection._base._get_feature_importances": "def _get_feature_importances(estimator, getter, transform_func=None, norm_order=1):\n    \"\"\"\n    Retrieve and aggregate (ndim > 1)  the feature importances\n    from an estimator. Also optionally applies transformation.\n\n    Parameters\n    ----------\n    estimator : estimator\n        A scikit-learn estimator from which we want to get the feature\n        importances.\n\n    getter : \"auto\", str or callable\n        An attribute or a callable to get the feature importance. If `\"auto\"`,\n        `estimator` is expected to expose `coef_` or `feature_importances`.\n\n    transform_func : {\"norm\", \"square\"}, default=None\n        The transform to apply to the feature importances. By default (`None`)\n        no transformation is applied.\n\n    norm_order : int, default=1\n        The norm order to apply when `transform_func=\"norm\"`. Only applied\n        when `importances.ndim > 1`.\n\n    Returns\n    -------\n    importances : ndarray of shape (n_features,)\n        The features importances, optionally transformed.\n    \"\"\"\n    if isinstance(getter, str):\n        if getter == \"auto\":\n            if hasattr(estimator, \"coef_\"):\n                getter = attrgetter(\"coef_\")\n            elif hasattr(estimator, \"feature_importances_\"):\n                getter = attrgetter(\"feature_importances_\")\n            else:\n                raise ValueError(\n                    \"when `importance_getter=='auto'`, the underlying \"\n                    f\"estimator {estimator.__class__.__name__} should have \"\n                    \"`coef_` or `feature_importances_` attribute. Either \"\n                    \"pass a fitted estimator to feature selector or call fit \"\n                    \"before calling transform.\"\n                )\n        else:\n            getter = attrgetter(getter)\n    elif not callable(getter):\n        raise ValueError(\"`importance_getter` has to be a string or `callable`\")\n\n    importances = getter(estimator)\n\n    if transform_func is None:\n        return importances\n    elif transform_func == \"norm\":\n        if importances.ndim == 1:\n            importances = np.abs(importances)\n        else:\n            importances = np.linalg.norm(importances, axis=0, ord=norm_order)\n    elif transform_func == \"square\":\n        if importances.ndim == 1:\n            importances = safe_sqr(importances)\n        else:\n            importances = safe_sqr(importances).sum(axis=0)\n    else:\n        raise ValueError(\n            \"Valid values for `transform_func` are \"\n            \"None, 'norm' and 'square'. Those two \"\n            \"transformation are only supported now\"\n        )\n\n    return importances",
    "scikit-learn.sklearn.feature_selection._from_model._calculate_threshold": "def _calculate_threshold(estimator, importances, threshold):\n    \"\"\"Interpret the threshold value\"\"\"\n\n    if threshold is None:\n        # determine default from estimator\n        est_name = estimator.__class__.__name__\n        is_l1_penalized = hasattr(estimator, \"penalty\") and estimator.penalty == \"l1\"\n        is_lasso = \"Lasso\" in est_name\n        is_elasticnet_l1_penalized = est_name == \"ElasticNet\" and (\n            hasattr(estimator, \"l1_ratio\") and np.isclose(estimator.l1_ratio, 1.0)\n        )\n        is_elasticnetcv_l1_penalized = est_name == \"ElasticNetCV\" and (\n            hasattr(estimator, \"l1_ratio_\") and np.isclose(estimator.l1_ratio_, 1.0)\n        )\n        is_logreg_l1_penalized = est_name == \"LogisticRegression\" and (\n            hasattr(estimator, \"l1_ratio\") and np.isclose(estimator.l1_ratio, 1.0)\n        )\n        is_logregcv_l1_penalized = est_name == \"LogisticRegressionCV\" and (\n            hasattr(estimator, \"l1_ratio_\")\n            and np.all(np.isclose(estimator.l1_ratio_, 1.0))\n        )\n        if (\n            is_l1_penalized\n            or is_lasso\n            or is_elasticnet_l1_penalized\n            or is_elasticnetcv_l1_penalized\n            or is_logreg_l1_penalized\n            or is_logregcv_l1_penalized\n        ):\n            # the natural default threshold is 0 when l1 penalty was used\n            threshold = 1e-5\n        else:\n            threshold = \"mean\"\n\n    if isinstance(threshold, str):\n        if \"*\" in threshold:\n            scale, reference = threshold.split(\"*\")\n            scale = float(scale.strip())\n            reference = reference.strip()\n\n            if reference == \"median\":\n                reference = np.median(importances)\n            elif reference == \"mean\":\n                reference = np.mean(importances)\n            else:\n                raise ValueError(\"Unknown reference: \" + reference)\n\n            threshold = scale * reference\n\n        elif threshold == \"median\":\n            threshold = np.median(importances)\n\n        elif threshold == \"mean\":\n            threshold = np.mean(importances)\n\n        else:\n            raise ValueError(\n                \"Expected threshold='mean' or threshold='median' got %s\" % threshold\n            )\n\n    else:\n        threshold = float(threshold)\n\n    return threshold",
    "scikit-learn.sklearn.utils.validation.check_is_fitted": "def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):\n    \"\"\"Perform is_fitted validation for estimator.\n\n    Checks if the estimator is fitted by verifying the presence of\n    fitted attributes (ending with a trailing underscore) and otherwise\n    raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to\n    specify if the estimator is fitted or not. See\n    :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\n    for an example on how to use the API.\n\n    If no `attributes` are passed, this function will pass if an estimator is stateless.\n    An estimator can indicate it's stateless by setting the `requires_fit` tag. See\n    :ref:`estimator_tags` for more information. Note that the `requires_fit` tag\n    is ignored if `attributes` are passed.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Raises\n    ------\n    TypeError\n        If the estimator is a class or not an estimator instance\n\n    NotFittedError\n        If the attributes are not found.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.utils.validation import check_is_fitted\n    >>> from sklearn.exceptions import NotFittedError\n    >>> lr = LogisticRegression()\n    >>> try:\n    ...     check_is_fitted(lr)\n    ... except NotFittedError as exc:\n    ...     print(f\"Model is not fitted yet.\")\n    Model is not fitted yet.\n    >>> lr.fit([[1, 2], [1, 3]], [1, 0])\n    LogisticRegression()\n    >>> check_is_fitted(lr)\n    \"\"\"\n    if isclass(estimator):\n        raise TypeError(\"{} is a class, not an instance.\".format(estimator))\n    if msg is None:\n        msg = (\n            \"This %(name)s instance is not fitted yet. Call 'fit' with \"\n            \"appropriate arguments before using this estimator.\"\n        )\n\n    if not hasattr(estimator, \"fit\"):\n        raise TypeError(\"%s is not an estimator instance.\" % (estimator))\n\n    tags = get_tags(estimator)\n\n    if not tags.requires_fit and attributes is None:\n        return\n\n    if not _is_fitted(estimator, attributes, all_or_any):\n        raise NotFittedError(msg % {\"name\": type(estimator).__name__})"
}