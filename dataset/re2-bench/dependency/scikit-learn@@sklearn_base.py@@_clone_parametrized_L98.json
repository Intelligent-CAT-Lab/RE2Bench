{
    "scikit-learn.sklearn.base.<dictcomp>": "return {k: clone(v, safe=safe) for k, v in estimator.items()}\n",
    "scikit-learn.sklearn.base.<listcomp>": "return estimator_type([clone(e, safe=safe) for e in estimator])\n",
    "scikit-learn.sklearn.base.get_params": "def get_params(self, deep=True):\n    \"\"\"\n    Get parameters for this estimator.\n\n    Parameters\n    ----------\n    deep : bool, default=True\n        If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n\n    Returns\n    -------\n    params : dict\n        Parameter names mapped to their values.\n    \"\"\"\n    out = dict()\n    for key in self._get_param_names():\n        value = getattr(self, key)\n        if deep and hasattr(value, \"get_params\") and not isinstance(value, type):\n            deep_items = value.get_params().items()\n            out.update((key + \"__\" + k, val) for k, val in deep_items)\n        out[key] = value\n    return out",
    "scikit-learn.sklearn.base.__repr__": "def __repr__(self, N_CHAR_MAX=700):\n    # N_CHAR_MAX is the (approximate) maximum number of non-blank\n    # characters to render. We pass it as an optional parameter to ease\n    # the tests.\n\n    from sklearn.utils._pprint import _EstimatorPrettyPrinter\n\n    N_MAX_ELEMENTS_TO_SHOW = 30  # number of elements to show in sequences\n\n    # use ellipsis for sequences with a lot of elements\n    pp = _EstimatorPrettyPrinter(\n        compact=True,\n        indent=1,\n        indent_at_name=True,\n        n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW,\n    )\n\n    repr_ = pp.pformat(self)\n\n    # Use bruteforce ellipsis when there are a lot of non-blank characters\n    n_nonblank = len(\"\".join(repr_.split()))\n    if n_nonblank > N_CHAR_MAX:\n        lim = N_CHAR_MAX // 2  # apprx number of chars to keep on both ends\n        regex = r\"^(\\s*\\S){%d}\" % lim\n        # The regex '^(\\s*\\S){%d}' % n\n        # matches from the start of the string until the nth non-blank\n        # character:\n        # - ^ matches the start of string\n        # - (pattern){n} matches n repetitions of pattern\n        # - \\s*\\S matches a non-blank char following zero or more blanks\n        left_lim = re.match(regex, repr_).end()\n        right_lim = re.match(regex, repr_[::-1]).end()\n\n        if \"\\n\" in repr_[left_lim:-right_lim]:\n            # The left side and right side aren't on the same line.\n            # To avoid weird cuts, e.g.:\n            # categoric...ore',\n            # we need to start the right side with an appropriate newline\n            # character so that it renders properly as:\n            # categoric...\n            # handle_unknown='ignore',\n            # so we add [^\\n]*\\n which matches until the next \\n\n            regex += r\"[^\\n]*\\n\"\n            right_lim = re.match(regex, repr_[::-1]).end()\n\n        ellipsis = \"...\"\n        if left_lim + len(ellipsis) < len(repr_) - right_lim:\n            # Only add ellipsis if it results in a shorter repr\n            repr_ = repr_[:left_lim] + \"...\" + repr_[-right_lim:]\n\n    return repr_",
    "scikit-learn.sklearn.base.clone": "def clone(estimator, *, safe=True):\n    \"\"\"Construct a new unfitted estimator with the same parameters.\n\n    Clone does a deep copy of the model in an estimator\n    without actually copying attached data. It returns a new estimator\n    with the same parameters that has not been fitted on any data.\n\n    .. versionchanged:: 1.3\n        Delegates to `estimator.__sklearn_clone__` if the method exists.\n\n    Parameters\n    ----------\n    estimator : {list, tuple, set} of estimator instance or a single \\\n            estimator instance\n        The estimator or group of estimators to be cloned.\n    safe : bool, default=True\n        If safe is False, clone will fall back to a deep copy on objects\n        that are not estimators. Ignored if `estimator.__sklearn_clone__`\n        exists.\n\n    Returns\n    -------\n    estimator : object\n        The deep copy of the input, an estimator if input is an estimator.\n\n    Notes\n    -----\n    If the estimator's `random_state` parameter is an integer (or if the\n    estimator doesn't have a `random_state` parameter), an *exact clone* is\n    returned: the clone and the original estimator will give the exact same\n    results. Otherwise, *statistical clone* is returned: the clone might\n    return different results from the original estimator. More details can be\n    found in :ref:`randomness`.\n\n    Examples\n    --------\n    >>> from sklearn.base import clone\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> X = [[-1, 0], [0, 1], [0, -1], [1, 0]]\n    >>> y = [0, 0, 1, 1]\n    >>> classifier = LogisticRegression().fit(X, y)\n    >>> cloned_classifier = clone(classifier)\n    >>> hasattr(classifier, \"classes_\")\n    True\n    >>> hasattr(cloned_classifier, \"classes_\")\n    False\n    >>> classifier is cloned_classifier\n    False\n    \"\"\"\n    if hasattr(estimator, \"__sklearn_clone__\") and not inspect.isclass(estimator):\n        return estimator.__sklearn_clone__()\n    return _clone_parametrized(estimator, safe=safe)",
    "scikit-learn.sklearn.calibration.__init__": "def __init__(\n    self,\n    estimator=None,\n    *,\n    method=\"sigmoid\",\n    cv=None,\n    n_jobs=None,\n    ensemble=\"auto\",\n):\n    self.estimator = estimator\n    self.method = method\n    self.cv = cv\n    self.n_jobs = n_jobs\n    self.ensemble = ensemble",
    "scikit-learn.sklearn.cluster._affinity_propagation.__init__": "def __init__(\n    self,\n    *,\n    damping=0.5,\n    max_iter=200,\n    convergence_iter=15,\n    copy=True,\n    preference=None,\n    affinity=\"euclidean\",\n    verbose=False,\n    random_state=None,\n):\n    self.damping = damping\n    self.max_iter = max_iter\n    self.convergence_iter = convergence_iter\n    self.copy = copy\n    self.verbose = verbose\n    self.preference = preference\n    self.affinity = affinity\n    self.random_state = random_state",
    "scikit-learn.sklearn.cluster._agglomerative.__init__": "def __init__(\n    self,\n    n_clusters=2,\n    *,\n    metric=\"euclidean\",\n    memory=None,\n    connectivity=None,\n    compute_full_tree=\"auto\",\n    linkage=\"ward\",\n    distance_threshold=None,\n    compute_distances=False,\n):\n    self.n_clusters = n_clusters\n    self.distance_threshold = distance_threshold\n    self.memory = memory\n    self.connectivity = connectivity\n    self.compute_full_tree = compute_full_tree\n    self.linkage = linkage\n    self.metric = metric\n    self.compute_distances = compute_distances",
    "scikit-learn.sklearn.cluster._bicluster.__init__": "def __init__(\n    self,\n    n_clusters=3,\n    *,\n    method=\"bistochastic\",\n    n_components=6,\n    n_best=3,\n    svd_method=\"randomized\",\n    n_svd_vecs=None,\n    mini_batch=False,\n    init=\"k-means++\",\n    n_init=10,\n    random_state=None,\n):\n    super().__init__(\n        n_clusters, svd_method, n_svd_vecs, mini_batch, init, n_init, random_state\n    )\n    self.method = method\n    self.n_components = n_components\n    self.n_best = n_best",
    "scikit-learn.sklearn.cluster._birch.__init__": "def __init__(\n    self,\n    *,\n    threshold=0.5,\n    branching_factor=50,\n    n_clusters=3,\n    compute_labels=True,\n):\n    self.threshold = threshold\n    self.branching_factor = branching_factor\n    self.n_clusters = n_clusters\n    self.compute_labels = compute_labels",
    "scikit-learn.sklearn.cluster._bisect_k_means.__init__": "def __init__(\n    self,\n    n_clusters=8,\n    *,\n    init=\"random\",\n    n_init=1,\n    random_state=None,\n    max_iter=300,\n    verbose=0,\n    tol=1e-4,\n    copy_x=True,\n    algorithm=\"lloyd\",\n    bisecting_strategy=\"biggest_inertia\",\n):\n    super().__init__(\n        n_clusters=n_clusters,\n        init=init,\n        max_iter=max_iter,\n        verbose=verbose,\n        random_state=random_state,\n        tol=tol,\n        n_init=n_init,\n    )\n\n    self.copy_x = copy_x\n    self.algorithm = algorithm\n    self.bisecting_strategy = bisecting_strategy",
    "scikit-learn.sklearn.cluster._dbscan.__init__": "def __init__(\n    self,\n    eps=0.5,\n    *,\n    min_samples=5,\n    metric=\"euclidean\",\n    metric_params=None,\n    algorithm=\"auto\",\n    leaf_size=30,\n    p=None,\n    n_jobs=None,\n):\n    self.eps = eps\n    self.min_samples = min_samples\n    self.metric = metric\n    self.metric_params = metric_params\n    self.algorithm = algorithm\n    self.leaf_size = leaf_size\n    self.p = p\n    self.n_jobs = n_jobs",
    "scikit-learn.sklearn.cluster._hdbscan.hdbscan.__init__": "def __init__(\n    self,\n    min_cluster_size=5,\n    min_samples=None,\n    cluster_selection_epsilon=0.0,\n    max_cluster_size=None,\n    metric=\"euclidean\",\n    metric_params=None,\n    alpha=1.0,\n    algorithm=\"auto\",\n    leaf_size=40,\n    n_jobs=None,\n    cluster_selection_method=\"eom\",\n    allow_single_cluster=False,\n    store_centers=None,\n    copy=\"warn\",\n):\n    self.min_cluster_size = min_cluster_size\n    self.min_samples = min_samples\n    self.alpha = alpha\n    self.max_cluster_size = max_cluster_size\n    self.cluster_selection_epsilon = cluster_selection_epsilon\n    self.metric = metric\n    self.metric_params = metric_params\n    self.algorithm = algorithm\n    self.leaf_size = leaf_size\n    self.n_jobs = n_jobs\n    self.cluster_selection_method = cluster_selection_method\n    self.allow_single_cluster = allow_single_cluster\n    self.store_centers = store_centers\n    self.copy = copy",
    "scikit-learn.sklearn.cluster._kmeans.__init__": "def __init__(\n    self,\n    n_clusters=8,\n    *,\n    init=\"k-means++\",\n    max_iter=100,\n    batch_size=1024,\n    verbose=0,\n    compute_labels=True,\n    random_state=None,\n    tol=0.0,\n    max_no_improvement=10,\n    init_size=None,\n    n_init=\"auto\",\n    reassignment_ratio=0.01,\n):\n    super().__init__(\n        n_clusters=n_clusters,\n        init=init,\n        max_iter=max_iter,\n        verbose=verbose,\n        random_state=random_state,\n        tol=tol,\n        n_init=n_init,\n    )\n\n    self.max_no_improvement = max_no_improvement\n    self.batch_size = batch_size\n    self.compute_labels = compute_labels\n    self.init_size = init_size\n    self.reassignment_ratio = reassignment_ratio",
    "scikit-learn.sklearn.cluster._mean_shift.__init__": "def __init__(\n    self,\n    *,\n    bandwidth=None,\n    seeds=None,\n    bin_seeding=False,\n    min_bin_freq=1,\n    cluster_all=True,\n    n_jobs=None,\n    max_iter=300,\n):\n    self.bandwidth = bandwidth\n    self.seeds = seeds\n    self.bin_seeding = bin_seeding\n    self.cluster_all = cluster_all\n    self.min_bin_freq = min_bin_freq\n    self.n_jobs = n_jobs\n    self.max_iter = max_iter",
    "scikit-learn.sklearn.cluster._optics.__init__": "def __init__(\n    self,\n    *,\n    min_samples=5,\n    max_eps=np.inf,\n    metric=\"minkowski\",\n    p=2,\n    metric_params=None,\n    cluster_method=\"xi\",\n    eps=None,\n    xi=0.05,\n    predecessor_correction=True,\n    min_cluster_size=None,\n    algorithm=\"auto\",\n    leaf_size=30,\n    memory=None,\n    n_jobs=None,\n):\n    self.max_eps = max_eps\n    self.min_samples = min_samples\n    self.min_cluster_size = min_cluster_size\n    self.algorithm = algorithm\n    self.metric = metric\n    self.metric_params = metric_params\n    self.p = p\n    self.leaf_size = leaf_size\n    self.cluster_method = cluster_method\n    self.eps = eps\n    self.xi = xi\n    self.predecessor_correction = predecessor_correction\n    self.memory = memory\n    self.n_jobs = n_jobs",
    "scikit-learn.sklearn.cluster._spectral.__init__": "def __init__(\n    self,\n    n_clusters=8,\n    *,\n    eigen_solver=None,\n    n_components=None,\n    random_state=None,\n    n_init=10,\n    gamma=1.0,\n    affinity=\"rbf\",\n    n_neighbors=10,\n    eigen_tol=\"auto\",\n    assign_labels=\"kmeans\",\n    degree=3,\n    coef0=1,\n    kernel_params=None,\n    n_jobs=None,\n    verbose=False,\n):\n    self.n_clusters = n_clusters\n    self.eigen_solver = eigen_solver\n    self.n_components = n_components\n    self.random_state = random_state\n    self.n_init = n_init\n    self.gamma = gamma\n    self.affinity = affinity\n    self.n_neighbors = n_neighbors\n    self.eigen_tol = eigen_tol\n    self.assign_labels = assign_labels\n    self.degree = degree\n    self.coef0 = coef0\n    self.kernel_params = kernel_params\n    self.n_jobs = n_jobs\n    self.verbose = verbose",
    "scikit-learn.sklearn.compose._column_transformer.__init__": "def __init__(\n    self,\n    transformers,\n    *,\n    remainder=\"drop\",\n    sparse_threshold=0.3,\n    n_jobs=None,\n    transformer_weights=None,\n    verbose=False,\n    verbose_feature_names_out=True,\n    force_int_remainder_cols=\"deprecated\",\n):\n    self.transformers = transformers\n    self.remainder = remainder\n    self.sparse_threshold = sparse_threshold\n    self.n_jobs = n_jobs\n    self.transformer_weights = transformer_weights\n    self.verbose = verbose\n    self.verbose_feature_names_out = verbose_feature_names_out\n    self.force_int_remainder_cols = force_int_remainder_cols",
    "scikit-learn.sklearn.compose._column_transformer.get_params": "def get_params(self, deep=True):\n    \"\"\"Get parameters for this estimator.\n\n    Returns the parameters given in the constructor as well as the\n    estimators contained within the `transformers` of the\n    `ColumnTransformer`.\n\n    Parameters\n    ----------\n    deep : bool, default=True\n        If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n\n    Returns\n    -------\n    params : dict\n        Parameter names mapped to their values.\n    \"\"\"\n    return self._get_params(\"_transformers\", deep=deep)",
    "scikit-learn.sklearn.compose._target.__init__": "def __init__(\n    self,\n    regressor=None,\n    *,\n    transformer=None,\n    func=None,\n    inverse_func=None,\n    check_inverse=True,\n):\n    self.regressor = regressor\n    self.transformer = transformer\n    self.func = func\n    self.inverse_func = inverse_func\n    self.check_inverse = check_inverse",
    "scikit-learn.sklearn.covariance._elliptic_envelope.__init__": "def __init__(\n    self,\n    *,\n    store_precision=True,\n    assume_centered=False,\n    support_fraction=None,\n    contamination=0.1,\n    random_state=None,\n):\n    super().__init__(\n        store_precision=store_precision,\n        assume_centered=assume_centered,\n        support_fraction=support_fraction,\n        random_state=random_state,\n    )\n    self.contamination = contamination",
    "scikit-learn.sklearn.covariance._empirical_covariance.__init__": "def __init__(self, *, store_precision=True, assume_centered=False):\n    self.store_precision = store_precision\n    self.assume_centered = assume_centered",
    "scikit-learn.sklearn.covariance._graph_lasso.__init__": "def __init__(\n    self,\n    *,\n    alphas=4,\n    n_refinements=4,\n    cv=None,\n    tol=1e-4,\n    enet_tol=1e-4,\n    max_iter=100,\n    mode=\"cd\",\n    n_jobs=None,\n    verbose=False,\n    eps=np.finfo(np.float64).eps,\n    assume_centered=False,\n):\n    super().__init__(\n        tol=tol,\n        enet_tol=enet_tol,\n        max_iter=max_iter,\n        mode=mode,\n        verbose=verbose,\n        eps=eps,\n        assume_centered=assume_centered,\n    )\n    self.alphas = alphas\n    self.n_refinements = n_refinements\n    self.cv = cv\n    self.n_jobs = n_jobs",
    "scikit-learn.sklearn.covariance._robust_covariance.__init__": "def __init__(\n    self,\n    *,\n    store_precision=True,\n    assume_centered=False,\n    support_fraction=None,\n    random_state=None,\n):\n    self.store_precision = store_precision\n    self.assume_centered = assume_centered\n    self.support_fraction = support_fraction\n    self.random_state = random_state",
    "scikit-learn.sklearn.covariance._shrunk_covariance.__init__": "def __init__(self, *, store_precision=True, assume_centered=False, block_size=1000):\n    super().__init__(\n        store_precision=store_precision, assume_centered=assume_centered\n    )\n    self.block_size = block_size",
    "scikit-learn.sklearn.cross_decomposition._pls.__init__": "def __init__(self, n_components=2, *, scale=True, copy=True):\n    self.n_components = n_components\n    self.scale = scale\n    self.copy = copy",
    "scikit-learn.sklearn.decomposition._dict_learning.__init__": "def __init__(\n    self,\n    n_components=None,\n    *,\n    alpha=1,\n    max_iter=1_000,\n    fit_algorithm=\"lars\",\n    n_jobs=None,\n    batch_size=256,\n    shuffle=True,\n    dict_init=None,\n    transform_algorithm=\"omp\",\n    transform_n_nonzero_coefs=None,\n    transform_alpha=None,\n    verbose=False,\n    split_sign=False,\n    random_state=None,\n    positive_code=False,\n    positive_dict=False,\n    transform_max_iter=1000,\n    callback=None,\n    tol=1e-3,\n    max_no_improvement=10,\n):\n    super().__init__(\n        transform_algorithm,\n        transform_n_nonzero_coefs,\n        transform_alpha,\n        split_sign,\n        n_jobs,\n        positive_code,\n        transform_max_iter,\n    )\n    self.n_components = n_components\n    self.alpha = alpha\n    self.max_iter = max_iter\n    self.fit_algorithm = fit_algorithm\n    self.dict_init = dict_init\n    self.verbose = verbose\n    self.shuffle = shuffle\n    self.batch_size = batch_size\n    self.split_sign = split_sign\n    self.random_state = random_state\n    self.positive_dict = positive_dict\n    self.callback = callback\n    self.max_no_improvement = max_no_improvement\n    self.tol = tol",
    "scikit-learn.sklearn.decomposition._factor_analysis.__init__": "def __init__(\n    self,\n    n_components=None,\n    *,\n    tol=1e-2,\n    copy=True,\n    max_iter=1000,\n    noise_variance_init=None,\n    svd_method=\"randomized\",\n    iterated_power=3,\n    rotation=None,\n    random_state=0,\n):\n    self.n_components = n_components\n    self.copy = copy\n    self.tol = tol\n    self.max_iter = max_iter\n    self.svd_method = svd_method\n\n    self.noise_variance_init = noise_variance_init\n    self.iterated_power = iterated_power\n    self.random_state = random_state\n    self.rotation = rotation",
    "scikit-learn.sklearn.decomposition._fastica.__init__": "def __init__(\n    self,\n    n_components=None,\n    *,\n    algorithm=\"parallel\",\n    whiten=\"unit-variance\",\n    fun=\"logcosh\",\n    fun_args=None,\n    max_iter=200,\n    tol=1e-4,\n    w_init=None,\n    whiten_solver=\"svd\",\n    random_state=None,\n):\n    super().__init__()\n    self.n_components = n_components\n    self.algorithm = algorithm\n    self.whiten = whiten\n    self.fun = fun\n    self.fun_args = fun_args\n    self.max_iter = max_iter\n    self.tol = tol\n    self.w_init = w_init\n    self.whiten_solver = whiten_solver\n    self.random_state = random_state",
    "scikit-learn.sklearn.decomposition._incremental_pca.__init__": "def __init__(self, n_components=None, *, whiten=False, copy=True, batch_size=None):\n    self.n_components = n_components\n    self.whiten = whiten\n    self.copy = copy\n    self.batch_size = batch_size",
    "scikit-learn.sklearn.decomposition._kernel_pca.__init__": "def __init__(\n    self,\n    n_components=None,\n    *,\n    kernel=\"linear\",\n    gamma=None,\n    degree=3,\n    coef0=1,\n    kernel_params=None,\n    alpha=1.0,\n    fit_inverse_transform=False,\n    eigen_solver=\"auto\",\n    tol=0,\n    max_iter=None,\n    iterated_power=\"auto\",\n    remove_zero_eig=False,\n    random_state=None,\n    copy_X=True,\n    n_jobs=None,\n):\n    self.n_components = n_components\n    self.kernel = kernel\n    self.kernel_params = kernel_params\n    self.gamma = gamma\n    self.degree = degree\n    self.coef0 = coef0\n    self.alpha = alpha\n    self.fit_inverse_transform = fit_inverse_transform\n    self.eigen_solver = eigen_solver\n    self.tol = tol\n    self.max_iter = max_iter\n    self.iterated_power = iterated_power\n    self.remove_zero_eig = remove_zero_eig\n    self.random_state = random_state\n    self.n_jobs = n_jobs\n    self.copy_X = copy_X",
    "scikit-learn.sklearn.decomposition._lda.__init__": "def __init__(\n    self,\n    n_components=10,\n    *,\n    doc_topic_prior=None,\n    topic_word_prior=None,\n    learning_method=\"batch\",\n    learning_decay=0.7,\n    learning_offset=10.0,\n    max_iter=10,\n    batch_size=128,\n    evaluate_every=-1,\n    total_samples=1e6,\n    perp_tol=1e-1,\n    mean_change_tol=1e-3,\n    max_doc_update_iter=100,\n    n_jobs=None,\n    verbose=0,\n    random_state=None,\n):\n    self.n_components = n_components\n    self.doc_topic_prior = doc_topic_prior\n    self.topic_word_prior = topic_word_prior\n    self.learning_method = learning_method\n    self.learning_decay = learning_decay\n    self.learning_offset = learning_offset\n    self.max_iter = max_iter\n    self.batch_size = batch_size\n    self.evaluate_every = evaluate_every\n    self.total_samples = total_samples\n    self.perp_tol = perp_tol\n    self.mean_change_tol = mean_change_tol\n    self.max_doc_update_iter = max_doc_update_iter\n    self.n_jobs = n_jobs\n    self.verbose = verbose\n    self.random_state = random_state",
    "scikit-learn.sklearn.decomposition._nmf.__init__": "def __init__(\n    self,\n    n_components=\"auto\",\n    *,\n    init=None,\n    batch_size=1024,\n    beta_loss=\"frobenius\",\n    tol=1e-4,\n    max_no_improvement=10,\n    max_iter=200,\n    alpha_W=0.0,\n    alpha_H=\"same\",\n    l1_ratio=0.0,\n    forget_factor=0.7,\n    fresh_restarts=False,\n    fresh_restarts_max_iter=30,\n    transform_max_iter=None,\n    random_state=None,\n    verbose=0,\n):\n    super().__init__(\n        n_components=n_components,\n        init=init,\n        beta_loss=beta_loss,\n        tol=tol,\n        max_iter=max_iter,\n        random_state=random_state,\n        alpha_W=alpha_W,\n        alpha_H=alpha_H,\n        l1_ratio=l1_ratio,\n        verbose=verbose,\n    )\n\n    self.max_no_improvement = max_no_improvement\n    self.batch_size = batch_size\n    self.forget_factor = forget_factor\n    self.fresh_restarts = fresh_restarts\n    self.fresh_restarts_max_iter = fresh_restarts_max_iter\n    self.transform_max_iter = transform_max_iter",
    "scikit-learn.sklearn.decomposition._pca.__init__": "def __init__(\n    self,\n    n_components=None,\n    *,\n    copy=True,\n    whiten=False,\n    svd_solver=\"auto\",\n    tol=0.0,\n    iterated_power=\"auto\",\n    n_oversamples=10,\n    power_iteration_normalizer=\"auto\",\n    random_state=None,\n):\n    self.n_components = n_components\n    self.copy = copy\n    self.whiten = whiten\n    self.svd_solver = svd_solver\n    self.tol = tol\n    self.iterated_power = iterated_power\n    self.n_oversamples = n_oversamples\n    self.power_iteration_normalizer = power_iteration_normalizer\n    self.random_state = random_state",
    "scikit-learn.sklearn.decomposition._sparse_pca.__init__": "def __init__(\n    self,\n    n_components=None,\n    *,\n    alpha=1,\n    ridge_alpha=0.01,\n    max_iter=1_000,\n    callback=None,\n    batch_size=3,\n    verbose=False,\n    shuffle=True,\n    n_jobs=None,\n    method=\"lars\",\n    random_state=None,\n    tol=1e-3,\n    max_no_improvement=10,\n):\n    super().__init__(\n        n_components=n_components,\n        alpha=alpha,\n        ridge_alpha=ridge_alpha,\n        max_iter=max_iter,\n        tol=tol,\n        method=method,\n        n_jobs=n_jobs,\n        verbose=verbose,\n        random_state=random_state,\n    )\n    self.callback = callback\n    self.batch_size = batch_size\n    self.shuffle = shuffle\n    self.max_no_improvement = max_no_improvement",
    "scikit-learn.sklearn.decomposition._truncated_svd.__init__": "def __init__(\n    self,\n    n_components=2,\n    *,\n    algorithm=\"randomized\",\n    n_iter=5,\n    n_oversamples=10,\n    power_iteration_normalizer=\"auto\",\n    random_state=None,\n    tol=0.0,\n):\n    self.algorithm = algorithm\n    self.n_components = n_components\n    self.n_iter = n_iter\n    self.n_oversamples = n_oversamples\n    self.power_iteration_normalizer = power_iteration_normalizer\n    self.random_state = random_state\n    self.tol = tol",
    "scikit-learn.sklearn.discriminant_analysis.__init__": "def __init__(\n    self,\n    *,\n    solver=\"svd\",\n    shrinkage=None,\n    priors=None,\n    reg_param=0.0,\n    store_covariance=False,\n    tol=1.0e-4,\n    covariance_estimator=None,\n):\n    self.solver = solver\n    self.shrinkage = shrinkage\n    self.priors = priors\n    self.reg_param = reg_param\n    self.store_covariance = store_covariance\n    self.tol = tol\n    self.covariance_estimator = covariance_estimator",
    "scikit-learn.sklearn.dummy.__init__": "def __init__(self, *, strategy=\"mean\", constant=None, quantile=None):\n    self.strategy = strategy\n    self.constant = constant\n    self.quantile = quantile",
    "scikit-learn.sklearn.ensemble._bagging.__init__": "def __init__(\n    self,\n    estimator=None,\n    n_estimators=10,\n    *,\n    max_samples=1.0,\n    max_features=1.0,\n    bootstrap=True,\n    bootstrap_features=False,\n    oob_score=False,\n    warm_start=False,\n    n_jobs=None,\n    random_state=None,\n    verbose=0,\n):\n    super().__init__(\n        estimator=estimator,\n        n_estimators=n_estimators,\n        max_samples=max_samples,\n        max_features=max_features,\n        bootstrap=bootstrap,\n        bootstrap_features=bootstrap_features,\n        oob_score=oob_score,\n        warm_start=warm_start,\n        n_jobs=n_jobs,\n        random_state=random_state,\n        verbose=verbose,\n    )",
    "scikit-learn.sklearn.ensemble._base.get_params": "def get_params(self, deep=True):\n    \"\"\"\n    Get the parameters of an estimator from the ensemble.\n\n    Returns the parameters given in the constructor as well as the\n    estimators contained within the `estimators` parameter.\n\n    Parameters\n    ----------\n    deep : bool, default=True\n        Setting it to True gets the various estimators and the parameters\n        of the estimators as well.\n\n    Returns\n    -------\n    params : dict\n        Parameter and estimator names mapped to their values or parameter\n        names mapped to their values.\n    \"\"\"\n    return super()._get_params(\"estimators\", deep=deep)",
    "scikit-learn.sklearn.ensemble._forest.__init__": "def __init__(\n    self,\n    n_estimators=100,\n    *,\n    max_depth=5,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    sparse_output=True,\n    n_jobs=None,\n    random_state=None,\n    verbose=0,\n    warm_start=False,\n):\n    super().__init__(\n        estimator=ExtraTreeRegressor(),\n        n_estimators=n_estimators,\n        estimator_params=(\n            \"criterion\",\n            \"max_depth\",\n            \"min_samples_split\",\n            \"min_samples_leaf\",\n            \"min_weight_fraction_leaf\",\n            \"max_features\",\n            \"max_leaf_nodes\",\n            \"min_impurity_decrease\",\n            \"random_state\",\n        ),\n        bootstrap=False,\n        oob_score=False,\n        n_jobs=n_jobs,\n        random_state=random_state,\n        verbose=verbose,\n        warm_start=warm_start,\n        max_samples=None,\n    )\n\n    self.max_depth = max_depth\n    self.min_samples_split = min_samples_split\n    self.min_samples_leaf = min_samples_leaf\n    self.min_weight_fraction_leaf = min_weight_fraction_leaf\n    self.max_leaf_nodes = max_leaf_nodes\n    self.min_impurity_decrease = min_impurity_decrease\n    self.sparse_output = sparse_output",
    "scikit-learn.sklearn.ensemble._gb.__init__": "def __init__(\n    self,\n    *,\n    loss=\"squared_error\",\n    learning_rate=0.1,\n    n_estimators=100,\n    subsample=1.0,\n    criterion=\"friedman_mse\",\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_depth=3,\n    min_impurity_decrease=0.0,\n    init=None,\n    random_state=None,\n    max_features=None,\n    alpha=0.9,\n    verbose=0,\n    max_leaf_nodes=None,\n    warm_start=False,\n    validation_fraction=0.1,\n    n_iter_no_change=None,\n    tol=1e-4,\n    ccp_alpha=0.0,\n):\n    super().__init__(\n        loss=loss,\n        learning_rate=learning_rate,\n        n_estimators=n_estimators,\n        criterion=criterion,\n        min_samples_split=min_samples_split,\n        min_samples_leaf=min_samples_leaf,\n        min_weight_fraction_leaf=min_weight_fraction_leaf,\n        max_depth=max_depth,\n        init=init,\n        subsample=subsample,\n        max_features=max_features,\n        min_impurity_decrease=min_impurity_decrease,\n        random_state=random_state,\n        alpha=alpha,\n        verbose=verbose,\n        max_leaf_nodes=max_leaf_nodes,\n        warm_start=warm_start,\n        validation_fraction=validation_fraction,\n        n_iter_no_change=n_iter_no_change,\n        tol=tol,\n        ccp_alpha=ccp_alpha,\n    )",
    "scikit-learn.sklearn.ensemble._hist_gradient_boosting.gradient_boosting.__init__": "def __init__(\n    self,\n    loss=\"log_loss\",\n    *,\n    learning_rate=0.1,\n    max_iter=100,\n    max_leaf_nodes=31,\n    max_depth=None,\n    min_samples_leaf=20,\n    l2_regularization=0.0,\n    max_features=1.0,\n    max_bins=255,\n    categorical_features=\"from_dtype\",\n    monotonic_cst=None,\n    interaction_cst=None,\n    warm_start=False,\n    early_stopping=\"auto\",\n    scoring=\"loss\",\n    validation_fraction=0.1,\n    n_iter_no_change=10,\n    tol=1e-7,\n    verbose=0,\n    random_state=None,\n    class_weight=None,\n):\n    super().__init__(\n        loss=loss,\n        learning_rate=learning_rate,\n        max_iter=max_iter,\n        max_leaf_nodes=max_leaf_nodes,\n        max_depth=max_depth,\n        min_samples_leaf=min_samples_leaf,\n        l2_regularization=l2_regularization,\n        max_features=max_features,\n        max_bins=max_bins,\n        categorical_features=categorical_features,\n        monotonic_cst=monotonic_cst,\n        interaction_cst=interaction_cst,\n        warm_start=warm_start,\n        early_stopping=early_stopping,\n        scoring=scoring,\n        validation_fraction=validation_fraction,\n        n_iter_no_change=n_iter_no_change,\n        tol=tol,\n        verbose=verbose,\n        random_state=random_state,\n    )\n    self.class_weight = class_weight",
    "scikit-learn.sklearn.ensemble._iforest.__init__": "def __init__(\n    self,\n    *,\n    n_estimators=100,\n    max_samples=\"auto\",\n    contamination=\"auto\",\n    max_features=1.0,\n    bootstrap=False,\n    n_jobs=None,\n    random_state=None,\n    verbose=0,\n    warm_start=False,\n):\n    super().__init__(\n        estimator=None,\n        # here above max_features has no links with self.max_features\n        bootstrap=bootstrap,\n        bootstrap_features=False,\n        n_estimators=n_estimators,\n        max_samples=max_samples,\n        max_features=max_features,\n        warm_start=warm_start,\n        n_jobs=n_jobs,\n        random_state=random_state,\n        verbose=verbose,\n    )\n\n    self.contamination = contamination",
    "scikit-learn.sklearn.ensemble._stacking.__init__": "def __init__(\n    self,\n    estimators,\n    final_estimator=None,\n    *,\n    cv=None,\n    n_jobs=None,\n    passthrough=False,\n    verbose=0,\n):\n    super().__init__(\n        estimators=estimators,\n        final_estimator=final_estimator,\n        cv=cv,\n        stack_method=\"predict\",\n        n_jobs=n_jobs,\n        passthrough=passthrough,\n        verbose=verbose,\n    )",
    "scikit-learn.sklearn.ensemble._voting.__init__": "def __init__(self, estimators, *, weights=None, n_jobs=None, verbose=False):\n    super().__init__(estimators=estimators)\n    self.weights = weights\n    self.n_jobs = n_jobs\n    self.verbose = verbose",
    "scikit-learn.sklearn.ensemble._weight_boosting.__init__": "def __init__(\n    self,\n    estimator=None,\n    *,\n    n_estimators=50,\n    learning_rate=1.0,\n    loss=\"linear\",\n    random_state=None,\n):\n    super().__init__(\n        estimator=estimator,\n        n_estimators=n_estimators,\n        learning_rate=learning_rate,\n        random_state=random_state,\n    )\n\n    self.loss = loss\n    self.random_state = random_state",
    "scikit-learn.sklearn.feature_extraction._dict_vectorizer.__init__": "def __init__(self, *, dtype=np.float64, separator=\"=\", sparse=True, sort=True):\n    self.dtype = dtype\n    self.separator = separator\n    self.sparse = sparse\n    self.sort = sort",
    "scikit-learn.sklearn.feature_extraction._hash.__init__": "def __init__(\n    self,\n    n_features=(2**20),\n    *,\n    input_type=\"dict\",\n    dtype=np.float64,\n    alternate_sign=True,\n):\n    self.dtype = dtype\n    self.input_type = input_type\n    self.n_features = n_features\n    self.alternate_sign = alternate_sign",
    "scikit-learn.sklearn.feature_extraction.image.__init__": "def __init__(self, *, patch_size=None, max_patches=None, random_state=None):\n    self.patch_size = patch_size\n    self.max_patches = max_patches\n    self.random_state = random_state",
    "scikit-learn.sklearn.feature_extraction.text.__init__": "def __init__(\n    self,\n    *,\n    input=\"content\",\n    encoding=\"utf-8\",\n    decode_error=\"strict\",\n    strip_accents=None,\n    lowercase=True,\n    preprocessor=None,\n    tokenizer=None,\n    stop_words=None,\n    token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n    ngram_range=(1, 1),\n    analyzer=\"word\",\n    n_features=(2**20),\n    binary=False,\n    norm=\"l2\",\n    alternate_sign=True,\n    dtype=np.float64,\n):\n    self.input = input\n    self.encoding = encoding\n    self.decode_error = decode_error\n    self.strip_accents = strip_accents\n    self.preprocessor = preprocessor\n    self.tokenizer = tokenizer\n    self.analyzer = analyzer\n    self.lowercase = lowercase\n    self.token_pattern = token_pattern\n    self.stop_words = stop_words\n    self.n_features = n_features\n    self.ngram_range = ngram_range\n    self.binary = binary\n    self.norm = norm\n    self.alternate_sign = alternate_sign\n    self.dtype = dtype",
    "scikit-learn.sklearn.feature_selection._from_model.__init__": "def __init__(\n    self,\n    estimator,\n    *,\n    threshold=None,\n    prefit=False,\n    norm_order=1,\n    max_features=None,\n    importance_getter=\"auto\",\n):\n    self.estimator = estimator\n    self.threshold = threshold\n    self.prefit = prefit\n    self.importance_getter = importance_getter\n    self.norm_order = norm_order\n    self.max_features = max_features",
    "scikit-learn.sklearn.feature_selection._rfe.__init__": "def __init__(\n    self,\n    estimator,\n    *,\n    step=1,\n    min_features_to_select=1,\n    cv=None,\n    scoring=None,\n    verbose=0,\n    n_jobs=None,\n    importance_getter=\"auto\",\n):\n    self.estimator = estimator\n    self.step = step\n    self.importance_getter = importance_getter\n    self.cv = cv\n    self.scoring = scoring\n    self.verbose = verbose\n    self.n_jobs = n_jobs\n    self.min_features_to_select = min_features_to_select",
    "scikit-learn.sklearn.feature_selection._sequential.__init__": "def __init__(\n    self,\n    estimator,\n    *,\n    n_features_to_select=\"auto\",\n    tol=None,\n    direction=\"forward\",\n    scoring=None,\n    cv=5,\n    n_jobs=None,\n):\n    self.estimator = estimator\n    self.n_features_to_select = n_features_to_select\n    self.tol = tol\n    self.direction = direction\n    self.scoring = scoring\n    self.cv = cv\n    self.n_jobs = n_jobs",
    "scikit-learn.sklearn.feature_selection._univariate_selection.__init__": "def __init__(self, score_func=f_classif, *, alpha=5e-2):\n    super().__init__(score_func=score_func)\n    self.alpha = alpha",
    "scikit-learn.sklearn.feature_selection._variance_threshold.__init__": "def __init__(self, threshold=0.0):\n    self.threshold = threshold",
    "scikit-learn.sklearn.gaussian_process._gpc.__init__": "def __init__(\n    self,\n    kernel=None,\n    *,\n    optimizer=\"fmin_l_bfgs_b\",\n    n_restarts_optimizer=0,\n    max_iter_predict=100,\n    warm_start=False,\n    copy_X_train=True,\n    random_state=None,\n    multi_class=\"one_vs_rest\",\n    n_jobs=None,\n):\n    self.kernel = kernel\n    self.optimizer = optimizer\n    self.n_restarts_optimizer = n_restarts_optimizer\n    self.max_iter_predict = max_iter_predict\n    self.warm_start = warm_start\n    self.copy_X_train = copy_X_train\n    self.random_state = random_state\n    self.multi_class = multi_class\n    self.n_jobs = n_jobs",
    "scikit-learn.sklearn.gaussian_process._gpr.__init__": "def __init__(\n    self,\n    kernel=None,\n    *,\n    alpha=1e-10,\n    optimizer=\"fmin_l_bfgs_b\",\n    n_restarts_optimizer=0,\n    normalize_y=False,\n    copy_X_train=True,\n    n_targets=None,\n    random_state=None,\n):\n    self.kernel = kernel\n    self.alpha = alpha\n    self.optimizer = optimizer\n    self.n_restarts_optimizer = n_restarts_optimizer\n    self.normalize_y = normalize_y\n    self.copy_X_train = copy_X_train\n    self.n_targets = n_targets\n    self.random_state = random_state",
    "scikit-learn.sklearn.gaussian_process.kernels.__init__": "def __init__(self, k1, k2):\n    self.k1 = k1\n    self.k2 = k2",
    "scikit-learn.sklearn.gaussian_process.kernels.get_params": "def get_params(self, deep=True):\n    \"\"\"Get parameters of this kernel.\n\n    Parameters\n    ----------\n    deep : bool, default=True\n        If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n\n    Returns\n    -------\n    params : dict\n        Parameter names mapped to their values.\n    \"\"\"\n    params = dict(k1=self.k1, k2=self.k2)\n    if deep:\n        deep_items = self.k1.get_params().items()\n        params.update((\"k1__\" + k, val) for k, val in deep_items)\n        deep_items = self.k2.get_params().items()\n        params.update((\"k2__\" + k, val) for k, val in deep_items)\n\n    return params",
    "scikit-learn.sklearn.impute._base.__init__": "def __init__(\n    self,\n    *,\n    missing_values=np.nan,\n    features=\"missing-only\",\n    sparse=\"auto\",\n    error_on_new=True,\n):\n    self.missing_values = missing_values\n    self.features = features\n    self.sparse = sparse\n    self.error_on_new = error_on_new",
    "scikit-learn.sklearn.impute._iterative.__init__": "def __init__(\n    self,\n    estimator=None,\n    *,\n    missing_values=np.nan,\n    sample_posterior=False,\n    max_iter=10,\n    tol=1e-3,\n    n_nearest_features=None,\n    initial_strategy=\"mean\",\n    fill_value=None,\n    imputation_order=\"ascending\",\n    skip_complete=False,\n    min_value=-np.inf,\n    max_value=np.inf,\n    verbose=0,\n    random_state=None,\n    add_indicator=False,\n    keep_empty_features=False,\n):\n    super().__init__(\n        missing_values=missing_values,\n        add_indicator=add_indicator,\n        keep_empty_features=keep_empty_features,\n    )\n\n    self.estimator = estimator\n    self.sample_posterior = sample_posterior\n    self.max_iter = max_iter\n    self.tol = tol\n    self.n_nearest_features = n_nearest_features\n    self.initial_strategy = initial_strategy\n    self.fill_value = fill_value\n    self.imputation_order = imputation_order\n    self.skip_complete = skip_complete\n    self.min_value = min_value\n    self.max_value = max_value\n    self.verbose = verbose\n    self.random_state = random_state",
    "scikit-learn.sklearn.impute._knn.__init__": "def __init__(\n    self,\n    *,\n    missing_values=np.nan,\n    n_neighbors=5,\n    weights=\"uniform\",\n    metric=\"nan_euclidean\",\n    copy=True,\n    add_indicator=False,\n    keep_empty_features=False,\n):\n    super().__init__(\n        missing_values=missing_values,\n        add_indicator=add_indicator,\n        keep_empty_features=keep_empty_features,\n    )\n    self.n_neighbors = n_neighbors\n    self.weights = weights\n    self.metric = metric\n    self.copy = copy",
    "scikit-learn.sklearn.isotonic.__init__": "def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds=\"nan\"):\n    self.y_min = y_min\n    self.y_max = y_max\n    self.increasing = increasing\n    self.out_of_bounds = out_of_bounds",
    "scikit-learn.sklearn.kernel_approximation.__init__": "def __init__(\n    self,\n    kernel=\"rbf\",\n    *,\n    gamma=None,\n    coef0=None,\n    degree=None,\n    kernel_params=None,\n    n_components=100,\n    random_state=None,\n    n_jobs=None,\n):\n    self.kernel = kernel\n    self.gamma = gamma\n    self.coef0 = coef0\n    self.degree = degree\n    self.kernel_params = kernel_params\n    self.n_components = n_components\n    self.random_state = random_state\n    self.n_jobs = n_jobs",
    "scikit-learn.sklearn.kernel_ridge.__init__": "def __init__(\n    self,\n    alpha=1,\n    *,\n    kernel=\"linear\",\n    gamma=None,\n    degree=3,\n    coef0=1,\n    kernel_params=None,\n):\n    self.alpha = alpha\n    self.kernel = kernel\n    self.gamma = gamma\n    self.degree = degree\n    self.coef0 = coef0\n    self.kernel_params = kernel_params",
    "scikit-learn.sklearn.linear_model._base.__init__": "def __init__(\n    self,\n    *,\n    fit_intercept=True,\n    copy_X=True,\n    tol=1e-6,\n    n_jobs=None,\n    positive=False,\n):\n    self.fit_intercept = fit_intercept\n    self.copy_X = copy_X\n    self.tol = tol\n    self.n_jobs = n_jobs\n    self.positive = positive",
    "scikit-learn.sklearn.linear_model._bayes.__init__": "def __init__(\n    self,\n    *,\n    max_iter=300,\n    tol=1.0e-3,\n    alpha_1=1.0e-6,\n    alpha_2=1.0e-6,\n    lambda_1=1.0e-6,\n    lambda_2=1.0e-6,\n    compute_score=False,\n    threshold_lambda=1.0e4,\n    fit_intercept=True,\n    copy_X=True,\n    verbose=False,\n):\n    self.max_iter = max_iter\n    self.tol = tol\n    self.fit_intercept = fit_intercept\n    self.alpha_1 = alpha_1\n    self.alpha_2 = alpha_2\n    self.lambda_1 = lambda_1\n    self.lambda_2 = lambda_2\n    self.compute_score = compute_score\n    self.threshold_lambda = threshold_lambda\n    self.copy_X = copy_X\n    self.verbose = verbose",
    "scikit-learn.sklearn.linear_model._coordinate_descent.__init__": "def __init__(\n    self,\n    alpha=1.0,\n    *,\n    l1_ratio=0.5,\n    fit_intercept=True,\n    precompute=False,\n    max_iter=1000,\n    copy_X=True,\n    tol=1e-4,\n    warm_start=False,\n    positive=False,\n    random_state=None,\n    selection=\"cyclic\",\n):\n    self.alpha = alpha\n    self.l1_ratio = l1_ratio\n    self.fit_intercept = fit_intercept\n    self.precompute = precompute\n    self.max_iter = max_iter\n    self.copy_X = copy_X\n    self.tol = tol\n    self.warm_start = warm_start\n    self.positive = positive\n    self.random_state = random_state\n    self.selection = selection",
    "scikit-learn.sklearn.linear_model._glm.glm.__init__": "def __init__(\n    self,\n    *,\n    power=0.0,\n    alpha=1.0,\n    fit_intercept=True,\n    link=\"auto\",\n    solver=\"lbfgs\",\n    max_iter=100,\n    tol=1e-4,\n    warm_start=False,\n    verbose=0,\n):\n    super().__init__(\n        alpha=alpha,\n        fit_intercept=fit_intercept,\n        solver=solver,\n        max_iter=max_iter,\n        tol=tol,\n        warm_start=warm_start,\n        verbose=verbose,\n    )\n    self.link = link\n    self.power = power",
    "scikit-learn.sklearn.linear_model._huber.__init__": "def __init__(\n    self,\n    *,\n    epsilon=1.35,\n    max_iter=100,\n    alpha=0.0001,\n    warm_start=False,\n    fit_intercept=True,\n    tol=1e-05,\n):\n    self.epsilon = epsilon\n    self.max_iter = max_iter\n    self.alpha = alpha\n    self.warm_start = warm_start\n    self.fit_intercept = fit_intercept\n    self.tol = tol",
    "scikit-learn.sklearn.linear_model._least_angle.__init__": "def __init__(\n    self,\n    criterion=\"aic\",\n    *,\n    fit_intercept=True,\n    verbose=False,\n    precompute=\"auto\",\n    max_iter=500,\n    eps=np.finfo(float).eps,\n    copy_X=True,\n    positive=False,\n    noise_variance=None,\n):\n    self.criterion = criterion\n    self.fit_intercept = fit_intercept\n    self.positive = positive\n    self.max_iter = max_iter\n    self.verbose = verbose\n    self.copy_X = copy_X\n    self.precompute = precompute\n    self.eps = eps\n    self.fit_path = True\n    self.noise_variance = noise_variance",
    "scikit-learn.sklearn.linear_model._logistic.__init__": "# Could not extract code for scikit-learn.sklearn.linear_model._logistic.__init__",
    "scikit-learn.sklearn.linear_model._omp.__init__": "def __init__(\n    self,\n    *,\n    n_nonzero_coefs=None,\n    tol=None,\n    fit_intercept=True,\n    precompute=\"auto\",\n):\n    self.n_nonzero_coefs = n_nonzero_coefs\n    self.tol = tol\n    self.fit_intercept = fit_intercept\n    self.precompute = precompute",
    "scikit-learn.sklearn.linear_model._passive_aggressive.__init__": "def __init__(\n    self,\n    *,\n    C=1.0,\n    fit_intercept=True,\n    max_iter=1000,\n    tol=1e-3,\n    early_stopping=False,\n    validation_fraction=0.1,\n    n_iter_no_change=5,\n    shuffle=True,\n    verbose=0,\n    loss=\"epsilon_insensitive\",\n    epsilon=DEFAULT_EPSILON,\n    random_state=None,\n    warm_start=False,\n    average=False,\n):\n    super().__init__(\n        loss=loss,\n        penalty=None,\n        l1_ratio=0,\n        epsilon=epsilon,\n        eta0=C,\n        fit_intercept=fit_intercept,\n        max_iter=max_iter,\n        tol=tol,\n        early_stopping=early_stopping,\n        validation_fraction=validation_fraction,\n        n_iter_no_change=n_iter_no_change,\n        shuffle=shuffle,\n        verbose=verbose,\n        random_state=random_state,\n        warm_start=warm_start,\n        average=average,\n    )\n    self.C = C",
    "scikit-learn.sklearn.linear_model._perceptron.__init__": "def __init__(\n    self,\n    *,\n    penalty=None,\n    alpha=0.0001,\n    l1_ratio=0.15,\n    fit_intercept=True,\n    max_iter=1000,\n    tol=1e-3,\n    shuffle=True,\n    verbose=0,\n    eta0=1.0,\n    n_jobs=None,\n    random_state=0,\n    early_stopping=False,\n    validation_fraction=0.1,\n    n_iter_no_change=5,\n    class_weight=None,\n    warm_start=False,\n):\n    super().__init__(\n        loss=\"perceptron\",\n        penalty=penalty,\n        alpha=alpha,\n        l1_ratio=l1_ratio,\n        fit_intercept=fit_intercept,\n        max_iter=max_iter,\n        tol=tol,\n        shuffle=shuffle,\n        verbose=verbose,\n        random_state=random_state,\n        learning_rate=\"constant\",\n        eta0=eta0,\n        early_stopping=early_stopping,\n        validation_fraction=validation_fraction,\n        n_iter_no_change=n_iter_no_change,\n        power_t=0.5,\n        warm_start=warm_start,\n        class_weight=class_weight,\n        n_jobs=n_jobs,\n    )",
    "scikit-learn.sklearn.linear_model._quantile.__init__": "def __init__(\n    self,\n    *,\n    quantile=0.5,\n    alpha=1.0,\n    fit_intercept=True,\n    solver=\"highs\",\n    solver_options=None,\n):\n    self.quantile = quantile\n    self.alpha = alpha\n    self.fit_intercept = fit_intercept\n    self.solver = solver\n    self.solver_options = solver_options",
    "scikit-learn.sklearn.linear_model._ransac.__init__": "def __init__(\n    self,\n    estimator=None,\n    *,\n    min_samples=None,\n    residual_threshold=None,\n    is_data_valid=None,\n    is_model_valid=None,\n    max_trials=100,\n    max_skips=np.inf,\n    stop_n_inliers=np.inf,\n    stop_score=np.inf,\n    stop_probability=0.99,\n    loss=\"absolute_error\",\n    random_state=None,\n):\n    self.estimator = estimator\n    self.min_samples = min_samples\n    self.residual_threshold = residual_threshold\n    self.is_data_valid = is_data_valid\n    self.is_model_valid = is_model_valid\n    self.max_trials = max_trials\n    self.max_skips = max_skips\n    self.stop_n_inliers = stop_n_inliers\n    self.stop_score = stop_score\n    self.stop_probability = stop_probability\n    self.random_state = random_state\n    self.loss = loss",
    "scikit-learn.sklearn.linear_model._ridge.__init__": "def __init__(\n    self,\n    alphas=(0.1, 1.0, 10.0),\n    *,\n    fit_intercept=True,\n    scoring=None,\n    cv=None,\n    class_weight=None,\n    store_cv_results=False,\n):\n    super().__init__(\n        alphas=alphas,\n        fit_intercept=fit_intercept,\n        scoring=scoring,\n        cv=cv,\n        store_cv_results=store_cv_results,\n    )\n    self.class_weight = class_weight",
    "scikit-learn.sklearn.linear_model._stochastic_gradient.__init__": "def __init__(\n    self,\n    nu=0.5,\n    fit_intercept=True,\n    max_iter=1000,\n    tol=1e-3,\n    shuffle=True,\n    verbose=0,\n    random_state=None,\n    learning_rate=\"optimal\",\n    eta0=0.01,\n    power_t=0.5,\n    warm_start=False,\n    average=False,\n):\n    self.nu = nu\n    super().__init__(\n        loss=\"hinge\",\n        penalty=\"l2\",\n        l1_ratio=0,\n        fit_intercept=fit_intercept,\n        max_iter=max_iter,\n        tol=tol,\n        shuffle=shuffle,\n        verbose=verbose,\n        epsilon=DEFAULT_EPSILON,\n        random_state=random_state,\n        learning_rate=learning_rate,\n        eta0=eta0,\n        power_t=power_t,\n        early_stopping=False,\n        validation_fraction=0.1,\n        n_iter_no_change=5,\n        warm_start=warm_start,\n        average=average,\n    )",
    "scikit-learn.sklearn.linear_model._theil_sen.__init__": "def __init__(\n    self,\n    *,\n    fit_intercept=True,\n    max_subpopulation=1e4,\n    n_subsamples=None,\n    max_iter=300,\n    tol=1.0e-3,\n    random_state=None,\n    n_jobs=None,\n    verbose=False,\n):\n    self.fit_intercept = fit_intercept\n    self.max_subpopulation = max_subpopulation\n    self.n_subsamples = n_subsamples\n    self.max_iter = max_iter\n    self.tol = tol\n    self.random_state = random_state\n    self.n_jobs = n_jobs\n    self.verbose = verbose",
    "scikit-learn.sklearn.manifold._classical_mds.__init__": "def __init__(\n    self,\n    n_components=2,\n    *,\n    metric=\"euclidean\",\n    metric_params=None,\n):\n    self.n_components = n_components\n    self.metric = metric\n    self.metric_params = metric_params",
    "scikit-learn.sklearn.manifold._isomap.__init__": "def __init__(\n    self,\n    *,\n    n_neighbors=5,\n    radius=None,\n    n_components=2,\n    eigen_solver=\"auto\",\n    tol=0,\n    max_iter=None,\n    path_method=\"auto\",\n    neighbors_algorithm=\"auto\",\n    n_jobs=None,\n    metric=\"minkowski\",\n    p=2,\n    metric_params=None,\n):\n    self.n_neighbors = n_neighbors\n    self.radius = radius\n    self.n_components = n_components\n    self.eigen_solver = eigen_solver\n    self.tol = tol\n    self.max_iter = max_iter\n    self.path_method = path_method\n    self.neighbors_algorithm = neighbors_algorithm\n    self.n_jobs = n_jobs\n    self.metric = metric\n    self.p = p\n    self.metric_params = metric_params",
    "scikit-learn.sklearn.manifold._locally_linear.__init__": "def __init__(\n    self,\n    *,\n    n_neighbors=5,\n    n_components=2,\n    reg=1e-3,\n    eigen_solver=\"auto\",\n    tol=1e-6,\n    max_iter=100,\n    method=\"standard\",\n    hessian_tol=1e-4,\n    modified_tol=1e-12,\n    neighbors_algorithm=\"auto\",\n    random_state=None,\n    n_jobs=None,\n):\n    self.n_neighbors = n_neighbors\n    self.n_components = n_components\n    self.reg = reg\n    self.eigen_solver = eigen_solver\n    self.tol = tol\n    self.max_iter = max_iter\n    self.method = method\n    self.hessian_tol = hessian_tol\n    self.modified_tol = modified_tol\n    self.random_state = random_state\n    self.neighbors_algorithm = neighbors_algorithm\n    self.n_jobs = n_jobs",
    "scikit-learn.sklearn.manifold._mds.__init__": "def __init__(\n    self,\n    n_components=2,\n    *,\n    metric_mds=True,\n    n_init=\"warn\",\n    init=\"warn\",\n    max_iter=300,\n    verbose=0,\n    eps=1e-6,\n    n_jobs=None,\n    random_state=None,\n    dissimilarity=\"deprecated\",\n    metric=\"euclidean\",\n    metric_params=None,\n    normalized_stress=\"auto\",\n):\n    self.n_components = n_components\n    self.dissimilarity = dissimilarity\n    self.metric = metric\n    self.metric_params = metric_params\n    self.metric_mds = metric_mds\n    self.n_init = n_init\n    self.init = init\n    self.max_iter = max_iter\n    self.eps = eps\n    self.verbose = verbose\n    self.n_jobs = n_jobs\n    self.random_state = random_state\n    self.normalized_stress = normalized_stress",
    "scikit-learn.sklearn.manifold._spectral_embedding.__init__": "def __init__(\n    self,\n    n_components=2,\n    *,\n    affinity=\"nearest_neighbors\",\n    gamma=None,\n    random_state=None,\n    eigen_solver=None,\n    eigen_tol=\"auto\",\n    n_neighbors=None,\n    n_jobs=None,\n):\n    self.n_components = n_components\n    self.affinity = affinity\n    self.gamma = gamma\n    self.random_state = random_state\n    self.eigen_solver = eigen_solver\n    self.eigen_tol = eigen_tol\n    self.n_neighbors = n_neighbors\n    self.n_jobs = n_jobs",
    "scikit-learn.sklearn.manifold._t_sne.__init__": "def __init__(\n    self,\n    n_components=2,\n    *,\n    perplexity=30.0,\n    early_exaggeration=12.0,\n    learning_rate=\"auto\",\n    max_iter=1000,\n    n_iter_without_progress=300,\n    min_grad_norm=1e-7,\n    metric=\"euclidean\",\n    metric_params=None,\n    init=\"pca\",\n    verbose=0,\n    random_state=None,\n    method=\"barnes_hut\",\n    angle=0.5,\n    n_jobs=None,\n):\n    self.n_components = n_components\n    self.perplexity = perplexity\n    self.early_exaggeration = early_exaggeration\n    self.learning_rate = learning_rate\n    self.max_iter = max_iter\n    self.n_iter_without_progress = n_iter_without_progress\n    self.min_grad_norm = min_grad_norm\n    self.metric = metric\n    self.metric_params = metric_params\n    self.init = init\n    self.verbose = verbose\n    self.random_state = random_state\n    self.method = method\n    self.angle = angle\n    self.n_jobs = n_jobs",
    "scikit-learn.sklearn.mixture._bayesian_mixture.__init__": "def __init__(\n    self,\n    *,\n    n_components=1,\n    covariance_type=\"full\",\n    tol=1e-3,\n    reg_covar=1e-6,\n    max_iter=100,\n    n_init=1,\n    init_params=\"kmeans\",\n    weight_concentration_prior_type=\"dirichlet_process\",\n    weight_concentration_prior=None,\n    mean_precision_prior=None,\n    mean_prior=None,\n    degrees_of_freedom_prior=None,\n    covariance_prior=None,\n    random_state=None,\n    warm_start=False,\n    verbose=0,\n    verbose_interval=10,\n):\n    super().__init__(\n        n_components=n_components,\n        tol=tol,\n        reg_covar=reg_covar,\n        max_iter=max_iter,\n        n_init=n_init,\n        init_params=init_params,\n        random_state=random_state,\n        warm_start=warm_start,\n        verbose=verbose,\n        verbose_interval=verbose_interval,\n    )\n\n    self.covariance_type = covariance_type\n    self.weight_concentration_prior_type = weight_concentration_prior_type\n    self.weight_concentration_prior = weight_concentration_prior\n    self.mean_precision_prior = mean_precision_prior\n    self.mean_prior = mean_prior\n    self.degrees_of_freedom_prior = degrees_of_freedom_prior\n    self.covariance_prior = covariance_prior",
    "scikit-learn.sklearn.mixture._gaussian_mixture.__init__": "def __init__(\n    self,\n    n_components=1,\n    *,\n    covariance_type=\"full\",\n    tol=1e-3,\n    reg_covar=1e-6,\n    max_iter=100,\n    n_init=1,\n    init_params=\"kmeans\",\n    weights_init=None,\n    means_init=None,\n    precisions_init=None,\n    random_state=None,\n    warm_start=False,\n    verbose=0,\n    verbose_interval=10,\n):\n    super().__init__(\n        n_components=n_components,\n        tol=tol,\n        reg_covar=reg_covar,\n        max_iter=max_iter,\n        n_init=n_init,\n        init_params=init_params,\n        random_state=random_state,\n        warm_start=warm_start,\n        verbose=verbose,\n        verbose_interval=verbose_interval,\n    )\n\n    self.covariance_type = covariance_type\n    self.weights_init = weights_init\n    self.means_init = means_init\n    self.precisions_init = precisions_init",
    "scikit-learn.sklearn.model_selection._classification_threshold.__init__": "def __init__(\n    self,\n    estimator,\n    *,\n    scoring=\"balanced_accuracy\",\n    response_method=\"auto\",\n    thresholds=100,\n    cv=None,\n    refit=True,\n    n_jobs=None,\n    random_state=None,\n    store_cv_results=False,\n):\n    super().__init__(estimator=estimator, response_method=response_method)\n    self.scoring = scoring\n    self.thresholds = thresholds\n    self.cv = cv\n    self.refit = refit\n    self.n_jobs = n_jobs\n    self.random_state = random_state\n    self.store_cv_results = store_cv_results",
    "scikit-learn.sklearn.model_selection._search.__init__": "def __init__(\n    self,\n    estimator,\n    param_distributions,\n    *,\n    n_iter=10,\n    scoring=None,\n    n_jobs=None,\n    refit=True,\n    cv=None,\n    verbose=0,\n    pre_dispatch=\"2*n_jobs\",\n    random_state=None,\n    error_score=np.nan,\n    return_train_score=False,\n):\n    self.param_distributions = param_distributions\n    self.n_iter = n_iter\n    self.random_state = random_state\n    super().__init__(\n        estimator=estimator,\n        scoring=scoring,\n        n_jobs=n_jobs,\n        refit=refit,\n        cv=cv,\n        verbose=verbose,\n        pre_dispatch=pre_dispatch,\n        error_score=error_score,\n        return_train_score=return_train_score,\n    )",
    "scikit-learn.sklearn.model_selection._search_successive_halving.__init__": "def __init__(\n    self,\n    estimator,\n    param_grid,\n    *,\n    factor=3,\n    resource=\"n_samples\",\n    max_resources=\"auto\",\n    min_resources=\"exhaust\",\n    aggressive_elimination=False,\n    cv=5,\n    scoring=None,\n    refit=True,\n    error_score=np.nan,\n    return_train_score=True,\n    random_state=None,\n    n_jobs=None,\n    verbose=0,\n):\n    super().__init__(\n        estimator,\n        scoring=scoring,\n        n_jobs=n_jobs,\n        refit=refit,\n        verbose=verbose,\n        cv=cv,\n        random_state=random_state,\n        error_score=error_score,\n        return_train_score=return_train_score,\n        max_resources=max_resources,\n        resource=resource,\n        factor=factor,\n        min_resources=min_resources,\n        aggressive_elimination=aggressive_elimination,\n    )\n    self.param_grid = param_grid",
    "scikit-learn.sklearn.multiclass.__init__": "def __init__(self, estimator, *, n_jobs=None):\n    self.estimator = estimator\n    self.n_jobs = n_jobs",
    "scikit-learn.sklearn.multioutput.__init__": "def __init__(\n    self,\n    estimator=None,\n    *,\n    order=None,\n    cv=None,\n    random_state=None,\n    verbose=False,\n    base_estimator=\"deprecated\",\n):\n    self.estimator = estimator\n    self.base_estimator = base_estimator\n    self.order = order\n    self.cv = cv\n    self.random_state = random_state\n    self.verbose = verbose",
    "scikit-learn.sklearn.naive_bayes.__init__": "def __init__(\n    self, *, alpha=1.0, force_alpha=True, fit_prior=True, class_prior=None\n):\n    super().__init__(\n        alpha=alpha,\n        fit_prior=fit_prior,\n        class_prior=class_prior,\n        force_alpha=force_alpha,\n    )",
    "scikit-learn.sklearn.neighbors._classification.__init__": "def __init__(\n    self,\n    radius=1.0,\n    *,\n    weights=\"uniform\",\n    algorithm=\"auto\",\n    leaf_size=30,\n    p=2,\n    metric=\"minkowski\",\n    outlier_label=None,\n    metric_params=None,\n    n_jobs=None,\n):\n    super().__init__(\n        radius=radius,\n        algorithm=algorithm,\n        leaf_size=leaf_size,\n        metric=metric,\n        p=p,\n        metric_params=metric_params,\n        n_jobs=n_jobs,\n    )\n    self.weights = weights\n    self.outlier_label = outlier_label",
    "scikit-learn.sklearn.neighbors._graph.__init__": "def __init__(\n    self,\n    *,\n    mode=\"distance\",\n    radius=1.0,\n    algorithm=\"auto\",\n    leaf_size=30,\n    metric=\"minkowski\",\n    p=2,\n    metric_params=None,\n    n_jobs=None,\n):\n    super().__init__(\n        n_neighbors=None,\n        radius=radius,\n        algorithm=algorithm,\n        leaf_size=leaf_size,\n        metric=metric,\n        p=p,\n        metric_params=metric_params,\n        n_jobs=n_jobs,\n    )\n    self.mode = mode",
    "scikit-learn.sklearn.neighbors._kde.__init__": "def __init__(\n    self,\n    *,\n    bandwidth=1.0,\n    algorithm=\"auto\",\n    kernel=\"gaussian\",\n    metric=\"euclidean\",\n    atol=0,\n    rtol=0,\n    breadth_first=True,\n    leaf_size=40,\n    metric_params=None,\n):\n    self.algorithm = algorithm\n    self.bandwidth = bandwidth\n    self.kernel = kernel\n    self.metric = metric\n    self.atol = atol\n    self.rtol = rtol\n    self.breadth_first = breadth_first\n    self.leaf_size = leaf_size\n    self.metric_params = metric_params",
    "scikit-learn.sklearn.neighbors._lof.__init__": "def __init__(\n    self,\n    n_neighbors=20,\n    *,\n    algorithm=\"auto\",\n    leaf_size=30,\n    metric=\"minkowski\",\n    p=2,\n    metric_params=None,\n    contamination=\"auto\",\n    novelty=False,\n    n_jobs=None,\n):\n    super().__init__(\n        n_neighbors=n_neighbors,\n        algorithm=algorithm,\n        leaf_size=leaf_size,\n        metric=metric,\n        p=p,\n        metric_params=metric_params,\n        n_jobs=n_jobs,\n    )\n    self.contamination = contamination\n    self.novelty = novelty",
    "scikit-learn.sklearn.neighbors._nca.__init__": "def __init__(\n    self,\n    n_components=None,\n    *,\n    init=\"auto\",\n    warm_start=False,\n    max_iter=50,\n    tol=1e-5,\n    callback=None,\n    verbose=0,\n    random_state=None,\n):\n    self.n_components = n_components\n    self.init = init\n    self.warm_start = warm_start\n    self.max_iter = max_iter\n    self.tol = tol\n    self.callback = callback\n    self.verbose = verbose\n    self.random_state = random_state",
    "scikit-learn.sklearn.neighbors._nearest_centroid.__init__": "def __init__(\n    self,\n    metric=\"euclidean\",\n    *,\n    shrink_threshold=None,\n    priors=\"uniform\",\n):\n    self.metric = metric\n    self.shrink_threshold = shrink_threshold\n    self.priors = priors",
    "scikit-learn.sklearn.neighbors._regression.__init__": "def __init__(\n    self,\n    radius=1.0,\n    *,\n    weights=\"uniform\",\n    algorithm=\"auto\",\n    leaf_size=30,\n    p=2,\n    metric=\"minkowski\",\n    metric_params=None,\n    n_jobs=None,\n):\n    super().__init__(\n        radius=radius,\n        algorithm=algorithm,\n        leaf_size=leaf_size,\n        p=p,\n        metric=metric,\n        metric_params=metric_params,\n        n_jobs=n_jobs,\n    )\n    self.weights = weights",
    "scikit-learn.sklearn.neighbors._unsupervised.__init__": "def __init__(\n    self,\n    *,\n    n_neighbors=5,\n    radius=1.0,\n    algorithm=\"auto\",\n    leaf_size=30,\n    metric=\"minkowski\",\n    p=2,\n    metric_params=None,\n    n_jobs=None,\n):\n    super().__init__(\n        n_neighbors=n_neighbors,\n        radius=radius,\n        algorithm=algorithm,\n        leaf_size=leaf_size,\n        metric=metric,\n        p=p,\n        metric_params=metric_params,\n        n_jobs=n_jobs,\n    )",
    "scikit-learn.sklearn.neural_network._multilayer_perceptron.__init__": "def __init__(\n    self,\n    loss=\"squared_error\",\n    hidden_layer_sizes=(100,),\n    activation=\"relu\",\n    *,\n    solver=\"adam\",\n    alpha=0.0001,\n    batch_size=\"auto\",\n    learning_rate=\"constant\",\n    learning_rate_init=0.001,\n    power_t=0.5,\n    max_iter=200,\n    shuffle=True,\n    random_state=None,\n    tol=1e-4,\n    verbose=False,\n    warm_start=False,\n    momentum=0.9,\n    nesterovs_momentum=True,\n    early_stopping=False,\n    validation_fraction=0.1,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-8,\n    n_iter_no_change=10,\n    max_fun=15000,\n):\n    super().__init__(\n        hidden_layer_sizes=hidden_layer_sizes,\n        activation=activation,\n        solver=solver,\n        alpha=alpha,\n        batch_size=batch_size,\n        learning_rate=learning_rate,\n        learning_rate_init=learning_rate_init,\n        power_t=power_t,\n        max_iter=max_iter,\n        loss=loss,\n        shuffle=shuffle,\n        random_state=random_state,\n        tol=tol,\n        verbose=verbose,\n        warm_start=warm_start,\n        momentum=momentum,\n        nesterovs_momentum=nesterovs_momentum,\n        early_stopping=early_stopping,\n        validation_fraction=validation_fraction,\n        beta_1=beta_1,\n        beta_2=beta_2,\n        epsilon=epsilon,\n        n_iter_no_change=n_iter_no_change,\n        max_fun=max_fun,\n    )",
    "scikit-learn.sklearn.neural_network._rbm.__init__": "def __init__(\n    self,\n    n_components=256,\n    *,\n    learning_rate=0.1,\n    batch_size=10,\n    n_iter=10,\n    verbose=0,\n    random_state=None,\n):\n    self.n_components = n_components\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.n_iter = n_iter\n    self.verbose = verbose\n    self.random_state = random_state",
    "scikit-learn.sklearn.pipeline.__init__": "def __init__(self, steps, *, transform_input=None, memory=None, verbose=False):\n    self.steps = steps\n    self.transform_input = transform_input\n    self.memory = memory\n    self.verbose = verbose",
    "scikit-learn.sklearn.pipeline.get_params": "def get_params(self, deep=True):\n    \"\"\"Get parameters for this estimator.\n\n    Returns the parameters given in the constructor as well as the\n    estimators contained within the `steps` of the `Pipeline`.\n\n    Parameters\n    ----------\n    deep : bool, default=True\n        If True, will return the parameters for this estimator and\n        contained subobjects that are estimators.\n\n    Returns\n    -------\n    params : mapping of string to any\n        Parameter names mapped to their values.\n    \"\"\"\n    return self._get_params(\"steps\", deep=deep)",
    "scikit-learn.sklearn.preprocessing._data.__init__": "def __init__(self, *, copy=True, with_mean=True, with_std=True):\n    self.with_mean = with_mean\n    self.with_std = with_std\n    self.copy = copy",
    "scikit-learn.sklearn.preprocessing._discretization.__init__": "def __init__(\n    self,\n    n_bins=5,\n    *,\n    encode=\"onehot\",\n    strategy=\"quantile\",\n    quantile_method=\"warn\",\n    dtype=None,\n    subsample=200_000,\n    random_state=None,\n):\n    self.n_bins = n_bins\n    self.encode = encode\n    self.strategy = strategy\n    self.quantile_method = quantile_method\n    self.dtype = dtype\n    self.subsample = subsample\n    self.random_state = random_state",
    "scikit-learn.sklearn.preprocessing._encoders.__init__": "def __init__(\n    self,\n    *,\n    categories=\"auto\",\n    drop=None,\n    sparse_output=True,\n    dtype=np.float64,\n    handle_unknown=\"error\",\n    min_frequency=None,\n    max_categories=None,\n    feature_name_combiner=\"concat\",\n):\n    self.categories = categories\n    self.sparse_output = sparse_output\n    self.dtype = dtype\n    self.handle_unknown = handle_unknown\n    self.drop = drop\n    self.min_frequency = min_frequency\n    self.max_categories = max_categories\n    self.feature_name_combiner = feature_name_combiner",
    "scikit-learn.sklearn.preprocessing._function_transformer.__init__": "def __init__(\n    self,\n    func=None,\n    inverse_func=None,\n    *,\n    validate=False,\n    accept_sparse=False,\n    check_inverse=True,\n    feature_names_out=None,\n    kw_args=None,\n    inv_kw_args=None,\n):\n    self.func = func\n    self.inverse_func = inverse_func\n    self.validate = validate\n    self.accept_sparse = accept_sparse\n    self.check_inverse = check_inverse\n    self.feature_names_out = feature_names_out\n    self.kw_args = kw_args\n    self.inv_kw_args = inv_kw_args",
    "scikit-learn.sklearn.preprocessing._label.__init__": "def __init__(self, *, classes=None, sparse_output=False):\n    self.classes = classes\n    self.sparse_output = sparse_output",
    "scikit-learn.sklearn.preprocessing._polynomial.__init__": "def __init__(\n    self,\n    n_knots=5,\n    degree=3,\n    *,\n    knots=\"uniform\",\n    extrapolation=\"constant\",\n    include_bias=True,\n    order=\"C\",\n    handle_missing=\"error\",\n    sparse_output=False,\n):\n    self.n_knots = n_knots\n    self.degree = degree\n    self.knots = knots\n    self.extrapolation = extrapolation\n    self.include_bias = include_bias\n    self.order = order\n    self.handle_missing = handle_missing\n    self.sparse_output = sparse_output",
    "scikit-learn.sklearn.preprocessing._target_encoder.__init__": "def __init__(\n    self,\n    categories=\"auto\",\n    target_type=\"auto\",\n    smooth=\"auto\",\n    cv=5,\n    shuffle=True,\n    random_state=None,\n):\n    self.categories = categories\n    self.smooth = smooth\n    self.target_type = target_type\n    self.cv = cv\n    self.shuffle = shuffle\n    self.random_state = random_state",
    "scikit-learn.sklearn.random_projection.__init__": "def __init__(\n    self,\n    n_components=\"auto\",\n    *,\n    density=\"auto\",\n    eps=0.1,\n    dense_output=False,\n    compute_inverse_components=False,\n    random_state=None,\n):\n    super().__init__(\n        n_components=n_components,\n        eps=eps,\n        compute_inverse_components=compute_inverse_components,\n        random_state=random_state,\n    )\n\n    self.dense_output = dense_output\n    self.density = density",
    "scikit-learn.sklearn.semi_supervised._label_propagation.__init__": "def __init__(\n    self,\n    kernel=\"rbf\",\n    *,\n    gamma=20,\n    n_neighbors=7,\n    alpha=0.2,\n    max_iter=30,\n    tol=1e-3,\n    n_jobs=None,\n):\n    # this one has different base parameters\n    super().__init__(\n        kernel=kernel,\n        gamma=gamma,\n        n_neighbors=n_neighbors,\n        alpha=alpha,\n        max_iter=max_iter,\n        tol=tol,\n        n_jobs=n_jobs,\n    )",
    "scikit-learn.sklearn.semi_supervised._self_training.__init__": "def __init__(\n    self,\n    estimator=None,\n    threshold=0.75,\n    criterion=\"threshold\",\n    k_best=10,\n    max_iter=10,\n    verbose=False,\n):\n    self.estimator = estimator\n    self.threshold = threshold\n    self.criterion = criterion\n    self.k_best = k_best\n    self.max_iter = max_iter\n    self.verbose = verbose",
    "scikit-learn.sklearn.svm._classes.__init__": "def __init__(\n    self,\n    *,\n    C=1.0,\n    kernel=\"rbf\",\n    degree=3,\n    gamma=\"scale\",\n    coef0=0.0,\n    shrinking=True,\n    probability=False,\n    tol=1e-3,\n    cache_size=200,\n    class_weight=None,\n    verbose=False,\n    max_iter=-1,\n    decision_function_shape=\"ovr\",\n    break_ties=False,\n    random_state=None,\n):\n    super().__init__(\n        kernel=kernel,\n        degree=degree,\n        gamma=gamma,\n        coef0=coef0,\n        tol=tol,\n        C=C,\n        nu=0.0,\n        shrinking=shrinking,\n        probability=probability,\n        cache_size=cache_size,\n        class_weight=class_weight,\n        verbose=verbose,\n        max_iter=max_iter,\n        decision_function_shape=decision_function_shape,\n        break_ties=break_ties,\n        random_state=random_state,\n    )",
    "scikit-learn.sklearn.tree._classes.__init__": "def __init__(\n    self,\n    *,\n    criterion=\"gini\",\n    splitter=\"best\",\n    max_depth=None,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_features=None,\n    random_state=None,\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    class_weight=None,\n    ccp_alpha=0.0,\n    monotonic_cst=None,\n):\n    super().__init__(\n        criterion=criterion,\n        splitter=splitter,\n        max_depth=max_depth,\n        min_samples_split=min_samples_split,\n        min_samples_leaf=min_samples_leaf,\n        min_weight_fraction_leaf=min_weight_fraction_leaf,\n        max_features=max_features,\n        max_leaf_nodes=max_leaf_nodes,\n        class_weight=class_weight,\n        random_state=random_state,\n        min_impurity_decrease=min_impurity_decrease,\n        monotonic_cst=monotonic_cst,\n        ccp_alpha=ccp_alpha,\n    )",
    "scikit-learn.sklearn.utils._mocking.__init__": "def __init__(\n    self,\n    *,\n    check_y=None,\n    check_y_params=None,\n    check_X=None,\n    check_X_params=None,\n    methods_to_check=\"all\",\n    foo_param=0,\n    expected_sample_weight=None,\n    expected_fit_params=None,\n    random_state=None,\n):\n    self.check_y = check_y\n    self.check_y_params = check_y_params\n    self.check_X = check_X\n    self.check_X_params = check_X_params\n    self.methods_to_check = methods_to_check\n    self.foo_param = foo_param\n    self.expected_sample_weight = expected_sample_weight\n    self.expected_fit_params = expected_fit_params\n    self.random_state = random_state",
    "scikit-learn.sklearn.utils.deprecation.wrapped": "def wrapped(cls, *args, **kwargs):\n    warnings.warn(msg, category=FutureWarning)\n    if new is object.__new__:\n        return object.__new__(cls)\n\n    return new(cls, *args, **kwargs)"
}