{
    "scikit-learn.sklearn.decomposition._lda._check_non_neg_array": "def _check_non_neg_array(self, X, reset_n_features, whom):\n    \"\"\"check X format\n\n    check X format and make sure no negative value in X.\n\n    Parameters\n    ----------\n    X :  array-like or sparse matrix\n\n    \"\"\"\n    dtype = [np.float64, np.float32] if reset_n_features else self.components_.dtype\n\n    X = validate_data(\n        self,\n        X,\n        reset=reset_n_features,\n        accept_sparse=\"csr\",\n        dtype=dtype,\n    )\n    check_non_negative(X, whom)\n\n    return X",
    "scikit-learn.sklearn.decomposition._lda._unnormalized_transform": "def _unnormalized_transform(self, X):\n    \"\"\"Transform data X according to fitted model.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Document word matrix.\n\n    Returns\n    -------\n    doc_topic_distr : ndarray of shape (n_samples, n_components)\n        Document topic distribution for X.\n    \"\"\"\n    doc_topic_distr, _ = self._e_step(X, cal_sstats=False, random_init=False)\n\n    return doc_topic_distr",
    "scikit-learn.sklearn.decomposition._lda._approx_bound": "def _approx_bound(self, X, doc_topic_distr, sub_sampling):\n    \"\"\"Estimate the variational bound.\n\n    Estimate the variational bound over \"all documents\" using only the\n    documents passed in as X. Since log-likelihood of each word cannot\n    be computed directly, we use this bound to estimate it.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Document word matrix.\n\n    doc_topic_distr : ndarray of shape (n_samples, n_components)\n        Document topic distribution. In the literature, this is called\n        gamma.\n\n    sub_sampling : bool, default=False\n        Compensate for subsampling of documents.\n        It is used in calculate bound in online learning.\n\n    Returns\n    -------\n    score : float\n\n    \"\"\"\n\n    def _loglikelihood(prior, distr, dirichlet_distr, size):\n        # calculate log-likelihood\n        score = np.sum((prior - distr) * dirichlet_distr)\n        score += np.sum(gammaln(distr) - gammaln(prior))\n        score += np.sum(gammaln(prior * size) - gammaln(np.sum(distr, 1)))\n        return score\n\n    is_sparse_x = sp.issparse(X)\n    n_samples, n_components = doc_topic_distr.shape\n    n_features = self.components_.shape[1]\n    score = 0\n\n    dirichlet_doc_topic = _dirichlet_expectation_2d(doc_topic_distr)\n    dirichlet_component_ = _dirichlet_expectation_2d(self.components_)\n    doc_topic_prior = self.doc_topic_prior_\n    topic_word_prior = self.topic_word_prior_\n\n    if is_sparse_x:\n        X_data = X.data\n        X_indices = X.indices\n        X_indptr = X.indptr\n\n    # E[log p(docs | theta, beta)]\n    for idx_d in range(0, n_samples):\n        if is_sparse_x:\n            ids = X_indices[X_indptr[idx_d] : X_indptr[idx_d + 1]]\n            cnts = X_data[X_indptr[idx_d] : X_indptr[idx_d + 1]]\n        else:\n            ids = np.nonzero(X[idx_d, :])[0]\n            cnts = X[idx_d, ids]\n        temp = (\n            dirichlet_doc_topic[idx_d, :, np.newaxis] + dirichlet_component_[:, ids]\n        )\n        norm_phi = logsumexp(temp, axis=0)\n        score += np.dot(cnts, norm_phi)\n\n    # compute E[log p(theta | alpha) - log q(theta | gamma)]\n    score += _loglikelihood(\n        doc_topic_prior, doc_topic_distr, dirichlet_doc_topic, self.n_components\n    )\n\n    # Compensate for the subsampling of the population of documents\n    if sub_sampling:\n        doc_ratio = float(self.total_samples) / n_samples\n        score *= doc_ratio\n\n    # E[log p(beta | eta) - log q (beta | lambda)]\n    score += _loglikelihood(\n        topic_word_prior, self.components_, dirichlet_component_, n_features\n    )\n\n    return score",
    "scikit-learn.sklearn.utils.validation.check_is_fitted": "def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):\n    \"\"\"Perform is_fitted validation for estimator.\n\n    Checks if the estimator is fitted by verifying the presence of\n    fitted attributes (ending with a trailing underscore) and otherwise\n    raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to\n    specify if the estimator is fitted or not. See\n    :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\n    for an example on how to use the API.\n\n    If no `attributes` are passed, this function will pass if an estimator is stateless.\n    An estimator can indicate it's stateless by setting the `requires_fit` tag. See\n    :ref:`estimator_tags` for more information. Note that the `requires_fit` tag\n    is ignored if `attributes` are passed.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Raises\n    ------\n    TypeError\n        If the estimator is a class or not an estimator instance\n\n    NotFittedError\n        If the attributes are not found.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.utils.validation import check_is_fitted\n    >>> from sklearn.exceptions import NotFittedError\n    >>> lr = LogisticRegression()\n    >>> try:\n    ...     check_is_fitted(lr)\n    ... except NotFittedError as exc:\n    ...     print(f\"Model is not fitted yet.\")\n    Model is not fitted yet.\n    >>> lr.fit([[1, 2], [1, 3]], [1, 0])\n    LogisticRegression()\n    >>> check_is_fitted(lr)\n    \"\"\"\n    if isclass(estimator):\n        raise TypeError(\"{} is a class, not an instance.\".format(estimator))\n    if msg is None:\n        msg = (\n            \"This %(name)s instance is not fitted yet. Call 'fit' with \"\n            \"appropriate arguments before using this estimator.\"\n        )\n\n    if not hasattr(estimator, \"fit\"):\n        raise TypeError(\"%s is not an estimator instance.\" % (estimator))\n\n    tags = get_tags(estimator)\n\n    if not tags.requires_fit and attributes is None:\n        return\n\n    if not _is_fitted(estimator, attributes, all_or_any):\n        raise NotFittedError(msg % {\"name\": type(estimator).__name__})"
}