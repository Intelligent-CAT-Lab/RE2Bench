{
    "scikit-learn.sklearn.base.is_classifier": "def is_classifier(estimator):\n    \"\"\"Return True if the given estimator is (probably) a classifier.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator object to test.\n\n    Returns\n    -------\n    out : bool\n        True if estimator is a classifier and False otherwise.\n\n    Examples\n    --------\n    >>> from sklearn.base import is_classifier\n    >>> from sklearn.cluster import KMeans\n    >>> from sklearn.svm import SVC, SVR\n    >>> classifier = SVC()\n    >>> regressor = SVR()\n    >>> kmeans = KMeans()\n    >>> is_classifier(classifier)\n    True\n    >>> is_classifier(regressor)\n    False\n    >>> is_classifier(kmeans)\n    False\n    \"\"\"\n    return get_tags(estimator).estimator_type == \"classifier\"",
    "scikit-learn.sklearn.base.wrapper": "@functools.wraps(fit_method)\ndef wrapper(estimator, *args, **kwargs):\n    global_skip_validation = get_config()[\"skip_parameter_validation\"]\n\n    # we don't want to validate again for each call to partial_fit\n    partial_fit_and_fitted = (\n        fit_method.__name__ == \"partial_fit\" and _is_fitted(estimator)\n    )\n\n    if not global_skip_validation and not partial_fit_and_fitted:\n        estimator._validate_params()\n\n    with config_context(\n        skip_parameter_validation=(\n            prefer_skip_nested_validation or global_skip_validation\n        )\n    ):\n        return fit_method(estimator, *args, **kwargs)",
    "scikit-learn.sklearn.linear_model._ridge.__init__": "def __init__(\n    self,\n    alphas=(0.1, 1.0, 10.0),\n    *,\n    fit_intercept=True,\n    scoring=None,\n    copy_X=True,\n    gcv_mode=None,\n    store_cv_results=False,\n    is_clf=False,\n    alpha_per_target=False,\n):\n    self.alphas = alphas\n    self.fit_intercept = fit_intercept\n    self.scoring = scoring\n    self.copy_X = copy_X\n    self.gcv_mode = gcv_mode\n    self.store_cv_results = store_cv_results\n    self.is_clf = is_clf\n    self.alpha_per_target = alpha_per_target",
    "scikit-learn.sklearn.linear_model._ridge.fit": "def fit(self, X, y, sample_weight=None, score_params=None):\n    \"\"\"Fit Ridge regression model with gcv.\n\n    Parameters\n    ----------\n    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        Training data. Will be cast to float64 if necessary.\n\n    y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n        Target values. Will be cast to float64 if necessary.\n\n    sample_weight : float or ndarray of shape (n_samples,), default=None\n        Individual weights for each sample. If given a float, every sample\n        will have the same weight. Note that the scale of `sample_weight`\n        has an impact on the loss; i.e. multiplying all weights by `k`\n        is equivalent to setting `alpha / k`.\n\n    score_params : dict, default=None\n        Parameters to be passed to the underlying scorer.\n\n        .. versionadded:: 1.5\n            See :ref:`Metadata Routing User Guide <metadata_routing>` for\n            more details.\n\n    Returns\n    -------\n    self : object\n    \"\"\"\n    xp, is_array_api, device_ = get_namespace_and_device(X)\n    y, sample_weight = move_to(y, sample_weight, xp=xp, device=device_)\n    if is_array_api or hasattr(getattr(X, \"dtype\", None), \"kind\"):\n        original_dtype = X.dtype\n    else:\n        # for X that does not have a simple dtype (e.g. pandas dataframe)\n        # the attributes will be stored in the dtype chosen by\n        # `validate_data``, i.e. np.float64\n        original_dtype = None\n    # Using float32 can be numerically unstable for this estimator. So if\n    # the array API namespace and device allow, convert the input values\n    # to float64 whenever possible before converting the results back to\n    # float32.\n    dtype = _max_precision_float_dtype(xp, device=device_)\n    X, y = validate_data(\n        self,\n        X,\n        y,\n        accept_sparse=[\"csr\", \"csc\", \"coo\"],\n        dtype=dtype,\n        multi_output=True,\n        y_numeric=True,\n    )\n\n    # alpha_per_target cannot be used in classifier mode. All subclasses\n    # of _RidgeGCV that are classifiers keep alpha_per_target at its\n    # default value: False, so the condition below should never happen.\n    assert not (self.is_clf and self.alpha_per_target)\n\n    if sample_weight is not None:\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n    self.alphas = np.asarray(self.alphas)\n\n    unscaled_y = y\n    X, y, X_offset, y_offset, X_scale, sqrt_sw = _preprocess_data(\n        X,\n        y,\n        fit_intercept=self.fit_intercept,\n        copy=self.copy_X,\n        sample_weight=sample_weight,\n        rescale_with_sw=True,\n    )\n\n    gcv_mode = _check_gcv_mode(X, self.gcv_mode)\n\n    if gcv_mode == \"eigen\":\n        decompose = self._eigen_decompose_gram\n        solve = self._solve_eigen_gram\n    elif gcv_mode == \"svd\":\n        if sparse.issparse(X):\n            decompose = self._eigen_decompose_covariance\n            solve = self._solve_eigen_covariance\n        else:\n            decompose = self._svd_decompose_design_matrix\n            solve = self._solve_svd_design_matrix\n\n    n_samples = X.shape[0]\n\n    if sqrt_sw is None:\n        sqrt_sw = xp.ones(n_samples, dtype=X.dtype, device=device_)\n\n    X_mean, *decomposition = decompose(X, y, sqrt_sw)\n\n    n_y = 1 if len(y.shape) == 1 else y.shape[1]\n    if (\n        isinstance(self.alphas, numbers.Number)\n        or getattr(self.alphas, \"ndim\", None) == 0\n    ):\n        alphas = [float(self.alphas)]\n    else:\n        alphas = list(map(float, self.alphas))\n    n_alphas = len(alphas)\n\n    if self.store_cv_results:\n        self.cv_results_ = xp.empty(\n            (n_samples * n_y, n_alphas), dtype=original_dtype, device=device_\n        )\n\n    best_coef, best_score, best_alpha = None, None, None\n\n    for i, alpha in enumerate(alphas):\n        G_inverse_diag, c = solve(float(alpha), y, sqrt_sw, X_mean, *decomposition)\n        if self.scoring is None:\n            squared_errors = (c / G_inverse_diag) ** 2\n            alpha_score = self._score_without_scorer(squared_errors=squared_errors)\n            if self.store_cv_results:\n                self.cv_results_[:, i] = _ravel(squared_errors)\n        else:\n            predictions = y - (c / G_inverse_diag)\n            # Rescale predictions back to original scale\n            if sample_weight is not None:  # avoid the unnecessary division by ones\n                if predictions.ndim > 1:\n                    predictions /= sqrt_sw[:, None]\n                else:\n                    predictions /= sqrt_sw\n            predictions += y_offset\n\n            if self.store_cv_results:\n                self.cv_results_[:, i] = _ravel(predictions)\n\n            score_params = score_params or {}\n            alpha_score = self._score(\n                predictions=predictions,\n                y=unscaled_y,\n                n_y=n_y,\n                scorer=self.scoring,\n                score_params=score_params,\n            )\n\n        # Keep track of the best model\n        if best_score is None:\n            # initialize\n            if self.alpha_per_target and n_y > 1:\n                best_coef = c\n                best_score = xp.reshape(alpha_score, shape=(-1,))\n                best_alpha = xp.full(n_y, alpha, device=device_)\n            else:\n                best_coef = c\n                best_score = alpha_score\n                best_alpha = alpha\n        else:\n            # update\n            if self.alpha_per_target and n_y > 1:\n                to_update = alpha_score > best_score\n                best_coef.T[to_update] = c.T[to_update]\n                best_score[to_update] = alpha_score[to_update]\n                best_alpha[to_update] = alpha\n            elif alpha_score > best_score:\n                best_coef, best_score, best_alpha = c, alpha_score, alpha\n\n    self.alpha_ = best_alpha\n    self.best_score_ = best_score\n    self.dual_coef_ = best_coef\n    # avoid torch warning about x.T for x with ndim != 2\n    if self.dual_coef_.ndim > 1:\n        dual_T = self.dual_coef_.T\n    else:\n        dual_T = self.dual_coef_\n    self.coef_ = dual_T @ X\n    if y.ndim == 1 or y.shape[1] == 1:\n        self.coef_ = _ravel(self.coef_)\n\n    if sparse.issparse(X):\n        X_offset = X_mean * X_scale\n    else:\n        X_offset += X_mean * X_scale\n    self._set_intercept(X_offset, y_offset, X_scale)\n\n    if self.store_cv_results:\n        if len(y.shape) == 1:\n            cv_results_shape = n_samples, n_alphas\n        else:\n            cv_results_shape = n_samples, n_y, n_alphas\n        self.cv_results_ = xp.reshape(self.cv_results_, shape=cv_results_shape)\n\n    if original_dtype is not None:\n        if type(self.intercept_) is not float:\n            self.intercept_ = xp.astype(self.intercept_, original_dtype, copy=False)\n        self.dual_coef_ = xp.astype(self.dual_coef_, original_dtype, copy=False)\n        self.coef_ = xp.astype(self.coef_, original_dtype, copy=False)\n    return self",
    "scikit-learn.sklearn.linear_model._ridge._get_scorer": "def _get_scorer(self):\n    \"\"\"Make sure the scorer is weighted if necessary.\n\n    This uses `self._get_scorer_instance()` implemented in child objects to get the\n    raw scorer instance of the estimator, which will be ignored if `self.scoring` is\n    not None.\n    \"\"\"\n    if _routing_enabled() and self.scoring is None:\n        # This estimator passes an array of 1s as sample_weight even if\n        # sample_weight is not provided by the user. Therefore we need to\n        # always request it. But we don't set it if it's passed explicitly\n        # by the user.\n        return self._get_scorer_instance().set_score_request(sample_weight=True)\n\n    return check_scoring(estimator=self, scoring=self.scoring, allow_none=True)",
    "scikit-learn.sklearn.model_selection._search.__init__": "def __init__(\n    self,\n    estimator,\n    param_grid,\n    *,\n    scoring=None,\n    n_jobs=None,\n    refit=True,\n    cv=None,\n    verbose=0,\n    pre_dispatch=\"2*n_jobs\",\n    error_score=np.nan,\n    return_train_score=False,\n):\n    super().__init__(\n        estimator=estimator,\n        scoring=scoring,\n        n_jobs=n_jobs,\n        refit=refit,\n        cv=cv,\n        verbose=verbose,\n        pre_dispatch=pre_dispatch,\n        error_score=error_score,\n        return_train_score=return_train_score,\n    )\n    self.param_grid = param_grid",
    "scikit-learn.sklearn.utils._bunch.__init__": "def __init__(self, **kwargs):\n    super().__init__(kwargs)\n\n    # Map from deprecated key to warning message\n    self.__dict__[\"_deprecated_key_to_warnings\"] = {}",
    "scikit-learn.sklearn.utils._bunch.__getattr__": "def __getattr__(self, key):\n    try:\n        return self[key]\n    except KeyError:\n        raise AttributeError(key)",
    "scikit-learn.sklearn.utils._metadata_requests.__get__": "def __get__(self, instance, owner):\n    # we would want to have a method which accepts only the expected args\n    def func(*args, **kw):\n        \"\"\"Updates the `_metadata_request` attribute of the consumer (`instance`)\n        for the parameters provided as `**kw`.\n\n        This docstring is overwritten below.\n        See REQUESTER_DOC for expected functionality.\n        \"\"\"\n        if not _routing_enabled():\n            raise RuntimeError(\n                \"This method is only available when metadata routing is enabled.\"\n                \" You can enable it using\"\n                \" sklearn.set_config(enable_metadata_routing=True).\"\n            )\n\n        if self.validate_keys and (set(kw) - set(self.keys)):\n            raise TypeError(\n                f\"Unexpected args: {set(kw) - set(self.keys)} in {self.name}. \"\n                f\"Accepted arguments are: {set(self.keys)}\"\n            )\n\n        # This makes it possible to use the decorated method as an unbound method,\n        # for instance when monkeypatching.\n        # https://github.com/scikit-learn/scikit-learn/issues/28632\n        if instance is None:\n            _instance = args[0]\n            args = args[1:]\n        else:\n            _instance = instance\n\n        # Replicating python's behavior when positional args are given other than\n        # `self`, and `self` is only allowed if this method is unbound.\n        if args:\n            raise TypeError(\n                f\"set_{self.name}_request() takes 0 positional argument but\"\n                f\" {len(args)} were given\"\n            )\n\n        requests = _instance._get_metadata_request()\n        method_metadata_request = getattr(requests, self.name)\n\n        for prop, alias in kw.items():\n            if alias is not UNCHANGED:\n                method_metadata_request.add_request(param=prop, alias=alias)\n        _instance._metadata_request = requests\n\n        return _instance\n\n    # Now we set the relevant attributes of the function so that it seems\n    # like a normal method to the end user, with known expected arguments.\n    func.__name__ = f\"set_{self.name}_request\"\n    params = [\n        inspect.Parameter(\n            name=\"self\",\n            kind=inspect.Parameter.POSITIONAL_OR_KEYWORD,\n            annotation=owner,\n        )\n    ]\n    params.extend(\n        [\n            inspect.Parameter(\n                k,\n                inspect.Parameter.KEYWORD_ONLY,\n                default=UNCHANGED,\n                annotation=Optional[Union[bool, None, str]],\n            )\n            for k in self.keys\n        ]\n    )\n    func.__signature__ = inspect.Signature(\n        params,\n        return_annotation=owner,\n    )\n    doc = REQUESTER_DOC.format(method=self.name)\n    for metadata in self.keys:\n        doc += REQUESTER_DOC_PARAM.format(metadata=metadata, method=self.name)\n    doc += REQUESTER_DOC_RETURN\n    func.__doc__ = doc\n    return func",
    "scikit-learn.sklearn.utils._metadata_requests.func": "def func(*args, **kw):\n    \"\"\"Updates the `_metadata_request` attribute of the consumer (`instance`)\n    for the parameters provided as `**kw`.\n\n    This docstring is overwritten below.\n    See REQUESTER_DOC for expected functionality.\n    \"\"\"\n    if not _routing_enabled():\n        raise RuntimeError(\n            \"This method is only available when metadata routing is enabled.\"\n            \" You can enable it using\"\n            \" sklearn.set_config(enable_metadata_routing=True).\"\n        )\n\n    if self.validate_keys and (set(kw) - set(self.keys)):\n        raise TypeError(\n            f\"Unexpected args: {set(kw) - set(self.keys)} in {self.name}. \"\n            f\"Accepted arguments are: {set(self.keys)}\"\n        )\n\n    # This makes it possible to use the decorated method as an unbound method,\n    # for instance when monkeypatching.\n    # https://github.com/scikit-learn/scikit-learn/issues/28632\n    if instance is None:\n        _instance = args[0]\n        args = args[1:]\n    else:\n        _instance = instance\n\n    # Replicating python's behavior when positional args are given other than\n    # `self`, and `self` is only allowed if this method is unbound.\n    if args:\n        raise TypeError(\n            f\"set_{self.name}_request() takes 0 positional argument but\"\n            f\" {len(args)} were given\"\n        )\n\n    requests = _instance._get_metadata_request()\n    method_metadata_request = getattr(requests, self.name)\n\n    for prop, alias in kw.items():\n        if alias is not UNCHANGED:\n            method_metadata_request.add_request(param=prop, alias=alias)\n    _instance._metadata_request = requests\n\n    return _instance",
    "scikit-learn.sklearn.utils._metadata_requests.process_routing": "def process_routing(_obj, _method, /, **kwargs):\n    \"\"\"Validate and route metadata.\n\n    This function is used inside a :term:`router`'s method, e.g. :term:`fit`,\n    to validate the metadata and handle the routing.\n\n    Assuming this signature of a router's fit method:\n    ``fit(self, X, y, sample_weight=None, **fit_params)``,\n    a call to this function would be:\n    ``process_routing(self, \"fit\", sample_weight=sample_weight, **fit_params)``.\n\n    Note that if routing is not enabled and ``kwargs`` is empty, then it\n    returns an empty routing where ``process_routing(...).ANYTHING.ANY_METHOD``\n    is always an empty dictionary.\n\n    .. versionadded:: 1.3\n\n    Parameters\n    ----------\n    _obj : object\n        An object implementing ``get_metadata_routing``. Typically a\n        :term:`meta-estimator`.\n\n    _method : str\n        The name of the router's method in which this function is called.\n\n    **kwargs : dict\n        Metadata to be routed.\n\n    Returns\n    -------\n    routed_params : Bunch\n        A :class:`~utils.Bunch` of the form ``{\"object_name\": {\"method_name\":\n        {metadata: value}}}`` which can be used to pass the required metadata to\n        A :class:`~sklearn.utils.Bunch` of the form ``{\"object_name\": {\"method_name\":\n        {metadata: value}}}`` which can be used to pass the required metadata to\n        corresponding methods or corresponding child objects. The object names\n        are those defined in `obj.get_metadata_routing()`.\n    \"\"\"\n    if not kwargs:\n        # If routing is not enabled and kwargs are empty, then we don't have to\n        # try doing any routing, we can simply return a structure which returns\n        # an empty dict on routed_params.ANYTHING.ANY_METHOD.\n        class EmptyRequest:\n            def get(self, name, default=None):\n                return Bunch(**{method: dict() for method in METHODS})\n\n            def __getitem__(self, name):\n                return Bunch(**{method: dict() for method in METHODS})\n\n            def __getattr__(self, name):\n                return Bunch(**{method: dict() for method in METHODS})\n\n        return EmptyRequest()\n\n    if not (hasattr(_obj, \"get_metadata_routing\") or isinstance(_obj, MetadataRouter)):\n        raise AttributeError(\n            f\"The given object ({_routing_repr(_obj)}) needs to either\"\n            \" implement the routing method `get_metadata_routing` or be a\"\n            \" `MetadataRouter` instance.\"\n        )\n    if _method not in METHODS:\n        raise TypeError(\n            f\"Can only route and process input on these methods: {METHODS}, \"\n            f\"while the passed method is: {_method}.\"\n        )\n\n    request_routing = get_routing_for_object(_obj)\n    request_routing.validate_metadata(params=kwargs, method=_method)\n    routed_params = request_routing.route_params(params=kwargs, caller=_method)\n\n    return routed_params",
    "scikit-learn.sklearn.utils._metadata_requests._routing_enabled": "def _routing_enabled():\n    \"\"\"Return whether metadata routing is enabled.\n\n    .. versionadded:: 1.3\n\n    Returns\n    -------\n    enabled : bool\n        Whether metadata routing is enabled. If the config is not set, it\n        defaults to False.\n    \"\"\"\n    return get_config().get(\"enable_metadata_routing\", False)",
    "scikit-learn.sklearn.utils._metadata_requests._raise_for_params": "def _raise_for_params(params, owner, method, allow=None):\n    \"\"\"Raise an error if metadata routing is not enabled and params are passed.\n\n    .. versionadded:: 1.4\n\n    Parameters\n    ----------\n    params : dict\n        The metadata passed to a method.\n\n    owner : object\n        The object to which the method belongs.\n\n    method : str\n        The name of the method, e.g. \"fit\".\n\n    allow : list of str, default=None\n        A list of parameters which are allowed to be passed even if metadata\n        routing is not enabled.\n\n    Raises\n    ------\n    ValueError\n        If metadata routing is not enabled and params are passed.\n    \"\"\"\n    caller = f\"{_routing_repr(owner)}.{method}\" if method else _routing_repr(owner)\n\n    allow = allow if allow is not None else {}\n\n    if not _routing_enabled() and (params.keys() - allow):\n        raise ValueError(\n            f\"Passing extra keyword arguments to {caller} is only supported if\"\n            \" enable_metadata_routing=True, which you can set using\"\n            \" `sklearn.set_config`. See the User Guide\"\n            \" <https://scikit-learn.org/stable/metadata_routing.html> for more\"\n            f\" details. Extra parameters passed are: {set(params)}\"\n        )",
    "scikit-learn.sklearn.utils.validation.check_scalar": "def check_scalar(\n    x,\n    name,\n    target_type,\n    *,\n    min_val=None,\n    max_val=None,\n    include_boundaries=\"both\",\n):\n    \"\"\"Validate scalar parameters type and value.\n\n    Parameters\n    ----------\n    x : object\n        The scalar parameter to validate.\n\n    name : str\n        The name of the parameter to be printed in error messages.\n\n    target_type : type or tuple\n        Acceptable data types for the parameter.\n\n    min_val : float or int, default=None\n        The minimum valid value the parameter can take. If None (default) it\n        is implied that the parameter does not have a lower bound.\n\n    max_val : float or int, default=None\n        The maximum valid value the parameter can take. If None (default) it\n        is implied that the parameter does not have an upper bound.\n\n    include_boundaries : {\"left\", \"right\", \"both\", \"neither\"}, default=\"both\"\n        Whether the interval defined by `min_val` and `max_val` should include\n        the boundaries. Possible choices are:\n\n        - `\"left\"`: only `min_val` is included in the valid interval.\n          It is equivalent to the interval `[ min_val, max_val )`.\n        - `\"right\"`: only `max_val` is included in the valid interval.\n          It is equivalent to the interval `( min_val, max_val ]`.\n        - `\"both\"`: `min_val` and `max_val` are included in the valid interval.\n          It is equivalent to the interval `[ min_val, max_val ]`.\n        - `\"neither\"`: neither `min_val` nor `max_val` are included in the\n          valid interval. It is equivalent to the interval `( min_val, max_val )`.\n\n    Returns\n    -------\n    x : numbers.Number\n        The validated number.\n\n    Raises\n    ------\n    TypeError\n        If the parameter's type does not match the desired type.\n\n    ValueError\n        If the parameter's value violates the given bounds.\n        If `min_val`, `max_val` and `include_boundaries` are inconsistent.\n\n    Examples\n    --------\n    >>> from sklearn.utils.validation import check_scalar\n    >>> check_scalar(10, \"x\", int, min_val=1, max_val=20)\n    10\n    \"\"\"\n\n    def type_name(t):\n        \"\"\"Convert type into humman readable string.\"\"\"\n        module = t.__module__\n        qualname = t.__qualname__\n        if module == \"builtins\":\n            return qualname\n        elif t == numbers.Real:\n            return \"float\"\n        elif t == numbers.Integral:\n            return \"int\"\n        return f\"{module}.{qualname}\"\n\n    if not isinstance(x, target_type):\n        if isinstance(target_type, tuple):\n            types_str = \", \".join(type_name(t) for t in target_type)\n            target_type_str = f\"{{{types_str}}}\"\n        else:\n            target_type_str = type_name(target_type)\n\n        raise TypeError(\n            f\"{name} must be an instance of {target_type_str}, not\"\n            f\" {type(x).__qualname__}.\"\n        )\n\n    expected_include_boundaries = (\"left\", \"right\", \"both\", \"neither\")\n    if include_boundaries not in expected_include_boundaries:\n        raise ValueError(\n            f\"Unknown value for `include_boundaries`: {include_boundaries!r}. \"\n            f\"Possible values are: {expected_include_boundaries}.\"\n        )\n\n    if max_val is None and include_boundaries == \"right\":\n        raise ValueError(\n            \"`include_boundaries`='right' without specifying explicitly `max_val` \"\n            \"is inconsistent.\"\n        )\n\n    if min_val is None and include_boundaries == \"left\":\n        raise ValueError(\n            \"`include_boundaries`='left' without specifying explicitly `min_val` \"\n            \"is inconsistent.\"\n        )\n\n    comparison_operator = (\n        operator.lt if include_boundaries in (\"left\", \"both\") else operator.le\n    )\n    if min_val is not None and comparison_operator(x, min_val):\n        raise ValueError(\n            f\"{name} == {x}, must be\"\n            f\" {'>=' if include_boundaries in ('left', 'both') else '>'} {min_val}.\"\n        )\n\n    comparison_operator = (\n        operator.gt if include_boundaries in (\"right\", \"both\") else operator.ge\n    )\n    if max_val is not None and comparison_operator(x, max_val):\n        raise ValueError(\n            f\"{name} == {x}, must be\"\n            f\" {'<=' if include_boundaries in ('right', 'both') else '<'} {max_val}.\"\n        )\n\n    return x"
}