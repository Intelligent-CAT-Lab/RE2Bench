{
    "scikit-learn.sklearn.svm._base.predict": "def predict(self, X):\n    \"\"\"Perform regression on samples in X.\n\n    For a one-class model, +1 (inlier) or -1 (outlier) is returned.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        For kernel=\"precomputed\", the expected shape of X is\n        (n_samples_test, n_samples_train).\n\n    Returns\n    -------\n    y_pred : ndarray of shape (n_samples,)\n        The predicted values.\n    \"\"\"\n    X = self._validate_for_predict(X)\n    predict = self._sparse_predict if self._sparse else self._dense_predict\n    return predict(X)",
    "scikit-learn.sklearn.svm._base.decision_function": "def decision_function(self, X):\n    \"\"\"Evaluate the decision function for the samples in X.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input samples.\n\n    Returns\n    -------\n    X : ndarray of shape (n_samples, n_classes * (n_classes-1) / 2)\n        Returns the decision function of the sample for each class\n        in the model.\n        If decision_function_shape='ovr', the shape is (n_samples,\n        n_classes).\n\n    Notes\n    -----\n    If decision_function_shape='ovo', the function values are proportional\n    to the distance of the samples X to the separating hyperplane. If the\n    exact distances are required, divide the function values by the norm of\n    the weight vector (``coef_``). See also `this question\n    <https://stats.stackexchange.com/questions/14876/\n    interpreting-distance-from-hyperplane-in-svm>`_ for further details.\n    If decision_function_shape='ovr', the decision function is a monotonic\n    transformation of ovo decision function.\n    \"\"\"\n    dec = self._decision_function(X)\n    if self.decision_function_shape == \"ovr\" and len(self.classes_) > 2:\n        return _ovr_decision_function(dec < 0, -dec, len(self.classes_))\n    return dec",
    "scikit-learn.sklearn.utils.validation.check_is_fitted": "def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):\n    \"\"\"Perform is_fitted validation for estimator.\n\n    Checks if the estimator is fitted by verifying the presence of\n    fitted attributes (ending with a trailing underscore) and otherwise\n    raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to\n    specify if the estimator is fitted or not. See\n    :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\n    for an example on how to use the API.\n\n    If no `attributes` are passed, this function will pass if an estimator is stateless.\n    An estimator can indicate it's stateless by setting the `requires_fit` tag. See\n    :ref:`estimator_tags` for more information. Note that the `requires_fit` tag\n    is ignored if `attributes` are passed.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Raises\n    ------\n    TypeError\n        If the estimator is a class or not an estimator instance\n\n    NotFittedError\n        If the attributes are not found.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.utils.validation import check_is_fitted\n    >>> from sklearn.exceptions import NotFittedError\n    >>> lr = LogisticRegression()\n    >>> try:\n    ...     check_is_fitted(lr)\n    ... except NotFittedError as exc:\n    ...     print(f\"Model is not fitted yet.\")\n    Model is not fitted yet.\n    >>> lr.fit([[1, 2], [1, 3]], [1, 0])\n    LogisticRegression()\n    >>> check_is_fitted(lr)\n    \"\"\"\n    if isclass(estimator):\n        raise TypeError(\"{} is a class, not an instance.\".format(estimator))\n    if msg is None:\n        msg = (\n            \"This %(name)s instance is not fitted yet. Call 'fit' with \"\n            \"appropriate arguments before using this estimator.\"\n        )\n\n    if not hasattr(estimator, \"fit\"):\n        raise TypeError(\"%s is not an estimator instance.\" % (estimator))\n\n    tags = get_tags(estimator)\n\n    if not tags.requires_fit and attributes is None:\n        return\n\n    if not _is_fitted(estimator, attributes, all_or_any):\n        raise NotFittedError(msg % {\"name\": type(estimator).__name__})"
}