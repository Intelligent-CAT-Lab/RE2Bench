{
    "scikit-learn.sklearn.feature_extraction.text.build_preprocessor": "def build_preprocessor(self):\n    \"\"\"Return a function to preprocess the text before tokenization.\n\n    Returns\n    -------\n    preprocessor: callable\n          A function to preprocess the text before tokenization.\n    \"\"\"\n    if self.preprocessor is not None:\n        return self.preprocessor\n\n    # accent stripping\n    if not self.strip_accents:\n        strip_accents = None\n    elif callable(self.strip_accents):\n        strip_accents = self.strip_accents\n    elif self.strip_accents == \"ascii\":\n        strip_accents = strip_accents_ascii\n    elif self.strip_accents == \"unicode\":\n        strip_accents = strip_accents_unicode\n    else:\n        raise ValueError(\n            'Invalid value for \"strip_accents\": %s' % self.strip_accents\n        )\n\n    return partial(_preprocess, accent_function=strip_accents, lower=self.lowercase)",
    "scikit-learn.sklearn.feature_extraction.text.build_tokenizer": "def build_tokenizer(self):\n    \"\"\"Return a function that splits a string into a sequence of tokens.\n\n    Returns\n    -------\n    tokenizer: callable\n          A function to split a string into a sequence of tokens.\n    \"\"\"\n    if self.tokenizer is not None:\n        return self.tokenizer\n    token_pattern = re.compile(self.token_pattern)\n\n    if token_pattern.groups > 1:\n        raise ValueError(\n            \"More than 1 capturing group in token pattern. Only a single \"\n            \"group should be captured.\"\n        )\n\n    return token_pattern.findall",
    "scikit-learn.sklearn.feature_extraction.text.get_stop_words": "def get_stop_words(self):\n    \"\"\"Build or fetch the effective stop words list.\n\n    Returns\n    -------\n    stop_words: list or None\n            A list of stop words.\n    \"\"\"\n    return _check_stop_list(self.stop_words)",
    "scikit-learn.sklearn.feature_extraction.text._check_stop_words_consistency": "def _check_stop_words_consistency(self, stop_words, preprocess, tokenize):\n    \"\"\"Check if stop words are consistent\n\n    Returns\n    -------\n    is_consistent : True if stop words are consistent with the preprocessor\n                    and tokenizer, False if they are not, None if the check\n                    was previously performed, \"error\" if it could not be\n                    performed (e.g. because of the use of a custom\n                    preprocessor / tokenizer)\n    \"\"\"\n    if id(self.stop_words) == getattr(self, \"_stop_words_id\", None):\n        # Stop words are were previously validated\n        return None\n\n    # NB: stop_words is validated, unlike self.stop_words\n    try:\n        inconsistent = set()\n        for w in stop_words or ():\n            tokens = list(tokenize(preprocess(w)))\n            for token in tokens:\n                if token not in stop_words:\n                    inconsistent.add(token)\n        self._stop_words_id = id(self.stop_words)\n\n        if inconsistent:\n            warnings.warn(\n                \"Your stop_words may be inconsistent with \"\n                \"your preprocessing. Tokenizing the stop \"\n                \"words generated tokens %r not in \"\n                \"stop_words.\" % sorted(inconsistent)\n            )\n        return not inconsistent\n    except Exception:\n        # Failed to check stop words consistency (e.g. because a custom\n        # preprocessor or tokenizer was used)\n        self._stop_words_id = id(self.stop_words)\n        return \"error\""
}