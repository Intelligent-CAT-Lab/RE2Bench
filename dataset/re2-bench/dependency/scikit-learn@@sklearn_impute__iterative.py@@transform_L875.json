{
    "scikit-learn.sklearn.impute._base._transform_indicator": "def _transform_indicator(self, X):\n    \"\"\"Compute the indicator mask.'\n\n    Note that X must be the original data as passed to the imputer before\n    any imputation, since imputation may be done inplace in some cases.\n    \"\"\"\n    if self.add_indicator:\n        if not hasattr(self, \"indicator_\"):\n            raise ValueError(\n                \"Make sure to call _fit_indicator before _transform_indicator\"\n            )\n        return self.indicator_.transform(X)",
    "scikit-learn.sklearn.impute._base._concatenate_indicator": "def _concatenate_indicator(self, X_imputed, X_indicator):\n    \"\"\"Concatenate indicator mask with the imputed data.\"\"\"\n    if not self.add_indicator:\n        return X_imputed\n\n    if sp.issparse(X_imputed):\n        # sp.hstack may result in different formats between sparse arrays and\n        # matrices; specify the format to keep consistent behavior\n        hstack = partial(sp.hstack, format=X_imputed.format)\n    else:\n        hstack = np.hstack\n\n    if X_indicator is None:\n        raise ValueError(\n            \"Data from the missing indicator are not provided. Call \"\n            \"_fit_indicator and _transform_indicator in the imputer \"\n            \"implementation.\"\n        )\n\n    return hstack((X_imputed, X_indicator))",
    "scikit-learn.sklearn.impute._iterative._impute_one_feature": "def _impute_one_feature(\n    self,\n    X_filled,\n    mask_missing_values,\n    feat_idx,\n    neighbor_feat_idx,\n    estimator=None,\n    fit_mode=True,\n    params=None,\n):\n    \"\"\"Impute a single feature from the others provided.\n\n    This function predicts the missing values of one of the features using\n    the current estimates of all the other features. The `estimator` must\n    support `return_std=True` in its `predict` method for this function\n    to work.\n\n    Parameters\n    ----------\n    X_filled : ndarray\n        Input data with the most recent imputations.\n\n    mask_missing_values : ndarray\n        Input data's missing indicator matrix.\n\n    feat_idx : int\n        Index of the feature currently being imputed.\n\n    neighbor_feat_idx : ndarray\n        Indices of the features to be used in imputing `feat_idx`.\n\n    estimator : object\n        The estimator to use at this step of the round-robin imputation.\n        If `sample_posterior=True`, the estimator must support\n        `return_std` in its `predict` method.\n        If None, it will be cloned from self._estimator.\n\n    fit_mode : boolean, default=True\n        Whether to fit and predict with the estimator or just predict.\n\n    params : dict\n        Additional params routed to the individual estimator.\n\n    Returns\n    -------\n    X_filled : ndarray\n        Input data with `X_filled[missing_row_mask, feat_idx]` updated.\n\n    estimator : estimator with sklearn API\n        The fitted estimator used to impute\n        `X_filled[missing_row_mask, feat_idx]`.\n    \"\"\"\n    if estimator is None and fit_mode is False:\n        raise ValueError(\n            \"If fit_mode is False, then an already-fitted \"\n            \"estimator should be passed in.\"\n        )\n\n    if estimator is None:\n        estimator = clone(self._estimator)\n\n    missing_row_mask = mask_missing_values[:, feat_idx]\n    if fit_mode:\n        X_train = _safe_indexing(\n            _safe_indexing(X_filled, neighbor_feat_idx, axis=1),\n            ~missing_row_mask,\n            axis=0,\n        )\n        y_train = _safe_indexing(\n            _safe_indexing(X_filled, feat_idx, axis=1),\n            ~missing_row_mask,\n            axis=0,\n        )\n        estimator.fit(X_train, y_train, **params)\n\n    # if no missing values, don't predict\n    if np.sum(missing_row_mask) == 0:\n        return X_filled, estimator\n\n    # get posterior samples if there is at least one missing value\n    X_test = _safe_indexing(\n        _safe_indexing(X_filled, neighbor_feat_idx, axis=1),\n        missing_row_mask,\n        axis=0,\n    )\n    if self.sample_posterior:\n        mus, sigmas = estimator.predict(X_test, return_std=True)\n        imputed_values = np.zeros(mus.shape, dtype=X_filled.dtype)\n        # two types of problems: (1) non-positive sigmas\n        # (2) mus outside legal range of min_value and max_value\n        # (results in inf sample)\n        positive_sigmas = sigmas > 0\n        imputed_values[~positive_sigmas] = mus[~positive_sigmas]\n        mus_too_low = mus < self._min_value[feat_idx]\n        imputed_values[mus_too_low] = self._min_value[feat_idx]\n        mus_too_high = mus > self._max_value[feat_idx]\n        imputed_values[mus_too_high] = self._max_value[feat_idx]\n        # the rest can be sampled without statistical issues\n        inrange_mask = positive_sigmas & ~mus_too_low & ~mus_too_high\n        mus = mus[inrange_mask]\n        sigmas = sigmas[inrange_mask]\n        a = (self._min_value[feat_idx] - mus) / sigmas\n        b = (self._max_value[feat_idx] - mus) / sigmas\n\n        truncated_normal = stats.truncnorm(a=a, b=b, loc=mus, scale=sigmas)\n        imputed_values[inrange_mask] = truncated_normal.rvs(\n            random_state=self.random_state_\n        )\n    else:\n        imputed_values = estimator.predict(X_test)\n        imputed_values = np.clip(\n            imputed_values, self._min_value[feat_idx], self._max_value[feat_idx]\n        )\n\n    # update the feature\n    _safe_assign(\n        X_filled,\n        imputed_values,\n        row_indexer=missing_row_mask,\n        column_indexer=feat_idx,\n    )\n    return X_filled, estimator",
    "scikit-learn.sklearn.impute._iterative._assign_where": "def _assign_where(X1, X2, cond):\n    \"\"\"Assign X2 to X1 where cond is True.\n\n    Parameters\n    ----------\n    X1 : ndarray or dataframe of shape (n_samples, n_features)\n        Data.\n\n    X2 : ndarray of shape (n_samples, n_features)\n        Data to be assigned.\n\n    cond : ndarray of shape (n_samples, n_features)\n        Boolean mask to assign data.\n    \"\"\"\n    if hasattr(X1, \"mask\"):  # pandas dataframes\n        X1.mask(cond=cond, other=X2, inplace=True)\n    else:  # ndarrays\n        X1[cond] = X2[cond]",
    "scikit-learn.sklearn.impute._iterative._initial_imputation": "def _initial_imputation(self, X, in_fit=False):\n    \"\"\"Perform initial imputation for input `X`.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_features)\n        Input data, where `n_samples` is the number of samples and\n        `n_features` is the number of features.\n\n    in_fit : bool, default=False\n        Whether function is called in :meth:`fit`.\n\n    Returns\n    -------\n    Xt : ndarray of shape (n_samples, n_features)\n        Input data, where `n_samples` is the number of samples and\n        `n_features` is the number of features.\n\n    X_filled : ndarray of shape (n_samples, n_features)\n        Input data with the most recent imputations.\n\n    mask_missing_values : ndarray of shape (n_samples, n_features)\n        Input data's missing indicator matrix, where `n_samples` is the\n        number of samples and `n_features` is the number of features,\n        masked by non-missing features.\n\n    X_missing_mask : ndarray, shape (n_samples, n_features)\n        Input data's mask matrix indicating missing datapoints, where\n        `n_samples` is the number of samples and `n_features` is the\n        number of features.\n    \"\"\"\n    if is_scalar_nan(self.missing_values):\n        ensure_all_finite = \"allow-nan\"\n    else:\n        ensure_all_finite = True\n\n    X = validate_data(\n        self,\n        X,\n        dtype=FLOAT_DTYPES,\n        order=\"F\",\n        reset=in_fit,\n        ensure_all_finite=ensure_all_finite,\n    )\n    _check_inputs_dtype(X, self.missing_values)\n\n    X_missing_mask = _get_mask(X, self.missing_values)\n    mask_missing_values = X_missing_mask.copy()\n\n    if self.initial_imputer_ is None:\n        self.initial_imputer_ = SimpleImputer(\n            missing_values=self.missing_values,\n            strategy=self.initial_strategy,\n            fill_value=self.fill_value,\n            keep_empty_features=self.keep_empty_features,\n        ).set_output(transform=\"default\")\n\n        X_filled = self.initial_imputer_.fit_transform(X)\n\n    else:\n        X_filled = self.initial_imputer_.transform(X)\n\n    if in_fit:\n        self._is_empty_feature = np.all(mask_missing_values, axis=0)\n\n    if not self.keep_empty_features:\n        # drop empty features\n        Xt = X[:, ~self._is_empty_feature]\n        mask_missing_values = mask_missing_values[:, ~self._is_empty_feature]\n\n    else:\n        # mark empty features as not missing and keep the original\n        # imputation\n        mask_missing_values[:, self._is_empty_feature] = False\n        Xt = X\n        Xt[:, self._is_empty_feature] = X_filled[:, self._is_empty_feature]\n\n    return Xt, X_filled, mask_missing_values, X_missing_mask",
    "scikit-learn.sklearn.utils.validation.check_is_fitted": "def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):\n    \"\"\"Perform is_fitted validation for estimator.\n\n    Checks if the estimator is fitted by verifying the presence of\n    fitted attributes (ending with a trailing underscore) and otherwise\n    raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to\n    specify if the estimator is fitted or not. See\n    :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\n    for an example on how to use the API.\n\n    If no `attributes` are passed, this function will pass if an estimator is stateless.\n    An estimator can indicate it's stateless by setting the `requires_fit` tag. See\n    :ref:`estimator_tags` for more information. Note that the `requires_fit` tag\n    is ignored if `attributes` are passed.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Raises\n    ------\n    TypeError\n        If the estimator is a class or not an estimator instance\n\n    NotFittedError\n        If the attributes are not found.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.utils.validation import check_is_fitted\n    >>> from sklearn.exceptions import NotFittedError\n    >>> lr = LogisticRegression()\n    >>> try:\n    ...     check_is_fitted(lr)\n    ... except NotFittedError as exc:\n    ...     print(f\"Model is not fitted yet.\")\n    Model is not fitted yet.\n    >>> lr.fit([[1, 2], [1, 3]], [1, 0])\n    LogisticRegression()\n    >>> check_is_fitted(lr)\n    \"\"\"\n    if isclass(estimator):\n        raise TypeError(\"{} is a class, not an instance.\".format(estimator))\n    if msg is None:\n        msg = (\n            \"This %(name)s instance is not fitted yet. Call 'fit' with \"\n            \"appropriate arguments before using this estimator.\"\n        )\n\n    if not hasattr(estimator, \"fit\"):\n        raise TypeError(\"%s is not an estimator instance.\" % (estimator))\n\n    tags = get_tags(estimator)\n\n    if not tags.requires_fit and attributes is None:\n        return\n\n    if not _is_fitted(estimator, attributes, all_or_any):\n        raise NotFittedError(msg % {\"name\": type(estimator).__name__})"
}