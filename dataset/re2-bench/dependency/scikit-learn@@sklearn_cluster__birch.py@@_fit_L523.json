{
    "scikit-learn.sklearn.cluster._birch.__init__": "def __init__(self, *, linear_sum=None):\n    if linear_sum is None:\n        self.n_samples_ = 0\n        self.squared_sum_ = 0.0\n        self.centroid_ = self.linear_sum_ = 0\n    else:\n        self.n_samples_ = 1\n        self.centroid_ = self.linear_sum_ = linear_sum\n        self.squared_sum_ = self.sq_norm_ = np.dot(\n            self.linear_sum_, self.linear_sum_\n        )\n    self.child_ = None",
    "scikit-learn.sklearn.cluster._birch.append_subcluster": "def append_subcluster(self, subcluster):\n    n_samples = len(self.subclusters_)\n    self.subclusters_.append(subcluster)\n    self.init_centroids_[n_samples] = subcluster.centroid_\n    self.init_sq_norm_[n_samples] = subcluster.sq_norm_\n\n    # Keep centroids and squared norm as views. In this way\n    # if we change init_centroids and init_sq_norm_, it is\n    # sufficient,\n    self.centroids_ = self.init_centroids_[: n_samples + 1, :]\n    self.squared_norm_ = self.init_sq_norm_[: n_samples + 1]",
    "scikit-learn.sklearn.cluster._birch.insert_cf_subcluster": "def insert_cf_subcluster(self, subcluster):\n    \"\"\"Insert a new subcluster into the node.\"\"\"\n    if not self.subclusters_:\n        self.append_subcluster(subcluster)\n        return False\n\n    threshold = self.threshold\n    branching_factor = self.branching_factor\n    # We need to find the closest subcluster among all the\n    # subclusters so that we can insert our new subcluster.\n    dist_matrix = np.dot(self.centroids_, subcluster.centroid_)\n    dist_matrix *= -2.0\n    dist_matrix += self.squared_norm_\n    closest_index = np.argmin(dist_matrix)\n    closest_subcluster = self.subclusters_[closest_index]\n\n    # If the subcluster has a child, we need a recursive strategy.\n    if closest_subcluster.child_ is not None:\n        split_child = closest_subcluster.child_.insert_cf_subcluster(subcluster)\n\n        if not split_child:\n            # If it is determined that the child need not be split, we\n            # can just update the closest_subcluster\n            closest_subcluster.update(subcluster)\n            self.init_centroids_[closest_index] = self.subclusters_[\n                closest_index\n            ].centroid_\n            self.init_sq_norm_[closest_index] = self.subclusters_[\n                closest_index\n            ].sq_norm_\n            return False\n\n        # things not too good. we need to redistribute the subclusters in\n        # our child node, and add a new subcluster in the parent\n        # subcluster to accommodate the new child.\n        else:\n            new_subcluster1, new_subcluster2 = _split_node(\n                closest_subcluster.child_,\n                threshold,\n                branching_factor,\n            )\n            self.update_split_subclusters(\n                closest_subcluster, new_subcluster1, new_subcluster2\n            )\n\n            if len(self.subclusters_) > self.branching_factor:\n                return True\n            return False\n\n    # good to go!\n    else:\n        merged = closest_subcluster.merge_subcluster(subcluster, self.threshold)\n        if merged:\n            self.init_centroids_[closest_index] = closest_subcluster.centroid_\n            self.init_sq_norm_[closest_index] = closest_subcluster.sq_norm_\n            return False\n\n        # not close to any other subclusters, and we still\n        # have space, so add.\n        elif len(self.subclusters_) < self.branching_factor:\n            self.append_subcluster(subcluster)\n            return False\n\n        # We do not have enough space nor is it closer to an\n        # other subcluster. We need to split.\n        else:\n            self.append_subcluster(subcluster)\n            return True",
    "scikit-learn.sklearn.cluster._birch._iterate_sparse_X": "def _iterate_sparse_X(X):\n    \"\"\"This little hack returns a densified row when iterating over a sparse\n    matrix, instead of constructing a sparse matrix for every row that is\n    expensive.\n    \"\"\"\n    n_samples = X.shape[0]\n    X_indices = X.indices\n    X_data = X.data\n    X_indptr = X.indptr\n\n    for i in range(n_samples):\n        row = np.zeros(X.shape[1])\n        startptr, endptr = X_indptr[i], X_indptr[i + 1]\n        nonzero_indices = X_indices[startptr:endptr]\n        row[nonzero_indices] = X_data[startptr:endptr]\n        yield row",
    "scikit-learn.sklearn.cluster._birch._split_node": "def _split_node(node, threshold, branching_factor):\n    \"\"\"The node has to be split if there is no place for a new subcluster\n    in the node.\n    1. Two empty nodes and two empty subclusters are initialized.\n    2. The pair of distant subclusters are found.\n    3. The properties of the empty subclusters and nodes are updated\n       according to the nearest distance between the subclusters to the\n       pair of distant subclusters.\n    4. The two nodes are set as children to the two subclusters.\n    \"\"\"\n    new_subcluster1 = _CFSubcluster()\n    new_subcluster2 = _CFSubcluster()\n    new_node1 = _CFNode(\n        threshold=threshold,\n        branching_factor=branching_factor,\n        is_leaf=node.is_leaf,\n        n_features=node.n_features,\n        dtype=node.init_centroids_.dtype,\n    )\n    new_node2 = _CFNode(\n        threshold=threshold,\n        branching_factor=branching_factor,\n        is_leaf=node.is_leaf,\n        n_features=node.n_features,\n        dtype=node.init_centroids_.dtype,\n    )\n    new_subcluster1.child_ = new_node1\n    new_subcluster2.child_ = new_node2\n\n    if node.is_leaf:\n        if node.prev_leaf_ is not None:\n            node.prev_leaf_.next_leaf_ = new_node1\n        new_node1.prev_leaf_ = node.prev_leaf_\n        new_node1.next_leaf_ = new_node2\n        new_node2.prev_leaf_ = new_node1\n        new_node2.next_leaf_ = node.next_leaf_\n        if node.next_leaf_ is not None:\n            node.next_leaf_.prev_leaf_ = new_node2\n\n    dist = euclidean_distances(\n        node.centroids_, Y_norm_squared=node.squared_norm_, squared=True\n    )\n    n_clusters = dist.shape[0]\n\n    farthest_idx = np.unravel_index(dist.argmax(), (n_clusters, n_clusters))\n    node1_dist, node2_dist = dist[(farthest_idx,)]\n\n    node1_closer = node1_dist < node2_dist\n    # make sure node1 is closest to itself even if all distances are equal.\n    # This can only happen when all node.centroids_ are duplicates leading to all\n    # distances between centroids being zero.\n    node1_closer[farthest_idx[0]] = True\n\n    for idx, subcluster in enumerate(node.subclusters_):\n        if node1_closer[idx]:\n            new_node1.append_subcluster(subcluster)\n            new_subcluster1.update(subcluster)\n        else:\n            new_node2.append_subcluster(subcluster)\n            new_subcluster2.update(subcluster)\n    return new_subcluster1, new_subcluster2",
    "scikit-learn.sklearn.cluster._birch.<listcomp>": "centroids = np.concatenate([leaf.centroids_ for leaf in self._get_leaves()])\n",
    "scikit-learn.sklearn.cluster._birch._get_leaves": "def _get_leaves(self):\n    \"\"\"\n    Retrieve the leaves of the CF Node.\n\n    Returns\n    -------\n    leaves : list of shape (n_leaves,)\n        List of the leaf nodes.\n    \"\"\"\n    leaf_ptr = self.dummy_leaf_.next_leaf_\n    leaves = []\n    while leaf_ptr is not None:\n        leaves.append(leaf_ptr)\n        leaf_ptr = leaf_ptr.next_leaf_\n    return leaves",
    "scikit-learn.sklearn.cluster._birch._global_clustering": "def _global_clustering(self, X=None):\n    \"\"\"\n    Global clustering for the subclusters obtained after fitting\n    \"\"\"\n    clusterer = self.n_clusters\n    centroids = self.subcluster_centers_\n    compute_labels = (X is not None) and self.compute_labels\n\n    # Preprocessing for the global clustering.\n    not_enough_centroids = False\n    if isinstance(clusterer, Integral):\n        clusterer = AgglomerativeClustering(n_clusters=self.n_clusters)\n        # There is no need to perform the global clustering step.\n        if len(centroids) < self.n_clusters:\n            not_enough_centroids = True\n\n    # To use in predict to avoid recalculation.\n    self._subcluster_norms = row_norms(self.subcluster_centers_, squared=True)\n\n    if clusterer is None or not_enough_centroids:\n        self.subcluster_labels_ = np.arange(len(centroids))\n        if not_enough_centroids:\n            warnings.warn(\n                \"Number of subclusters found (%d) by BIRCH is less \"\n                \"than (%d). Decrease the threshold.\"\n                % (len(centroids), self.n_clusters),\n                ConvergenceWarning,\n            )\n    else:\n        # The global clustering step that clusters the subclusters of\n        # the leaves. It assumes the centroids of the subclusters as\n        # samples and finds the final centroids.\n        self.subcluster_labels_ = clusterer.fit_predict(self.subcluster_centers_)\n\n    if compute_labels:\n        self.labels_ = self._predict(X)",
    "scikit-learn.sklearn.utils.validation.validate_data": "def validate_data(\n    _estimator,\n    /,\n    X=\"no_validation\",\n    y=\"no_validation\",\n    reset=True,\n    validate_separately=False,\n    skip_check_array=False,\n    **check_params,\n):\n    \"\"\"Validate input data and set or check feature names and counts of the input.\n\n    This helper function should be used in an estimator that requires input\n    validation. This mutates the estimator and sets the `n_features_in_` and\n    `feature_names_in_` attributes if `reset=True`.\n\n    .. versionadded:: 1.6\n\n    Parameters\n    ----------\n    _estimator : estimator instance\n        The estimator to validate the input for.\n\n    X : {array-like, sparse matrix, dataframe} of shape \\\n            (n_samples, n_features), default='no validation'\n        The input samples.\n        If `'no_validation'`, no validation is performed on `X`. This is\n        useful for meta-estimator which can delegate input validation to\n        their underlying estimator(s). In that case `y` must be passed and\n        the only accepted `check_params` are `multi_output` and\n        `y_numeric`.\n\n    y : array-like of shape (n_samples,), default='no_validation'\n        The targets.\n\n        - If `None`, :func:`~sklearn.utils.check_array` is called on `X`. If\n          the estimator's `requires_y` tag is True, then an error will be raised.\n        - If `'no_validation'`, :func:`~sklearn.utils.check_array` is called\n          on `X` and the estimator's `requires_y` tag is ignored. This is a default\n          placeholder and is never meant to be explicitly set. In that case `X` must be\n          passed.\n        - Otherwise, only `y` with `_check_y` or both `X` and `y` are checked with\n          either :func:`~sklearn.utils.check_array` or\n          :func:`~sklearn.utils.check_X_y` depending on `validate_separately`.\n\n    reset : bool, default=True\n        Whether to reset the `n_features_in_` attribute.\n        If False, the input will be checked for consistency with data\n        provided when reset was last True.\n\n        .. note::\n\n           It is recommended to call `reset=True` in `fit` and in the first\n           call to `partial_fit`. All other methods that validate `X`\n           should set `reset=False`.\n\n    validate_separately : False or tuple of dicts, default=False\n        Only used if `y` is not `None`.\n        If `False`, call :func:`~sklearn.utils.check_X_y`. Else, it must be a tuple of\n        kwargs to be used for calling :func:`~sklearn.utils.check_array` on `X` and `y`\n        respectively.\n\n        `estimator=self` is automatically added to these dicts to generate\n        more informative error message in case of invalid input data.\n\n    skip_check_array : bool, default=False\n        If `True`, `X` and `y` are unchanged and only `feature_names_in_` and\n        `n_features_in_` are checked. Otherwise, :func:`~sklearn.utils.check_array`\n        is called on `X` and `y`.\n\n    **check_params : kwargs\n        Parameters passed to :func:`~sklearn.utils.check_array` or\n        :func:`~sklearn.utils.check_X_y`. Ignored if validate_separately\n        is not False.\n\n        `estimator=self` is automatically added to these params to generate\n        more informative error message in case of invalid input data.\n\n    Returns\n    -------\n    out : {ndarray, sparse matrix} or tuple of these\n        The validated input. A tuple is returned if both `X` and `y` are\n        validated.\n    \"\"\"\n    _check_feature_names(_estimator, X, reset=reset)\n    tags = get_tags(_estimator)\n    if y is None and tags.target_tags.required:\n        raise ValueError(\n            f\"This {_estimator.__class__.__name__} estimator \"\n            \"requires y to be passed, but the target y is None.\"\n        )\n\n    no_val_X = isinstance(X, str) and X == \"no_validation\"\n    no_val_y = y is None or (isinstance(y, str) and y == \"no_validation\")\n\n    if no_val_X and no_val_y:\n        raise ValueError(\"Validation should be done on X, y or both.\")\n\n    default_check_params = {\"estimator\": _estimator}\n    check_params = {**default_check_params, **check_params}\n\n    if skip_check_array:\n        if not no_val_X and no_val_y:\n            out = X\n        elif no_val_X and not no_val_y:\n            out = y\n        else:\n            out = X, y\n    elif not no_val_X and no_val_y:\n        out = check_array(X, input_name=\"X\", **check_params)\n    elif no_val_X and not no_val_y:\n        out = _check_y(y, **check_params)\n    else:\n        if validate_separately:\n            # We need this because some estimators validate X and y\n            # separately, and in general, separately calling check_array()\n            # on X and y isn't equivalent to just calling check_X_y()\n            # :(\n            check_X_params, check_y_params = validate_separately\n            if \"estimator\" not in check_X_params:\n                check_X_params = {**default_check_params, **check_X_params}\n            X = check_array(X, input_name=\"X\", **check_X_params)\n            if \"estimator\" not in check_y_params:\n                check_y_params = {**default_check_params, **check_y_params}\n            y = check_array(y, input_name=\"y\", **check_y_params)\n        else:\n            X, y = check_X_y(X, y, **check_params)\n        out = X, y\n\n    if not no_val_X and check_params.get(\"ensure_2d\", True):\n        _check_n_features(_estimator, X, reset=reset)\n\n    return out"
}