{
    "scikit-learn.sklearn.externals.array_api_compat._internal.wrapped_f": "@wraps(f)\ndef wrapped_f(*args: object, **kwargs: object) -> object:\n    return f(*args, xp=xp, **kwargs)",
    "scikit-learn.sklearn.externals.array_api_compat.numpy._aliases.astype": "def astype(\n    x: Array,\n    dtype: DType,\n    /,\n    *,\n    copy: py_bool = True,\n    device: Device | None = None,\n) -> Array:\n    _helpers._check_device(np, device)\n    return x.astype(dtype=dtype, copy=copy)",
    "scikit-learn.sklearn.linear_model._base._preprocess_data": "def _preprocess_data(\n    X,\n    y,\n    *,\n    fit_intercept,\n    copy=True,\n    copy_y=True,\n    sample_weight=None,\n    check_input=True,\n    rescale_with_sw=True,\n):\n    \"\"\"Common data preprocessing for fitting linear models.\n\n    This helper is in charge of the following steps:\n\n    - `sample_weight` is assumed to be `None` or a validated array with same dtype as\n      `X`.\n    - If `check_input=True`, perform standard input validation of `X`, `y`.\n    - Perform copies if requested to avoid side-effects in case of inplace\n      modifications of the input.\n\n    Then, if `fit_intercept=True` this preprocessing centers both `X` and `y` as\n    follows:\n        - if `X` is dense, center the data and\n        store the mean vector in `X_offset`.\n        - if `X` is sparse, store the mean in `X_offset`\n        without centering `X`. The centering is expected to be handled by the\n        linear solver where appropriate.\n        - in either case, always center `y` and store the mean in `y_offset`.\n        - both `X_offset` and `y_offset` are always weighted by `sample_weight`\n          if not set to `None`.\n\n    If `fit_intercept=False`, no centering is performed and `X_offset`, `y_offset`\n    are set to zero.\n\n    If `rescale_with_sw` is True, then X and y are rescaled with the square root of\n    sample weights.\n\n    Returns\n    -------\n    X_out : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        If copy=True a copy of the input X is triggered, otherwise operations are\n        inplace.\n        If input X is dense, then X_out is centered.\n    y_out : {ndarray, sparse matrix} of shape (n_samples,) or (n_samples, n_targets)\n        Centered version of y. Possibly performed inplace on input y depending\n        on the copy_y parameter.\n    X_offset : ndarray of shape (n_features,)\n        The mean per column of input X.\n    y_offset : float or ndarray of shape (n_features,)\n    X_scale : ndarray of shape (n_features,)\n        Always an array of ones. TODO: refactor the code base to make it\n        possible to remove this unused variable.\n    sample_weight_sqrt : ndarray of shape (n_samples, ) or None\n        `np.sqrt(sample_weight)`\n    \"\"\"\n    xp, _, device_ = get_namespace_and_device(X, y, sample_weight)\n    n_samples, n_features = X.shape\n    X_is_sparse = sp.issparse(X)\n\n    if check_input:\n        X = check_array(\n            X, copy=copy, accept_sparse=[\"csr\", \"csc\"], dtype=supported_float_dtypes(xp)\n        )\n        y = check_array(y, dtype=X.dtype, copy=copy_y, ensure_2d=False)\n    else:\n        y = xp.astype(y, X.dtype, copy=copy_y)\n        if copy:\n            if X_is_sparse:\n                X = X.copy()\n            else:\n                X = _asarray_with_order(X, order=\"K\", copy=True, xp=xp)\n\n    dtype_ = X.dtype\n\n    if fit_intercept:\n        if X_is_sparse:\n            X_offset, X_var = mean_variance_axis(X, axis=0, weights=sample_weight)\n        else:\n            X_offset = _average(X, axis=0, weights=sample_weight, xp=xp)\n\n            X_offset = xp.astype(X_offset, X.dtype, copy=False)\n            X -= X_offset\n\n        y_offset = _average(y, axis=0, weights=sample_weight, xp=xp)\n        y -= y_offset\n    else:\n        X_offset = xp.zeros(n_features, dtype=X.dtype, device=device_)\n        if y.ndim == 1:\n            y_offset = xp.asarray(0.0, dtype=dtype_, device=device_)\n        else:\n            y_offset = xp.zeros(y.shape[1], dtype=dtype_, device=device_)\n\n    # X_scale is no longer needed. It is a historic artifact from the\n    # time where linear model exposed the normalize parameter.\n    X_scale = xp.ones(n_features, dtype=X.dtype, device=device_)\n\n    if sample_weight is not None and rescale_with_sw:\n        # Sample weight can be implemented via a simple rescaling.\n        # For sparse X and y, it triggers copies anyway.\n        # For dense X and y that already have been copied, we safely do inplace\n        # rescaling.\n        X, y, sample_weight_sqrt = _rescale_data(X, y, sample_weight, inplace=copy)\n    else:\n        sample_weight_sqrt = None\n    return X, y, X_offset, y_offset, X_scale, sample_weight_sqrt",
    "scikit-learn.sklearn.linear_model._base._set_intercept": "def _set_intercept(self, X_offset, y_offset, X_scale=None):\n    \"\"\"Set the intercept_\"\"\"\n    xp, _ = get_namespace(X_offset, y_offset, X_scale)\n\n    if self.fit_intercept:\n        # We always want coef_.dtype=X.dtype. For instance, X.dtype can differ from\n        # coef_.dtype if warm_start=True.\n        self.coef_ = xp.astype(self.coef_, X_offset.dtype, copy=False)\n        if X_scale is not None:\n            self.coef_ = xp.divide(self.coef_, X_scale)\n\n        if self.coef_.ndim == 1:\n            self.intercept_ = y_offset - X_offset @ self.coef_\n        else:\n            self.intercept_ = y_offset - X_offset @ self.coef_.T\n\n    else:\n        self.intercept_ = 0.0",
    "scikit-learn.sklearn.linear_model._ridge._check_gcv_mode": "def _check_gcv_mode(X, gcv_mode):\n    if gcv_mode in [\"eigen\", \"svd\"]:\n        return gcv_mode\n    # if X has more rows than columns, use decomposition of X^T.X,\n    # otherwise X.X^T\n    if X.shape[0] > X.shape[1]:\n        return \"svd\"\n    return \"eigen\"",
    "scikit-learn.sklearn.linear_model._ridge._eigen_decompose_gram": "def _eigen_decompose_gram(self, X, y, sqrt_sw):\n    \"\"\"Eigendecomposition of X.X^T, used when n_samples <= n_features.\"\"\"\n    # if X is dense it has already been centered in preprocessing\n    xp, is_array_api = get_namespace(X)\n    K, X_mean = self._compute_gram(X, sqrt_sw)\n    if self.fit_intercept:\n        # to emulate centering X with sample weights,\n        # ie removing the weighted average, we add a column\n        # containing the square roots of the sample weights.\n        # by centering, it is orthogonal to the other columns\n        K += xp.linalg.outer(sqrt_sw, sqrt_sw)\n    eigvals, Q = xp.linalg.eigh(K)\n    QT_y = Q.T @ y\n    return X_mean, eigvals, Q, QT_y",
    "scikit-learn.sklearn.linear_model._ridge._solve_eigen_gram": "def _solve_eigen_gram(self, alpha, y, sqrt_sw, X_mean, eigvals, Q, QT_y):\n    \"\"\"Compute dual coefficients and diagonal of G^-1.\n\n    Used when we have a decomposition of X.X^T (n_samples <= n_features).\n    \"\"\"\n    xp, is_array_api = get_namespace(eigvals)\n    w = 1.0 / (eigvals + alpha)\n    if self.fit_intercept:\n        # the vector containing the square roots of the sample weights (1\n        # when no sample weights) is the eigenvector of XX^T which\n        # corresponds to the intercept; we cancel the regularization on\n        # this dimension. the corresponding eigenvalue is\n        # sum(sample_weight).\n        norm = xp.linalg.vector_norm if is_array_api else np.linalg.norm\n        normalized_sw = sqrt_sw / norm(sqrt_sw)\n        intercept_dim = _find_smallest_angle(normalized_sw, Q)\n        w[intercept_dim] = 0  # cancel regularization for the intercept\n\n    c = Q @ self._diag_dot(w, QT_y)\n    G_inverse_diag = self._decomp_diag(w, Q)\n    # handle case where y is 2-d\n    if len(y.shape) != 1:\n        G_inverse_diag = G_inverse_diag[:, None]\n    return G_inverse_diag, c",
    "scikit-learn.sklearn.linear_model._ridge._eigen_decompose_covariance": "def _eigen_decompose_covariance(self, X, y, sqrt_sw):\n    \"\"\"Eigendecomposition of X^T.X, used when n_samples > n_features\n    and X is sparse.\n    \"\"\"\n    n_samples, n_features = X.shape\n    cov = np.empty((n_features + 1, n_features + 1), dtype=X.dtype)\n    cov[:-1, :-1], X_mean = self._compute_covariance(X, sqrt_sw)\n    if not self.fit_intercept:\n        cov = cov[:-1, :-1]\n    # to emulate centering X with sample weights,\n    # ie removing the weighted average, we add a column\n    # containing the square roots of the sample weights.\n    # by centering, it is orthogonal to the other columns\n    # when all samples have the same weight we add a column of 1\n    else:\n        cov[-1] = 0\n        cov[:, -1] = 0\n        cov[-1, -1] = sqrt_sw.dot(sqrt_sw)\n    nullspace_dim = max(0, n_features - n_samples)\n    eigvals, V = linalg.eigh(cov)\n    # remove eigenvalues and vectors in the null space of X^T.X\n    eigvals = eigvals[nullspace_dim:]\n    V = V[:, nullspace_dim:]\n    return X_mean, eigvals, V, X",
    "scikit-learn.sklearn.linear_model._ridge._solve_eigen_covariance": "def _solve_eigen_covariance(self, alpha, y, sqrt_sw, X_mean, eigvals, V, X):\n    \"\"\"Compute dual coefficients and diagonal of G^-1.\n\n    Used when we have a decomposition of X^T.X\n    (n_samples > n_features and X is sparse).\n    \"\"\"\n    if self.fit_intercept:\n        return self._solve_eigen_covariance_intercept(\n            alpha, y, sqrt_sw, X_mean, eigvals, V, X\n        )\n    return self._solve_eigen_covariance_no_intercept(\n        alpha, y, sqrt_sw, X_mean, eigvals, V, X\n    )",
    "scikit-learn.sklearn.linear_model._ridge._svd_decompose_design_matrix": "def _svd_decompose_design_matrix(self, X, y, sqrt_sw):\n    xp, _, device_ = get_namespace_and_device(X)\n    # X already centered\n    X_mean = xp.zeros(X.shape[1], dtype=X.dtype, device=device_)\n    if self.fit_intercept:\n        # to emulate fit_intercept=True situation, add a column\n        # containing the square roots of the sample weights\n        # by centering, the other columns are orthogonal to that one\n        intercept_column = sqrt_sw[:, None]\n        X = xp.concat((X, intercept_column), axis=1)\n    U, singvals, _ = xp.linalg.svd(X, full_matrices=False)\n    singvals_sq = singvals**2\n    UT_y = U.T @ y\n    return X_mean, singvals_sq, U, UT_y",
    "scikit-learn.sklearn.linear_model._ridge._solve_svd_design_matrix": "def _solve_svd_design_matrix(self, alpha, y, sqrt_sw, X_mean, singvals_sq, U, UT_y):\n    \"\"\"Compute dual coefficients and diagonal of G^-1.\n\n    Used when we have an SVD decomposition of X\n    (n_samples > n_features and X is dense).\n    \"\"\"\n    xp, is_array_api = get_namespace(U)\n    w = ((singvals_sq + alpha) ** -1) - (alpha**-1)\n    if self.fit_intercept:\n        # detect intercept column\n        normalized_sw = sqrt_sw / xp.linalg.vector_norm(sqrt_sw)\n        intercept_dim = int(_find_smallest_angle(normalized_sw, U))\n        # cancel the regularization for the intercept\n        w[intercept_dim] = -(alpha**-1)\n    c = U @ self._diag_dot(w, UT_y) + (alpha**-1) * y\n    G_inverse_diag = self._decomp_diag(w, U) + (alpha**-1)\n    if len(y.shape) != 1:\n        # handle case where y is 2-d\n        G_inverse_diag = G_inverse_diag[:, None]\n    return G_inverse_diag, c",
    "scikit-learn.sklearn.linear_model._ridge._score_without_scorer": "def _score_without_scorer(self, squared_errors):\n    \"\"\"Performs scoring using squared errors when the scorer is None.\"\"\"\n    xp, _ = get_namespace(squared_errors)\n    if self.alpha_per_target:\n        _score = xp.mean(-squared_errors, axis=0)\n    else:\n        _score = xp.mean(-squared_errors)\n\n    return _score",
    "scikit-learn.sklearn.linear_model._ridge._score": "def _score(self, *, predictions, y, n_y, scorer, score_params):\n    \"\"\"Performs scoring with the specified scorer using the\n    predictions and the true y values.\n    \"\"\"\n    xp, _, device_ = get_namespace_and_device(y)\n    if self.is_clf:\n        identity_estimator = _IdentityClassifier(\n            classes=xp.arange(n_y, device=device_)\n        )\n        _score = scorer(\n            identity_estimator,\n            predictions,\n            xp.argmax(y, axis=1),\n            **score_params,\n        )\n    else:\n        identity_estimator = _IdentityRegressor()\n        if self.alpha_per_target:\n            _score = xp.asarray(\n                [\n                    scorer(\n                        identity_estimator,\n                        predictions[:, j],\n                        y[:, j],\n                        **score_params,\n                    )\n                    for j in range(n_y)\n                ],\n                device=device_,\n            )\n        else:\n            _score = scorer(\n                identity_estimator,\n                predictions,\n                y,\n                **score_params,\n            )\n\n    return _score",
    "scikit-learn.sklearn.utils._array_api.get_namespace_and_device": "def get_namespace_and_device(\n    *array_list, remove_none=True, remove_types=(str,), xp=None\n):\n    \"\"\"Combination into one single function of `get_namespace` and `device`.\n\n    Parameters\n    ----------\n    *array_list : array objects\n        Array objects.\n    remove_none : bool, default=True\n        Whether to ignore None objects passed in arrays.\n    remove_types : tuple or list, default=(str,)\n        Types to ignore in the arrays.\n    xp : module, default=None\n        Precomputed array namespace module. When passed, typically from a caller\n        that has already performed inspection of its own inputs, skips array\n        namespace inspection.\n\n    Returns\n    -------\n    namespace : module\n        Namespace shared by array objects. If any of the `arrays` are not arrays,\n        the namespace defaults to NumPy.\n    is_array_api_compliant : bool\n        True if the arrays are containers that implement the Array API spec.\n        Always False when array_api_dispatch=False.\n    device : device\n        `device` object (see the \"Device Support\" section of the array API spec).\n    \"\"\"\n    skip_remove_kwargs = dict(remove_none=False, remove_types=[])\n\n    array_list = _remove_non_arrays(\n        *array_list,\n        remove_none=remove_none,\n        remove_types=remove_types,\n    )\n    arrays_device = device(*array_list, **skip_remove_kwargs)\n\n    if xp is None:\n        xp, is_array_api = get_namespace(*array_list, **skip_remove_kwargs)\n    else:\n        xp, is_array_api = xp, True\n\n    if is_array_api:\n        return xp, is_array_api, arrays_device\n    else:\n        return xp, False, arrays_device",
    "scikit-learn.sklearn.utils._array_api.move_to": "def move_to(*arrays, xp, device):\n    \"\"\"Move all arrays to `xp` and `device`.\n\n    Each array will be moved to the reference namespace and device if\n    it is not already using it. Otherwise the array is left unchanged.\n\n    `array` may contain `None` entries, these are left unchanged.\n\n    Sparse arrays are accepted (as pass through) if the reference namespace is\n    Numpy, in which case they are returned unchanged. Otherwise a `TypeError`\n    is raised.\n\n    Parameters\n    ----------\n    *arrays : iterable of arrays\n        Arrays to (potentially) move.\n\n    xp : namespace\n        Array API namespace to move arrays to.\n\n    device : device\n        Array API device to move arrays to.\n\n    Returns\n    -------\n    arrays : tuple or array\n        Tuple of arrays with the same namespace and device as reference. Single array\n        returned if only one `arrays` input.\n    \"\"\"\n    sparse_mask = [sp.issparse(array) for array in arrays]\n    none_mask = [array is None for array in arrays]\n    if any(sparse_mask) and not _is_numpy_namespace(xp):\n        raise TypeError(\n            \"Sparse arrays are only accepted (and passed through) when the target \"\n            \"namespace is Numpy\"\n        )\n\n    converted_arrays = []\n\n    for array, is_sparse, is_none in zip(arrays, sparse_mask, none_mask):\n        if is_none:\n            converted_arrays.append(None)\n        elif is_sparse:\n            converted_arrays.append(array)\n        else:\n            xp_array, _, device_array = get_namespace_and_device(array)\n            if xp == xp_array and device == device_array:\n                converted_arrays.append(array)\n            else:\n                try:\n                    # The dlpack protocol is the future proof and library agnostic\n                    # method to transfer arrays across namespace and device boundaries\n                    # hence this method is attempted first and going through NumPy is\n                    # only used as fallback in case of failure.\n                    # Note: copy=None is the default since array-api 2023.12. Namespace\n                    # libraries should only trigger a copy automatically if needed.\n                    array_converted = xp.from_dlpack(array, device=device)\n                    # `AttributeError` occurs when `__dlpack__` and `__dlpack_device__`\n                    # methods are not present on the input array\n                    # `TypeError` and `NotImplementedError` for packages that do not\n                    # yet support dlpack 1.0\n                    # (i.e. the `device`/`copy` kwargs, e.g., torch <= 2.8.0)\n                    # See https://github.com/data-apis/array-api/pull/741 for\n                    # more details about the introduction of the `copy` and `device`\n                    # kwargs in the from_dlpack method and their expected\n                    # meaning by namespaces implementing the array API spec.\n                    # TODO: try removing this once DLPack v1 more widely supported\n                    # TODO: ValueError should not be needed but is in practice:\n                    # https://github.com/numpy/numpy/issues/30341\n                except (\n                    AttributeError,\n                    TypeError,\n                    NotImplementedError,\n                    BufferError,\n                    ValueError,\n                ):\n                    # Converting to numpy is tricky, handle this via dedicated function\n                    if _is_numpy_namespace(xp):\n                        array_converted = _convert_to_numpy(array, xp_array)\n                    # Convert from numpy, all array libraries can do this\n                    elif _is_numpy_namespace(xp_array):\n                        array_converted = xp.asarray(array, device=device)\n                    else:\n                        # There is no generic way to convert from namespace A to B\n                        # So we first convert from A to numpy and then from numpy to B\n                        # The way to avoid this round trip is to lobby for DLpack\n                        # support in libraries A and B\n                        array_np = _convert_to_numpy(array, xp_array)\n                        array_converted = xp.asarray(array_np, device=device)\n                converted_arrays.append(array_converted)\n\n    return (\n        converted_arrays[0] if len(converted_arrays) == 1 else tuple(converted_arrays)\n    )",
    "scikit-learn.sklearn.utils._array_api._max_precision_float_dtype": "def _max_precision_float_dtype(xp, device):\n    \"\"\"Return the float dtype with the highest precision supported by the device.\"\"\"\n    # TODO: Update to use `__array_namespace__info__()` from array-api v2023.12\n    # when/if that becomes more widespread.\n    if _is_xp_namespace(xp, \"torch\") and str(device).startswith(\n        \"mps\"\n    ):  # pragma: no cover\n        return xp.float32\n    return xp.float64",
    "scikit-learn.sklearn.utils._array_api._ravel": "def _ravel(array, xp=None):\n    \"\"\"Array API compliant version of np.ravel.\n\n    For non numpy namespaces, it just returns a flattened array, that might\n    be or not be a copy.\n    \"\"\"\n    xp, _ = get_namespace(array, xp=xp)\n    if _is_numpy_namespace(xp):\n        array = numpy.asarray(array)\n        return xp.asarray(numpy.ravel(array, order=\"C\"))\n\n    return xp.reshape(array, shape=(-1,))",
    "scikit-learn.sklearn.utils.validation._check_sample_weight": "def _check_sample_weight(\n    sample_weight,\n    X,\n    *,\n    dtype=None,\n    force_float_dtype=True,\n    ensure_non_negative=False,\n    ensure_same_device=True,\n    copy=False,\n):\n    \"\"\"Validate sample weights.\n\n    Note that passing sample_weight=None will output an array of ones.\n    Therefore, in some cases, you may want to protect the call with:\n    if sample_weight is not None:\n        sample_weight = _check_sample_weight(...)\n\n    Parameters\n    ----------\n    sample_weight : {ndarray, Number or None}, shape (n_samples,)\n        Input sample weights.\n\n    X : {ndarray, list, sparse matrix}\n        Input data.\n\n    dtype : dtype, default=None\n        dtype of the validated `sample_weight`.\n        If None, and `sample_weight` is an array:\n\n            - If `sample_weight.dtype` is one of `{np.float64, np.float32}`,\n              then the dtype is preserved.\n            - Else the output has NumPy's default dtype: `np.float64`.\n\n        If `dtype` is not `{np.float32, np.float64, None}`, then output will\n        be `np.float64`.\n\n    force_float_dtype : bool, default=True\n        Whether `X` should be forced to be float dtype, when `dtype` is a non-float\n        dtype or None.\n\n    ensure_non_negative : bool, default=False,\n        Whether or not the weights are expected to be non-negative.\n\n        .. versionadded:: 1.0\n\n    ensure_same_device : bool, default=True\n        Whether `sample_weight` should be forced to be on the same device as `X`.\n\n    copy : bool, default=False\n        If True, a copy of sample_weight will be created.\n\n    Returns\n    -------\n    sample_weight : ndarray of shape (n_samples,)\n        Validated sample weight. It is guaranteed to be \"C\" contiguous.\n    \"\"\"\n    xp, is_array_api, device = get_namespace_and_device(X, remove_types=(int, float))\n\n    n_samples = _num_samples(X)\n\n    max_float_type = _max_precision_float_dtype(xp, device)\n    float_dtypes = (\n        [xp.float32] if max_float_type == xp.float32 else [xp.float64, xp.float32]\n    )\n    if force_float_dtype and dtype is not None and dtype not in float_dtypes:\n        dtype = max_float_type\n\n    if sample_weight is None:\n        sample_weight = xp.ones(n_samples, dtype=dtype, device=device)\n    elif isinstance(sample_weight, numbers.Number):\n        sample_weight = xp.full(n_samples, sample_weight, dtype=dtype, device=device)\n    else:\n        if force_float_dtype and dtype is None:\n            dtype = float_dtypes\n        if is_array_api and ensure_same_device:\n            sample_weight = xp.asarray(sample_weight, device=device)\n        sample_weight = check_array(\n            sample_weight,\n            accept_sparse=False,\n            ensure_2d=False,\n            dtype=dtype,\n            order=\"C\",\n            copy=copy,\n            input_name=\"sample_weight\",\n        )\n        if sample_weight.ndim != 1:\n            raise ValueError(\n                f\"Sample weights must be 1D array or scalar, got \"\n                f\"{sample_weight.ndim}D array. Expected either a scalar value \"\n                f\"or a 1D array of length {n_samples}.\"\n            )\n\n        if sample_weight.shape != (n_samples,):\n            raise ValueError(\n                \"sample_weight.shape == {}, expected {}!\".format(\n                    sample_weight.shape, (n_samples,)\n                )\n            )\n\n    if ensure_non_negative:\n        check_non_negative(sample_weight, \"`sample_weight`\")\n\n    return sample_weight",
    "scikit-learn.sklearn.utils.validation.validate_data": "def validate_data(\n    _estimator,\n    /,\n    X=\"no_validation\",\n    y=\"no_validation\",\n    reset=True,\n    validate_separately=False,\n    skip_check_array=False,\n    **check_params,\n):\n    \"\"\"Validate input data and set or check feature names and counts of the input.\n\n    This helper function should be used in an estimator that requires input\n    validation. This mutates the estimator and sets the `n_features_in_` and\n    `feature_names_in_` attributes if `reset=True`.\n\n    .. versionadded:: 1.6\n\n    Parameters\n    ----------\n    _estimator : estimator instance\n        The estimator to validate the input for.\n\n    X : {array-like, sparse matrix, dataframe} of shape \\\n            (n_samples, n_features), default='no validation'\n        The input samples.\n        If `'no_validation'`, no validation is performed on `X`. This is\n        useful for meta-estimator which can delegate input validation to\n        their underlying estimator(s). In that case `y` must be passed and\n        the only accepted `check_params` are `multi_output` and\n        `y_numeric`.\n\n    y : array-like of shape (n_samples,), default='no_validation'\n        The targets.\n\n        - If `None`, :func:`~sklearn.utils.check_array` is called on `X`. If\n          the estimator's `requires_y` tag is True, then an error will be raised.\n        - If `'no_validation'`, :func:`~sklearn.utils.check_array` is called\n          on `X` and the estimator's `requires_y` tag is ignored. This is a default\n          placeholder and is never meant to be explicitly set. In that case `X` must be\n          passed.\n        - Otherwise, only `y` with `_check_y` or both `X` and `y` are checked with\n          either :func:`~sklearn.utils.check_array` or\n          :func:`~sklearn.utils.check_X_y` depending on `validate_separately`.\n\n    reset : bool, default=True\n        Whether to reset the `n_features_in_` attribute.\n        If False, the input will be checked for consistency with data\n        provided when reset was last True.\n\n        .. note::\n\n           It is recommended to call `reset=True` in `fit` and in the first\n           call to `partial_fit`. All other methods that validate `X`\n           should set `reset=False`.\n\n    validate_separately : False or tuple of dicts, default=False\n        Only used if `y` is not `None`.\n        If `False`, call :func:`~sklearn.utils.check_X_y`. Else, it must be a tuple of\n        kwargs to be used for calling :func:`~sklearn.utils.check_array` on `X` and `y`\n        respectively.\n\n        `estimator=self` is automatically added to these dicts to generate\n        more informative error message in case of invalid input data.\n\n    skip_check_array : bool, default=False\n        If `True`, `X` and `y` are unchanged and only `feature_names_in_` and\n        `n_features_in_` are checked. Otherwise, :func:`~sklearn.utils.check_array`\n        is called on `X` and `y`.\n\n    **check_params : kwargs\n        Parameters passed to :func:`~sklearn.utils.check_array` or\n        :func:`~sklearn.utils.check_X_y`. Ignored if validate_separately\n        is not False.\n\n        `estimator=self` is automatically added to these params to generate\n        more informative error message in case of invalid input data.\n\n    Returns\n    -------\n    out : {ndarray, sparse matrix} or tuple of these\n        The validated input. A tuple is returned if both `X` and `y` are\n        validated.\n    \"\"\"\n    _check_feature_names(_estimator, X, reset=reset)\n    tags = get_tags(_estimator)\n    if y is None and tags.target_tags.required:\n        raise ValueError(\n            f\"This {_estimator.__class__.__name__} estimator \"\n            \"requires y to be passed, but the target y is None.\"\n        )\n\n    no_val_X = isinstance(X, str) and X == \"no_validation\"\n    no_val_y = y is None or (isinstance(y, str) and y == \"no_validation\")\n\n    if no_val_X and no_val_y:\n        raise ValueError(\"Validation should be done on X, y or both.\")\n\n    default_check_params = {\"estimator\": _estimator}\n    check_params = {**default_check_params, **check_params}\n\n    if skip_check_array:\n        if not no_val_X and no_val_y:\n            out = X\n        elif no_val_X and not no_val_y:\n            out = y\n        else:\n            out = X, y\n    elif not no_val_X and no_val_y:\n        out = check_array(X, input_name=\"X\", **check_params)\n    elif no_val_X and not no_val_y:\n        out = _check_y(y, **check_params)\n    else:\n        if validate_separately:\n            # We need this because some estimators validate X and y\n            # separately, and in general, separately calling check_array()\n            # on X and y isn't equivalent to just calling check_X_y()\n            # :(\n            check_X_params, check_y_params = validate_separately\n            if \"estimator\" not in check_X_params:\n                check_X_params = {**default_check_params, **check_X_params}\n            X = check_array(X, input_name=\"X\", **check_X_params)\n            if \"estimator\" not in check_y_params:\n                check_y_params = {**default_check_params, **check_y_params}\n            y = check_array(y, input_name=\"y\", **check_y_params)\n        else:\n            X, y = check_X_y(X, y, **check_params)\n        out = X, y\n\n    if not no_val_X and check_params.get(\"ensure_2d\", True):\n        _check_n_features(_estimator, X, reset=reset)\n\n    return out"
}