{
    "scikit-learn.sklearn.decomposition._fastica._logcosh": "def _logcosh(x, fun_args=None):\n    alpha = fun_args.get(\"alpha\", 1.0)  # comment it out?\n\n    x *= alpha\n    gx = np.tanh(x, x)  # apply the tanh inplace\n    g_x = np.empty(x.shape[0], dtype=x.dtype)\n    # XXX compute in chunks to avoid extra allocation\n    for i, gx_i in enumerate(gx):  # please don't vectorize.\n        g_x[i] = (alpha * (1 - gx_i**2)).mean()\n    return gx, g_x",
    "scikit-learn.sklearn.decomposition._fastica._exp": "def _exp(x, fun_args):\n    exp = np.exp(-(x**2) / 2)\n    gx = x * exp\n    g_x = (1 - x**2) * exp\n    return gx, g_x.mean(axis=-1)",
    "scikit-learn.sklearn.decomposition._fastica._cube": "def _cube(x, fun_args):\n    return x**3, (3 * x**2).mean(axis=-1)",
    "scikit-learn.sklearn.decomposition._fastica.g": "def g(x, fun_args):\n    return self.fun(x, **fun_args)",
    "scikit-learn.sklearn.decomposition._fastica._sym_decorrelation": "def _sym_decorrelation(W):\n    \"\"\"Symmetric decorrelation\n    i.e. W <- (W * W.T) ^{-1/2} * W\n    \"\"\"\n    s, u = linalg.eigh(np.dot(W, W.T))\n    # Avoid sqrt of negative values because of rounding errors. Note that\n    # np.sqrt(tiny) is larger than tiny and therefore this clipping also\n    # prevents division by zero in the next step.\n    s = np.clip(s, a_min=np.finfo(W.dtype).tiny, a_max=None)\n\n    # u (resp. s) contains the eigenvectors (resp. square roots of\n    # the eigenvalues) of W * W.T\n    return np.linalg.multi_dot([u * (1.0 / np.sqrt(s)), u.T, W])"
}