{
    "scikit-learn.sklearn.feature_selection._rfe._fit": "def _fit(self, X, y, step_score=None, **fit_params):\n    # Parameter step_score controls the calculation of self.step_scores_\n    # step_score is not exposed to users and is used when implementing RFECV\n    # self.step_scores_ will not be calculated when calling _fit through fit\n\n    X, y = validate_data(\n        self,\n        X,\n        y,\n        accept_sparse=\"csc\",\n        ensure_min_features=2,\n        ensure_all_finite=False,\n        multi_output=True,\n    )\n\n    # Initialization\n    n_features = X.shape[1]\n    if self.n_features_to_select is None:\n        n_features_to_select = n_features // 2\n    elif isinstance(self.n_features_to_select, Integral):  # int\n        n_features_to_select = self.n_features_to_select\n        if n_features_to_select > n_features:\n            warnings.warn(\n                (\n                    f\"Found {n_features_to_select=} > {n_features=}. There will be\"\n                    \" no feature selection and all features will be kept.\"\n                ),\n                UserWarning,\n            )\n    else:  # float\n        n_features_to_select = int(n_features * self.n_features_to_select)\n\n    if 0.0 < self.step < 1.0:\n        step = int(max(1, self.step * n_features))\n    else:\n        step = int(self.step)\n\n    support_ = np.ones(n_features, dtype=bool)\n    ranking_ = np.ones(n_features, dtype=int)\n\n    if step_score:\n        self.step_n_features_ = []\n        self.step_scores_ = []\n        self.step_support_ = []\n        self.step_ranking_ = []\n\n    # Elimination\n    while np.sum(support_) > n_features_to_select:\n        # Remaining features\n        features = np.arange(n_features)[support_]\n\n        # Rank the remaining features\n        estimator = clone(self.estimator)\n        if self.verbose > 0:\n            print(\"Fitting estimator with %d features.\" % np.sum(support_))\n\n        estimator.fit(X[:, features], y, **fit_params)\n\n        # Compute step values on the previous selection iteration because\n        # 'estimator' must use features that have not been eliminated yet\n        if step_score:\n            self.step_n_features_.append(len(features))\n            self.step_scores_.append(step_score(estimator, features))\n            self.step_support_.append(list(support_))\n            self.step_ranking_.append(list(ranking_))\n\n        # Get importance and rank them\n        importances = _get_feature_importances(\n            estimator,\n            self.importance_getter,\n            transform_func=\"square\",\n        )\n        ranks = np.argsort(importances)\n\n        # for sparse case ranks is matrix\n        ranks = np.ravel(ranks)\n\n        # Eliminate the worse features\n        threshold = min(step, np.sum(support_) - n_features_to_select)\n\n        support_[features[ranks][:threshold]] = False\n        ranking_[np.logical_not(support_)] += 1\n\n    # Set final attributes\n    features = np.arange(n_features)[support_]\n    self.estimator_ = clone(self.estimator)\n    self.estimator_.fit(X[:, features], y, **fit_params)\n\n    # Compute step values when only n_features_to_select features left\n    if step_score:\n        self.step_n_features_.append(len(features))\n        self.step_scores_.append(step_score(self.estimator_, features))\n        self.step_support_.append(support_)\n        self.step_ranking_.append(ranking_)\n    self.n_features_ = support_.sum()\n    self.support_ = support_\n    self.ranking_ = ranking_\n\n    return self",
    "scikit-learn.sklearn.utils._bunch.__getattr__": "def __getattr__(self, key):\n    try:\n        return self[key]\n    except KeyError:\n        raise AttributeError(key)",
    "scikit-learn.sklearn.utils._metadata_requests.__getattr__": "def __getattr__(self, name):\n    return Bunch(**{method: dict() for method in METHODS})",
    "scikit-learn.sklearn.utils.metaestimators._safe_split": "def _safe_split(estimator, X, y, indices, train_indices=None):\n    \"\"\"Create subset of dataset and properly handle kernels.\n\n    Slice X, y according to indices for cross-validation, but take care of\n    precomputed kernel-matrices or pairwise affinities / distances.\n\n    If ``estimator._pairwise is True``, X needs to be square and\n    we slice rows and columns. If ``train_indices`` is not None,\n    we slice rows using ``indices`` (assumed the test set) and columns\n    using ``train_indices``, indicating the training set.\n\n    Labels y will always be indexed only along the first axis.\n\n    Parameters\n    ----------\n    estimator : object\n        Estimator to determine whether we should slice only rows or rows and\n        columns.\n\n    X : array-like, sparse matrix or iterable\n        Data to be indexed. If ``estimator._pairwise is True``,\n        this needs to be a square array-like or sparse matrix.\n\n    y : array-like, sparse matrix or iterable\n        Targets to be indexed.\n\n    indices : array of int\n        Rows to select from X and y.\n        If ``estimator._pairwise is True`` and ``train_indices is None``\n        then ``indices`` will also be used to slice columns.\n\n    train_indices : array of int or None, default=None\n        If ``estimator._pairwise is True`` and ``train_indices is not None``,\n        then ``train_indices`` will be use to slice the columns of X.\n\n    Returns\n    -------\n    X_subset : array-like, sparse matrix or list\n        Indexed data.\n\n    y_subset : array-like, sparse matrix or list\n        Indexed targets.\n\n    \"\"\"\n    if get_tags(estimator).input_tags.pairwise:\n        if not hasattr(X, \"shape\"):\n            raise ValueError(\n                \"Precomputed kernels or affinity matrices have \"\n                \"to be passed as arrays or sparse matrices.\"\n            )\n        # X is a precomputed square kernel matrix\n        if X.shape[0] != X.shape[1]:\n            raise ValueError(\"X should be a square kernel matrix\")\n        if train_indices is None:\n            X_subset = X[np.ix_(indices, indices)]\n        else:\n            X_subset = X[np.ix_(indices, train_indices)]\n    else:\n        X_subset = _safe_indexing(X, indices)\n\n    if y is not None:\n        y_subset = _safe_indexing(y, indices)\n    else:\n        y_subset = None\n\n    return X_subset, y_subset",
    "scikit-learn.sklearn.utils.validation._check_method_params": "def _check_method_params(X, params, indices=None):\n    \"\"\"Check and validate the parameters passed to a specific\n    method like `fit`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data array.\n\n    params : dict\n        Dictionary containing the parameters passed to the method.\n\n    indices : array-like of shape (n_samples,), default=None\n        Indices to be selected if the parameter has the same size as `X`.\n\n    Returns\n    -------\n    method_params_validated : dict\n        Validated parameters. We ensure that the values support indexing.\n    \"\"\"\n    from sklearn.utils import _safe_indexing\n\n    method_params_validated = {}\n    for param_key, param_value in params.items():\n        if (\n            not _is_arraylike(param_value) and not sp.issparse(param_value)\n        ) or _num_samples(param_value) != _num_samples(X):\n            # Non-indexable pass-through (for now for backward-compatibility).\n            # https://github.com/scikit-learn/scikit-learn/issues/15805\n            method_params_validated[param_key] = param_value\n        else:\n            # Any other method_params should support indexing\n            # (e.g. for cross-validation).\n            method_params_validated[param_key] = _make_indexable(param_value)\n            method_params_validated[param_key] = _safe_indexing(\n                method_params_validated[param_key], indices\n            )\n\n    return method_params_validated"
}