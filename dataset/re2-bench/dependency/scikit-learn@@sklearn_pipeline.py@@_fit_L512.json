{
    "scikit-learn.sklearn.base.clone": "def clone(estimator, *, safe=True):\n    \"\"\"Construct a new unfitted estimator with the same parameters.\n\n    Clone does a deep copy of the model in an estimator\n    without actually copying attached data. It returns a new estimator\n    with the same parameters that has not been fitted on any data.\n\n    .. versionchanged:: 1.3\n        Delegates to `estimator.__sklearn_clone__` if the method exists.\n\n    Parameters\n    ----------\n    estimator : {list, tuple, set} of estimator instance or a single \\\n            estimator instance\n        The estimator or group of estimators to be cloned.\n    safe : bool, default=True\n        If safe is False, clone will fall back to a deep copy on objects\n        that are not estimators. Ignored if `estimator.__sklearn_clone__`\n        exists.\n\n    Returns\n    -------\n    estimator : object\n        The deep copy of the input, an estimator if input is an estimator.\n\n    Notes\n    -----\n    If the estimator's `random_state` parameter is an integer (or if the\n    estimator doesn't have a `random_state` parameter), an *exact clone* is\n    returned: the clone and the original estimator will give the exact same\n    results. Otherwise, *statistical clone* is returned: the clone might\n    return different results from the original estimator. More details can be\n    found in :ref:`randomness`.\n\n    Examples\n    --------\n    >>> from sklearn.base import clone\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> X = [[-1, 0], [0, 1], [0, -1], [1, 0]]\n    >>> y = [0, 0, 1, 1]\n    >>> classifier = LogisticRegression().fit(X, y)\n    >>> cloned_classifier = clone(classifier)\n    >>> hasattr(classifier, \"classes_\")\n    True\n    >>> hasattr(cloned_classifier, \"classes_\")\n    False\n    >>> classifier is cloned_classifier\n    False\n    \"\"\"\n    if hasattr(estimator, \"__sklearn_clone__\") and not inspect.isclass(estimator):\n        return estimator.__sklearn_clone__()\n    return _clone_parametrized(estimator, safe=safe)",
    "scikit-learn.sklearn.pipeline._validate_steps": "def _validate_steps(self):\n    if not self.steps:\n        raise ValueError(\"The pipeline is empty. Please add steps.\")\n    names, estimators = zip(*self.steps)\n\n    # validate names\n    self._validate_names(names)\n\n    # validate estimators\n    transformers = estimators[:-1]\n    estimator = estimators[-1]\n\n    for t in transformers:\n        if t is None or t == \"passthrough\":\n            continue\n        if not (hasattr(t, \"fit\") or hasattr(t, \"fit_transform\")) or not hasattr(\n            t, \"transform\"\n        ):\n            raise TypeError(\n                \"All intermediate steps should be \"\n                \"transformers and implement fit and transform \"\n                \"or be the string 'passthrough' \"\n                \"'%s' (type %s) doesn't\" % (t, type(t))\n            )\n\n    # We allow last estimator to be None as an identity transformation\n    if (\n        estimator is not None\n        and estimator != \"passthrough\"\n        and not hasattr(estimator, \"fit\")\n    ):\n        raise TypeError(\n            \"Last step of Pipeline should implement fit \"\n            \"or be the string 'passthrough'. \"\n            \"'%s' (type %s) doesn't\" % (estimator, type(estimator))\n        )",
    "scikit-learn.sklearn.pipeline._iter": "def _iter(self, with_final=True, filter_passthrough=True):\n    \"\"\"\n    Generate (idx, (name, trans)) tuples from self.steps\n\n    When filter_passthrough is True, 'passthrough' and None transformers\n    are filtered out.\n    \"\"\"\n    stop = len(self.steps)\n    if not with_final:\n        stop -= 1\n\n    for idx, (name, trans) in enumerate(islice(self.steps, 0, stop)):\n        if not filter_passthrough:\n            yield idx, name, trans\n        elif trans is not None and trans != \"passthrough\":\n            yield idx, name, trans",
    "scikit-learn.sklearn.pipeline._log_message": "def _log_message(self, step_idx):\n    if not self.verbose:\n        return None\n    name, _ = self.steps[step_idx]\n\n    return \"(step %d of %d) Processing %s\" % (step_idx + 1, len(self.steps), name)",
    "scikit-learn.sklearn.pipeline._get_metadata_for_step": "def _get_metadata_for_step(self, *, step_idx, step_params, all_params):\n    \"\"\"Get params (metadata) for step `name`.\n\n    This transforms the metadata up to this step if required, which is\n    indicated by the `transform_input` parameter.\n\n    If a param in `step_params` is included in the `transform_input` list,\n    it will be transformed.\n\n    Parameters\n    ----------\n    step_idx : int\n        Index of the step in the pipeline.\n\n    step_params : dict\n        Parameters specific to the step. These are routed parameters, e.g.\n        `routed_params[name]`. If a parameter name here is included in the\n        `pipeline.transform_input`, then it will be transformed. Note that\n        these parameters are *after* routing, so the aliases are already\n        resolved.\n\n    all_params : dict\n        All parameters passed by the user. Here this is used to call\n        `transform` on the slice of the pipeline itself.\n\n    Returns\n    -------\n    dict\n        Parameters to be passed to the step. The ones which should be\n        transformed are transformed.\n    \"\"\"\n    if (\n        self.transform_input is None\n        or not all_params\n        or not step_params\n        or step_idx == 0\n    ):\n        # we only need to process step_params if transform_input is set\n        # and metadata is given by the user.\n        return step_params\n\n    sub_pipeline = self[:step_idx]\n    sub_metadata_routing = get_routing_for_object(sub_pipeline)\n    # here we get the metadata required by sub_pipeline.transform\n    transform_params = {\n        key: value\n        for key, value in all_params.items()\n        if key\n        in sub_metadata_routing.consumes(\n            method=\"transform\", params=all_params.keys()\n        )\n    }\n    transformed_params = dict()  # this is to be returned\n    transformed_cache = dict()  # used to transform each param once\n    # `step_params` is the output of `process_routing`, so it has a dict for each\n    # method (e.g. fit, transform, predict), which are the args to be passed to\n    # those methods. We need to transform the parameters which are in the\n    # `transform_input`, before returning these dicts.\n    for method, method_params in step_params.items():\n        transformed_params[method] = Bunch()\n        for param_name, param_value in method_params.items():\n            # An example of `(param_name, param_value)` is\n            # `('sample_weight', array([0.5, 0.5, ...]))`\n            if param_name in self.transform_input:\n                # This parameter now needs to be transformed by the sub_pipeline, to\n                # this step. We cache these computations to avoid repeating them.\n                transformed_params[method][param_name] = _cached_transform(\n                    sub_pipeline,\n                    cache=transformed_cache,\n                    param_name=param_name,\n                    param_value=param_value,\n                    transform_params=transform_params,\n                )\n            else:\n                transformed_params[method][param_name] = param_value\n    return transformed_params",
    "scikit-learn.sklearn.utils._bunch.__getitem__": "def __getitem__(self, key):\n    if key in self.__dict__.get(\"_deprecated_key_to_warnings\", {}):\n        warnings.warn(\n            self._deprecated_key_to_warnings[key],\n            FutureWarning,\n        )\n    return super().__getitem__(key)",
    "scikit-learn.sklearn.utils._metadata_requests.__getitem__": "def __getitem__(self, name):\n    return Bunch(**{method: dict() for method in METHODS})",
    "scikit-learn.sklearn.utils.validation.check_memory": "def check_memory(memory):\n    \"\"\"Check that ``memory`` is joblib.Memory-like.\n\n    joblib.Memory-like means that ``memory`` can be converted into a\n    joblib.Memory instance (typically a str denoting the ``location``)\n    or has the same interface (has a ``cache`` method).\n\n    Parameters\n    ----------\n    memory : None, str or object with the joblib.Memory interface\n        - If string, the location where to create the `joblib.Memory` interface.\n        - If None, no caching is done and the Memory object is completely transparent.\n\n    Returns\n    -------\n    memory : object with the joblib.Memory interface\n        A correct joblib.Memory object.\n\n    Raises\n    ------\n    ValueError\n        If ``memory`` is not joblib.Memory-like.\n\n    Examples\n    --------\n    >>> from sklearn.utils.validation import check_memory\n    >>> check_memory(\"caching_dir\")\n    Memory(location=caching_dir/joblib)\n    \"\"\"\n    if memory is None or isinstance(memory, str):\n        memory = joblib.Memory(location=memory, verbose=0)\n    elif not hasattr(memory, \"cache\"):\n        raise ValueError(\n            \"'memory' should be None, a string or have the same\"\n            \" interface as joblib.Memory.\"\n            \" Got memory='{}' instead.\".format(memory)\n        )\n    return memory"
}