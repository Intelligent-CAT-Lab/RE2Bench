{
    "scikit-learn.sklearn.preprocessing._data.fit": "def fit(self, K, y=None):\n    \"\"\"Fit KernelCenterer.\n\n    Parameters\n    ----------\n    K : ndarray of shape (n_samples, n_samples)\n        Kernel matrix.\n\n    y : None\n        Ignored.\n\n    Returns\n    -------\n    self : object\n        Returns the instance itself.\n    \"\"\"\n    xp, _ = get_namespace(K)\n\n    K = validate_data(self, K, dtype=_array_api.supported_float_dtypes(xp))\n\n    if K.shape[0] != K.shape[1]:\n        raise ValueError(\n            \"Kernel matrix must be a square matrix.\"\n            \" Input is a {}x{} matrix.\".format(K.shape[0], K.shape[1])\n        )\n\n    n_samples = K.shape[0]\n    self.K_fit_rows_ = xp.sum(K, axis=0) / n_samples\n    self.K_fit_all_ = xp.sum(self.K_fit_rows_) / n_samples\n    return self",
    "scikit-learn.sklearn.utils._arpack._init_arpack_v0": "def _init_arpack_v0(size, random_state):\n    \"\"\"Initialize the starting vector for iteration in ARPACK functions.\n\n    Initialize an ndarray with values sampled from the uniform distribution on\n    [-1, 1]. This initialization model has been chosen to be consistent with\n    the ARPACK one as another initialization can lead to convergence issues.\n\n    Parameters\n    ----------\n    size : int\n        The size of the eigenvalue vector to be initialized.\n\n    random_state : int, RandomState instance or None, default=None\n        The seed of the pseudo random number generator used to generate a\n        uniform distribution. If int, random_state is the seed used by the\n        random number generator; If RandomState instance, random_state is the\n        random number generator; If None, the random number generator is the\n        RandomState instance used by `np.random`.\n\n    Returns\n    -------\n    v0 : ndarray of shape (size,)\n        The initialized vector.\n    \"\"\"\n    random_state = check_random_state(random_state)\n    v0 = random_state.uniform(-1, 1, size)\n    return v0",
    "scikit-learn.sklearn.utils._set_output.wrapped": "@wraps(f)\ndef wrapped(self, X, *args, **kwargs):\n    data_to_wrap = f(self, X, *args, **kwargs)\n    if isinstance(data_to_wrap, tuple):\n        # only wrap the first output for cross decomposition\n        return_tuple = (\n            _wrap_data_with_container(method, data_to_wrap[0], X, self),\n            *data_to_wrap[1:],\n        )\n        # Support for namedtuples `_make` is a documented API for namedtuples:\n        # https://docs.python.org/3/library/collections.html#collections.somenamedtuple._make\n        if hasattr(type(data_to_wrap), \"_make\"):\n            return type(data_to_wrap)._make(return_tuple)\n        return return_tuple\n\n    return _wrap_data_with_container(method, data_to_wrap, X, self)",
    "scikit-learn.sklearn.utils.extmath._randomized_eigsh": "def _randomized_eigsh(\n    M,\n    n_components,\n    *,\n    n_oversamples=10,\n    n_iter=\"auto\",\n    power_iteration_normalizer=\"auto\",\n    selection=\"module\",\n    random_state=None,\n):\n    \"\"\"Computes a truncated eigendecomposition using randomized methods\n\n    This method solves the fixed-rank approximation problem described in the\n    Halko et al paper.\n\n    The choice of which components to select can be tuned with the `selection`\n    parameter.\n\n    .. versionadded:: 0.24\n\n    Parameters\n    ----------\n    M : ndarray or sparse matrix\n        Matrix to decompose, it should be real symmetric square or complex\n        hermitian\n\n    n_components : int\n        Number of eigenvalues and vectors to extract.\n\n    n_oversamples : int, default=10\n        Additional number of random vectors to sample the range of M so as\n        to ensure proper conditioning. The total number of random vectors\n        used to find the range of M is n_components + n_oversamples. Smaller\n        number can improve speed but can negatively impact the quality of\n        approximation of eigenvectors and eigenvalues. Users might wish\n        to increase this parameter up to `2*k - n_components` where k is the\n        effective rank, for large matrices, noisy problems, matrices with\n        slowly decaying spectrums, or to increase precision accuracy. See Halko\n        et al (pages 5, 23 and 26).\n\n    n_iter : int or 'auto', default='auto'\n        Number of power iterations. It can be used to deal with very noisy\n        problems. When 'auto', it is set to 4, unless `n_components` is small\n        (< .1 * min(X.shape)) in which case `n_iter` is set to 7.\n        This improves precision with few components. Note that in general\n        users should rather increase `n_oversamples` before increasing `n_iter`\n        as the principle of the randomized method is to avoid usage of these\n        more costly power iterations steps. When `n_components` is equal\n        or greater to the effective matrix rank and the spectrum does not\n        present a slow decay, `n_iter=0` or `1` should even work fine in theory\n        (see Halko et al paper, page 9).\n\n    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n        Whether the power iterations are normalized with step-by-step\n        QR factorization (the slowest but most accurate), 'none'\n        (the fastest but numerically unstable when `n_iter` is large, e.g.\n        typically 5 or larger), or 'LU' factorization (numerically stable\n        but can lose slightly in accuracy). The 'auto' mode applies no\n        normalization if `n_iter` <= 2 and switches to LU otherwise.\n\n    selection : {'value', 'module'}, default='module'\n        Strategy used to select the n components. When `selection` is `'value'`\n        (not yet implemented, will become the default when implemented), the\n        components corresponding to the n largest eigenvalues are returned.\n        When `selection` is `'module'`, the components corresponding to the n\n        eigenvalues with largest modules are returned.\n\n    random_state : int, RandomState instance, default=None\n        The seed of the pseudo random number generator to use when shuffling\n        the data, i.e. getting the random vectors to initialize the algorithm.\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Notes\n    -----\n    This algorithm finds a (usually very good) approximate truncated\n    eigendecomposition using randomized methods to speed up the computations.\n\n    This method is particularly fast on large matrices on which\n    you wish to extract only a small number of components. In order to\n    obtain further speed up, `n_iter` can be set <=2 (at the cost of\n    loss of precision). To increase the precision it is recommended to\n    increase `n_oversamples`, up to `2*k-n_components` where k is the\n    effective rank. Usually, `n_components` is chosen to be greater than k\n    so increasing `n_oversamples` up to `n_components` should be enough.\n\n    Strategy 'value': not implemented yet.\n    Algorithms 5.3, 5.4 and 5.5 in the Halko et al paper should provide good\n    candidates for a future implementation.\n\n    Strategy 'module':\n    The principle is that for diagonalizable matrices, the singular values and\n    eigenvalues are related: if t is an eigenvalue of A, then :math:`|t|` is a\n    singular value of A. This method relies on a randomized SVD to find the n\n    singular components corresponding to the n singular values with largest\n    modules, and then uses the signs of the singular vectors to find the true\n    sign of t: if the sign of left and right singular vectors are different\n    then the corresponding eigenvalue is negative.\n\n    Returns\n    -------\n    eigvals : 1D array of shape (n_components,) containing the `n_components`\n        eigenvalues selected (see ``selection`` parameter).\n    eigvecs : 2D array of shape (M.shape[0], n_components) containing the\n        `n_components` eigenvectors corresponding to the `eigvals`, in the\n        corresponding order. Note that this follows the `scipy.linalg.eigh`\n        convention.\n\n    See Also\n    --------\n    :func:`randomized_svd`\n\n    References\n    ----------\n    * :arxiv:`\"Finding structure with randomness:\n      Stochastic algorithms for constructing approximate matrix decompositions\"\n      (Algorithm 4.3 for strategy 'module') <0909.4061>`\n      Halko, et al. (2009)\n    \"\"\"\n    if selection == \"value\":  # pragma: no cover\n        # to do : an algorithm can be found in the Halko et al reference\n        raise NotImplementedError()\n\n    elif selection == \"module\":\n        # Note: no need for deterministic U and Vt (flip_sign=True),\n        # as we only use the dot product UVt afterwards\n        U, S, Vt = randomized_svd(\n            M,\n            n_components=n_components,\n            n_oversamples=n_oversamples,\n            n_iter=n_iter,\n            power_iteration_normalizer=power_iteration_normalizer,\n            flip_sign=False,\n            random_state=random_state,\n        )\n\n        eigvecs = U[:, :n_components]\n        eigvals = S[:n_components]\n\n        # Conversion of Singular values into Eigenvalues:\n        # For any eigenvalue t, the corresponding singular value is |t|.\n        # So if there is a negative eigenvalue t, the corresponding singular\n        # value will be -t, and the left (U) and right (V) singular vectors\n        # will have opposite signs.\n        # Fastest way: see <https://stackoverflow.com/a/61974002/7262247>\n        diag_VtU = np.einsum(\"ji,ij->j\", Vt[:n_components, :], U[:, :n_components])\n        signs = np.sign(diag_VtU)\n        eigvals = eigvals * signs\n\n    else:  # pragma: no cover\n        raise ValueError(\"Invalid `selection`: %r\" % selection)\n\n    return eigvals, eigvecs",
    "scikit-learn.sklearn.utils.extmath.svd_flip": "def svd_flip(u, v, u_based_decision=True):\n    \"\"\"Sign correction to ensure deterministic output from SVD.\n\n    Adjusts the columns of u and the rows of v such that the loadings in the\n    columns in u that are largest in absolute value are always positive.\n\n    If u_based_decision is False, then the same sign correction is applied to\n    so that the rows in v that are largest in absolute value are always\n    positive.\n\n    Parameters\n    ----------\n    u : ndarray\n        Parameters u and v are the output of `linalg.svd` or\n        :func:`~sklearn.utils.extmath.randomized_svd`, with matching inner\n        dimensions so one can compute `np.dot(u * s, v)`.\n        u can be None if `u_based_decision` is False.\n\n    v : ndarray\n        Parameters u and v are the output of `linalg.svd` or\n        :func:`~sklearn.utils.extmath.randomized_svd`, with matching inner\n        dimensions so one can compute `np.dot(u * s, v)`. The input v should\n        really be called vt to be consistent with scipy's output.\n        v can be None if `u_based_decision` is True.\n\n    u_based_decision : bool, default=True\n        If True, use the columns of u as the basis for sign flipping.\n        Otherwise, use the rows of v. The choice of which variable to base the\n        decision on is generally algorithm dependent.\n\n    Returns\n    -------\n    u_adjusted : ndarray\n        Array u with adjusted columns and the same dimensions as u.\n\n    v_adjusted : ndarray\n        Array v with adjusted rows and the same dimensions as v.\n    \"\"\"\n    xp, _ = get_namespace(*[a for a in [u, v] if a is not None])\n\n    if u_based_decision:\n        # columns of u, rows of v, or equivalently rows of u.T and v\n        max_abs_u_cols = xp.argmax(xp.abs(u.T), axis=1)\n        shift = xp.arange(u.T.shape[0], device=device(u))\n        indices = max_abs_u_cols + shift * u.T.shape[1]\n        signs = xp.sign(xp.take(xp.reshape(u.T, (-1,)), indices, axis=0))\n        u *= signs[np.newaxis, :]\n        if v is not None:\n            v *= signs[:, np.newaxis]\n    else:\n        # rows of v, columns of u\n        max_abs_v_rows = xp.argmax(xp.abs(v), axis=1)\n        shift = xp.arange(v.shape[0], device=device(v))\n        indices = max_abs_v_rows + shift * v.shape[1]\n        signs = xp.sign(xp.take(xp.reshape(v, (-1,)), indices, axis=0))\n        if u is not None:\n            u *= signs[np.newaxis, :]\n        v *= signs[:, np.newaxis]\n    return u, v",
    "scikit-learn.sklearn.utils.validation._check_psd_eigenvalues": "def _check_psd_eigenvalues(lambdas, enable_warnings=False):\n    \"\"\"Check the eigenvalues of a positive semidefinite (PSD) matrix.\n\n    Checks the provided array of PSD matrix eigenvalues for numerical or\n    conditioning issues and returns a fixed validated version. This method\n    should typically be used if the PSD matrix is user-provided (e.g. a\n    Gram matrix) or computed using a user-provided dissimilarity metric\n    (e.g. kernel function), or if the decomposition process uses approximation\n    methods (randomized SVD, etc.).\n\n    It checks for three things:\n\n    - that there are no significant imaginary parts in eigenvalues (more than\n      1e-5 times the maximum real part). If this check fails, it raises a\n      ``ValueError``. Otherwise all non-significant imaginary parts that may\n      remain are set to zero. This operation is traced with a\n      ``PositiveSpectrumWarning`` when ``enable_warnings=True``.\n\n    - that eigenvalues are not all negative. If this check fails, it raises a\n      ``ValueError``\n\n    - that there are no significant negative eigenvalues with absolute value\n      more than 1e-10 (1e-6) and more than 1e-5 (5e-3) times the largest\n      positive eigenvalue in double (simple) precision. If this check fails,\n      it raises a ``ValueError``. Otherwise all negative eigenvalues that may\n      remain are set to zero. This operation is traced with a\n      ``PositiveSpectrumWarning`` when ``enable_warnings=True``.\n\n    Finally, all the positive eigenvalues that are too small (with a value\n    smaller than the maximum eigenvalue multiplied by 1e-12 (2e-7)) are set to\n    zero. This operation is traced with a ``PositiveSpectrumWarning`` when\n    ``enable_warnings=True``.\n\n    Parameters\n    ----------\n    lambdas : array-like of shape (n_eigenvalues,)\n        Array of eigenvalues to check / fix.\n\n    enable_warnings : bool, default=False\n        When this is set to ``True``, a ``PositiveSpectrumWarning`` will be\n        raised when there are imaginary parts, negative eigenvalues, or\n        extremely small non-zero eigenvalues. Otherwise no warning will be\n        raised. In both cases, imaginary parts, negative eigenvalues, and\n        extremely small non-zero eigenvalues will be set to zero.\n\n    Returns\n    -------\n    lambdas_fixed : ndarray of shape (n_eigenvalues,)\n        A fixed validated copy of the array of eigenvalues.\n\n    Examples\n    --------\n    >>> from sklearn.utils.validation import _check_psd_eigenvalues\n    >>> _check_psd_eigenvalues([1, 2])      # nominal case\n    array([1, 2])\n    >>> _check_psd_eigenvalues([5, 5j])     # significant imag part\n    Traceback (most recent call last):\n        ...\n    ValueError: There are significant imaginary parts in eigenvalues (1\n        of the maximum real part). Either the matrix is not PSD, or there was\n        an issue while computing the eigendecomposition of the matrix.\n    >>> _check_psd_eigenvalues([5, 5e-5j])  # insignificant imag part\n    array([5., 0.])\n    >>> _check_psd_eigenvalues([-5, -1])    # all negative\n    Traceback (most recent call last):\n        ...\n    ValueError: All eigenvalues are negative (maximum is -1). Either the\n        matrix is not PSD, or there was an issue while computing the\n        eigendecomposition of the matrix.\n    >>> _check_psd_eigenvalues([5, -1])     # significant negative\n    Traceback (most recent call last):\n        ...\n    ValueError: There are significant negative eigenvalues (0.2 of the\n        maximum positive). Either the matrix is not PSD, or there was an issue\n        while computing the eigendecomposition of the matrix.\n    >>> _check_psd_eigenvalues([5, -5e-5])  # insignificant negative\n    array([5., 0.])\n    >>> _check_psd_eigenvalues([5, 4e-12])  # bad conditioning (too small)\n    array([5., 0.])\n\n    \"\"\"\n\n    lambdas = np.array(lambdas)\n    is_double_precision = lambdas.dtype == np.float64\n\n    # note: the minimum value available is\n    #  - single-precision: np.finfo('float32').eps = 1.2e-07\n    #  - double-precision: np.finfo('float64').eps = 2.2e-16\n\n    # the various thresholds used for validation\n    # we may wish to change the value according to precision.\n    significant_imag_ratio = 1e-5\n    significant_neg_ratio = 1e-5 if is_double_precision else 5e-3\n    significant_neg_value = 1e-10 if is_double_precision else 1e-6\n    small_pos_ratio = 1e-12 if is_double_precision else 2e-7\n\n    # Check that there are no significant imaginary parts\n    if not np.isreal(lambdas).all():\n        max_imag_abs = np.abs(np.imag(lambdas)).max()\n        max_real_abs = np.abs(np.real(lambdas)).max()\n        if max_imag_abs > significant_imag_ratio * max_real_abs:\n            raise ValueError(\n                \"There are significant imaginary parts in eigenvalues (%g \"\n                \"of the maximum real part). Either the matrix is not PSD, or \"\n                \"there was an issue while computing the eigendecomposition \"\n                \"of the matrix.\" % (max_imag_abs / max_real_abs)\n            )\n\n        # warn about imaginary parts being removed\n        if enable_warnings:\n            warnings.warn(\n                \"There are imaginary parts in eigenvalues (%g \"\n                \"of the maximum real part). Either the matrix is not\"\n                \" PSD, or there was an issue while computing the \"\n                \"eigendecomposition of the matrix. Only the real \"\n                \"parts will be kept.\" % (max_imag_abs / max_real_abs),\n                PositiveSpectrumWarning,\n            )\n\n    # Remove all imaginary parts (even if zero)\n    lambdas = np.real(lambdas)\n\n    # Check that there are no significant negative eigenvalues\n    max_eig = lambdas.max()\n    if max_eig < 0:\n        raise ValueError(\n            \"All eigenvalues are negative (maximum is %g). \"\n            \"Either the matrix is not PSD, or there was an \"\n            \"issue while computing the eigendecomposition of \"\n            \"the matrix.\" % max_eig\n        )\n\n    else:\n        min_eig = lambdas.min()\n        if (\n            min_eig < -significant_neg_ratio * max_eig\n            and min_eig < -significant_neg_value\n        ):\n            raise ValueError(\n                \"There are significant negative eigenvalues (%g\"\n                \" of the maximum positive). Either the matrix is \"\n                \"not PSD, or there was an issue while computing \"\n                \"the eigendecomposition of the matrix.\" % (-min_eig / max_eig)\n            )\n        elif min_eig < 0:\n            # Remove all negative values and warn about it\n            if enable_warnings:\n                warnings.warn(\n                    \"There are negative eigenvalues (%g of the \"\n                    \"maximum positive). Either the matrix is not \"\n                    \"PSD, or there was an issue while computing the\"\n                    \" eigendecomposition of the matrix. Negative \"\n                    \"eigenvalues will be replaced with 0.\" % (-min_eig / max_eig),\n                    PositiveSpectrumWarning,\n                )\n            lambdas[lambdas < 0] = 0\n\n    # Check for conditioning (small positive non-zeros)\n    too_small_lambdas = (0 < lambdas) & (lambdas < small_pos_ratio * max_eig)\n    if too_small_lambdas.any():\n        if enable_warnings:\n            warnings.warn(\n                \"Badly conditioned PSD matrix spectrum: the largest \"\n                \"eigenvalue is more than %g times the smallest. \"\n                \"Small eigenvalues will be replaced with 0.\"\n                \"\" % (1 / small_pos_ratio),\n                PositiveSpectrumWarning,\n            )\n        lambdas[too_small_lambdas] = 0\n\n    return lambdas"
}