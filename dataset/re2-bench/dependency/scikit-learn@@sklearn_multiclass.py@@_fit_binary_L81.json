{
    "scikit-learn.sklearn.base.wrapper": "@functools.wraps(fit_method)\ndef wrapper(estimator, *args, **kwargs):\n    global_skip_validation = get_config()[\"skip_parameter_validation\"]\n\n    # we don't want to validate again for each call to partial_fit\n    partial_fit_and_fitted = (\n        fit_method.__name__ == \"partial_fit\" and _is_fitted(estimator)\n    )\n\n    if not global_skip_validation and not partial_fit_and_fitted:\n        estimator._validate_params()\n\n    with config_context(\n        skip_parameter_validation=(\n            prefer_skip_nested_validation or global_skip_validation\n        )\n    ):\n        return fit_method(estimator, *args, **kwargs)",
    "scikit-learn.sklearn.base.clone": "def clone(estimator, *, safe=True):\n    \"\"\"Construct a new unfitted estimator with the same parameters.\n\n    Clone does a deep copy of the model in an estimator\n    without actually copying attached data. It returns a new estimator\n    with the same parameters that has not been fitted on any data.\n\n    .. versionchanged:: 1.3\n        Delegates to `estimator.__sklearn_clone__` if the method exists.\n\n    Parameters\n    ----------\n    estimator : {list, tuple, set} of estimator instance or a single \\\n            estimator instance\n        The estimator or group of estimators to be cloned.\n    safe : bool, default=True\n        If safe is False, clone will fall back to a deep copy on objects\n        that are not estimators. Ignored if `estimator.__sklearn_clone__`\n        exists.\n\n    Returns\n    -------\n    estimator : object\n        The deep copy of the input, an estimator if input is an estimator.\n\n    Notes\n    -----\n    If the estimator's `random_state` parameter is an integer (or if the\n    estimator doesn't have a `random_state` parameter), an *exact clone* is\n    returned: the clone and the original estimator will give the exact same\n    results. Otherwise, *statistical clone* is returned: the clone might\n    return different results from the original estimator. More details can be\n    found in :ref:`randomness`.\n\n    Examples\n    --------\n    >>> from sklearn.base import clone\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> X = [[-1, 0], [0, 1], [0, -1], [1, 0]]\n    >>> y = [0, 0, 1, 1]\n    >>> classifier = LogisticRegression().fit(X, y)\n    >>> cloned_classifier = clone(classifier)\n    >>> hasattr(classifier, \"classes_\")\n    True\n    >>> hasattr(cloned_classifier, \"classes_\")\n    False\n    >>> classifier is cloned_classifier\n    False\n    \"\"\"\n    if hasattr(estimator, \"__sklearn_clone__\") and not inspect.isclass(estimator):\n        return estimator.__sklearn_clone__()\n    return _clone_parametrized(estimator, safe=safe)",
    "scikit-learn.sklearn.gaussian_process._gpc.fit": "def fit(self, X, y):\n    \"\"\"Fit Gaussian process classification model.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features) or list of object\n        Feature vectors or other representations of training data.\n\n    y : array-like of shape (n_samples,)\n        Target values, must be binary.\n\n    Returns\n    -------\n    self : returns an instance of self.\n    \"\"\"\n    if self.kernel is None:  # Use an RBF kernel as default\n        self.kernel_ = C(1.0, constant_value_bounds=\"fixed\") * RBF(\n            1.0, length_scale_bounds=\"fixed\"\n        )\n    else:\n        self.kernel_ = clone(self.kernel)\n\n    self.rng = check_random_state(self.random_state)\n\n    self.X_train_ = np.copy(X) if self.copy_X_train else X\n\n    # Encode class labels and check that it is a binary classification\n    # problem\n    label_encoder = LabelEncoder()\n    self.y_train_ = label_encoder.fit_transform(y)\n    self.classes_ = label_encoder.classes_\n    if self.classes_.size > 2:\n        raise ValueError(\n            \"%s supports only binary classification. y contains classes %s\"\n            % (self.__class__.__name__, self.classes_)\n        )\n    elif self.classes_.size == 1:\n        raise ValueError(\n            \"{0:s} requires 2 classes; got {1:d} class\".format(\n                self.__class__.__name__, self.classes_.size\n            )\n        )\n\n    if self.optimizer is not None and self.kernel_.n_dims > 0:\n        # Choose hyperparameters based on maximizing the log-marginal\n        # likelihood (potentially starting from several initial values)\n        def obj_func(theta, eval_gradient=True):\n            if eval_gradient:\n                lml, grad = self.log_marginal_likelihood(\n                    theta, eval_gradient=True, clone_kernel=False\n                )\n                return -lml, -grad\n            else:\n                return -self.log_marginal_likelihood(theta, clone_kernel=False)\n\n        # First optimize starting from theta specified in kernel\n        optima = [\n            self._constrained_optimization(\n                obj_func, self.kernel_.theta, self.kernel_.bounds\n            )\n        ]\n\n        # Additional runs are performed from log-uniform chosen initial\n        # theta\n        if self.n_restarts_optimizer > 0:\n            if not np.isfinite(self.kernel_.bounds).all():\n                raise ValueError(\n                    \"Multiple optimizer restarts (n_restarts_optimizer>0) \"\n                    \"requires that all bounds are finite.\"\n                )\n            bounds = self.kernel_.bounds\n            for iteration in range(self.n_restarts_optimizer):\n                theta_initial = np.exp(self.rng.uniform(bounds[:, 0], bounds[:, 1]))\n                optima.append(\n                    self._constrained_optimization(obj_func, theta_initial, bounds)\n                )\n        # Select result from run with minimal (negative) log-marginal\n        # likelihood\n        lml_values = list(map(itemgetter(1), optima))\n        self.kernel_.theta = optima[np.argmin(lml_values)][0]\n        self.kernel_._check_bounds_params()\n\n        self.log_marginal_likelihood_value_ = -np.min(lml_values)\n    else:\n        self.log_marginal_likelihood_value_ = self.log_marginal_likelihood(\n            self.kernel_.theta\n        )\n\n    # Precompute quantities required for predictions which are independent\n    # of actual query points\n    K = self.kernel_(self.X_train_)\n\n    _, (self.pi_, self.W_sr_, self.L_, _, _) = self._posterior_mode(\n        K, return_temporaries=True\n    )\n\n    return self",
    "scikit-learn.sklearn.multiclass.fit": "def fit(self, X, y):\n    check_params = dict(\n        ensure_all_finite=False, dtype=None, ensure_2d=False, accept_sparse=True\n    )\n    validate_data(\n        self, X, y, reset=True, validate_separately=(check_params, check_params)\n    )\n    self.y_ = y\n    return self",
    "scikit-learn.sklearn.utils._mocking.fit": "def fit(self, X, y, sample_weight=None, **fit_params):\n    \"\"\"Fit classifier.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Training vector, where `n_samples` is the number of samples and\n        `n_features` is the number of features.\n\n    y : array-like of shape (n_samples, n_outputs) or (n_samples,), \\\n            default=None\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights. If None, then samples are equally weighted.\n\n    **fit_params : dict of string -> object\n        Parameters passed to the ``fit`` method of the estimator\n\n    Returns\n    -------\n    self\n    \"\"\"\n    assert _num_samples(X) == _num_samples(y)\n    if self.methods_to_check == \"all\" or \"fit\" in self.methods_to_check:\n        X, y = self._check_X_y(X, y, should_be_fitted=False)\n    self.n_features_in_ = np.shape(X)[1]\n    self.classes_ = np.unique(check_array(y, ensure_2d=False, allow_nd=True))\n    if self.expected_fit_params:\n        missing = set(self.expected_fit_params) - set(fit_params)\n        if missing:\n            raise AssertionError(\n                f\"Expected fit parameter(s) {list(missing)} not seen.\"\n            )\n        for key, value in fit_params.items():\n            if _num_samples(value) != _num_samples(X):\n                raise AssertionError(\n                    f\"Fit parameter {key} has length {_num_samples(value)}\"\n                    f\"; expected {_num_samples(X)}.\"\n                )\n    if self.expected_sample_weight:\n        if sample_weight is None:\n            raise AssertionError(\"Expected sample_weight to be passed\")\n        _check_sample_weight(sample_weight, X)\n\n    return self"
}