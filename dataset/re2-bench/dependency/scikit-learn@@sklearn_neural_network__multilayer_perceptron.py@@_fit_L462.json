{
    "scikit-learn.sklearn.neural_network._multilayer_perceptron._validate_input": "def _validate_input(self, X, y, incremental, reset):\n    X, y = validate_data(\n        self,\n        X,\n        y,\n        accept_sparse=[\"csr\", \"csc\"],\n        multi_output=True,\n        y_numeric=True,\n        dtype=(np.float64, np.float32),\n        reset=reset,\n    )\n    if y.ndim == 2 and y.shape[1] == 1:\n        y = column_or_1d(y, warn=True)\n    return X, y",
    "scikit-learn.sklearn.neural_network._multilayer_perceptron._initialize": "def _initialize(self, y, layer_units, dtype):\n    # set all attributes, allocate weights etc. for first call\n    # Initialize parameters\n    self.n_iter_ = 0\n    self.t_ = 0\n    self.n_outputs_ = y.shape[1]\n\n    # Compute the number of layers\n    self.n_layers_ = len(layer_units)\n\n    # Output for regression\n    if not is_classifier(self):\n        if self.loss == \"poisson\":\n            self.out_activation_ = \"exp\"\n        else:\n            # loss = \"squared_error\"\n            self.out_activation_ = \"identity\"\n    # Output for multi class\n    elif self._label_binarizer.y_type_ == \"multiclass\":\n        self.out_activation_ = \"softmax\"\n    # Output for binary class and multi-label\n    else:\n        self.out_activation_ = \"logistic\"\n\n    # Initialize coefficient and intercept layers\n    self.coefs_ = []\n    self.intercepts_ = []\n\n    for i in range(self.n_layers_ - 1):\n        coef_init, intercept_init = self._init_coef(\n            layer_units[i], layer_units[i + 1], dtype\n        )\n        self.coefs_.append(coef_init)\n        self.intercepts_.append(intercept_init)\n\n    self._best_coefs = [c.copy() for c in self.coefs_]\n    self._best_intercepts = [i.copy() for i in self.intercepts_]\n\n    if self.solver in _STOCHASTIC_SOLVERS:\n        self.loss_curve_ = []\n        self._no_improvement_count = 0\n        if self.early_stopping:\n            self.validation_scores_ = []\n            self.best_validation_score_ = -np.inf\n            self.best_loss_ = None\n        else:\n            self.best_loss_ = np.inf\n            self.validation_scores_ = None\n            self.best_validation_score_ = None",
    "scikit-learn.sklearn.neural_network._multilayer_perceptron.<listcomp>": "intercept_grads = [\n    np.empty(n_fan_out_, dtype=X.dtype) for n_fan_out_ in layer_units[1:]\n",
    "scikit-learn.sklearn.neural_network._multilayer_perceptron.<genexpr>": "if not all(np.isfinite(w).all() for w in weights):\n    raise ValueError(\n        \"Solver produced non-finite parameter weights. The input data may\"\n        \" contain large values and need to be preprocessed.\"\n    )\n\n",
    "scikit-learn.sklearn.neural_network._multilayer_perceptron._fit_lbfgs": "def _fit_lbfgs(\n    self,\n    X,\n    y,\n    sample_weight,\n    activations,\n    deltas,\n    coef_grads,\n    intercept_grads,\n    layer_units,\n):\n    # Store meta information for the parameters\n    self._coef_indptr = []\n    self._intercept_indptr = []\n    start = 0\n\n    # Save sizes and indices of coefficients for faster unpacking\n    for i in range(self.n_layers_ - 1):\n        n_fan_in, n_fan_out = layer_units[i], layer_units[i + 1]\n\n        end = start + (n_fan_in * n_fan_out)\n        self._coef_indptr.append((start, end, (n_fan_in, n_fan_out)))\n        start = end\n\n    # Save sizes and indices of intercepts for faster unpacking\n    for i in range(self.n_layers_ - 1):\n        end = start + layer_units[i + 1]\n        self._intercept_indptr.append((start, end))\n        start = end\n\n    # Run LBFGS\n    packed_coef_inter = _pack(self.coefs_, self.intercepts_)\n\n    if self.verbose is True or self.verbose >= 1:\n        iprint = 1\n    else:\n        iprint = -1\n\n    opt_res = scipy.optimize.minimize(\n        self._loss_grad_lbfgs,\n        packed_coef_inter,\n        method=\"L-BFGS-B\",\n        jac=True,\n        options={\n            \"maxfun\": self.max_fun,\n            \"maxiter\": self.max_iter,\n            \"gtol\": self.tol,\n            **_get_additional_lbfgs_options_dict(\"iprint\", iprint),\n        },\n        args=(\n            X,\n            y,\n            sample_weight,\n            activations,\n            deltas,\n            coef_grads,\n            intercept_grads,\n        ),\n    )\n    self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n    self.loss_ = opt_res.fun\n    self._unpack(opt_res.x)",
    "scikit-learn.sklearn.neural_network._multilayer_perceptron._fit_stochastic": "def _fit_stochastic(\n    self,\n    X,\n    y,\n    sample_weight,\n    activations,\n    deltas,\n    coef_grads,\n    intercept_grads,\n    layer_units,\n    incremental,\n):\n    params = self.coefs_ + self.intercepts_\n    if not incremental or not hasattr(self, \"_optimizer\"):\n        if self.solver == \"sgd\":\n            self._optimizer = SGDOptimizer(\n                params,\n                self.learning_rate_init,\n                self.learning_rate,\n                self.momentum,\n                self.nesterovs_momentum,\n                self.power_t,\n            )\n        elif self.solver == \"adam\":\n            self._optimizer = AdamOptimizer(\n                params,\n                self.learning_rate_init,\n                self.beta_1,\n                self.beta_2,\n                self.epsilon,\n            )\n\n    # early_stopping in partial_fit doesn't make sense\n    if self.early_stopping and incremental:\n        raise ValueError(\"partial_fit does not support early_stopping=True\")\n    early_stopping = self.early_stopping\n    if early_stopping:\n        # don't stratify in multilabel classification\n        should_stratify = is_classifier(self) and self.n_outputs_ == 1\n        stratify = y if should_stratify else None\n        if sample_weight is None:\n            X_train, X_val, y_train, y_val = train_test_split(\n                X,\n                y,\n                random_state=self._random_state,\n                test_size=self.validation_fraction,\n                stratify=stratify,\n            )\n            sample_weight_train = sample_weight_val = None\n        else:\n            # TODO: incorporate sample_weight in sampling here.\n            (\n                X_train,\n                X_val,\n                y_train,\n                y_val,\n                sample_weight_train,\n                sample_weight_val,\n            ) = train_test_split(\n                X,\n                y,\n                sample_weight,\n                random_state=self._random_state,\n                test_size=self.validation_fraction,\n                stratify=stratify,\n            )\n        if X_val.shape[0] < 2:\n            raise ValueError(\n                \"The validation set is too small. Increase 'validation_fraction' \"\n                \"or the size of your dataset.\"\n            )\n\n        if is_classifier(self):\n            y_val = self._label_binarizer.inverse_transform(y_val)\n    else:\n        X_train, y_train, sample_weight_train = X, y, sample_weight\n        X_val = y_val = sample_weight_val = None\n\n    n_samples = X_train.shape[0]\n    sample_idx = np.arange(n_samples, dtype=int)\n\n    if self.batch_size == \"auto\":\n        batch_size = min(200, n_samples)\n    else:\n        if self.batch_size > n_samples:\n            warnings.warn(\n                \"Got `batch_size` less than 1 or larger than \"\n                \"sample size. It is going to be clipped\"\n            )\n        batch_size = np.clip(self.batch_size, 1, n_samples)\n\n    try:\n        self.n_iter_ = 0\n        for it in range(self.max_iter):\n            if self.shuffle:\n                # Only shuffle the sample indices instead of X and y to\n                # reduce the memory footprint. These indices will be used\n                # to slice the X and y.\n                sample_idx = shuffle(sample_idx, random_state=self._random_state)\n\n            accumulated_loss = 0.0\n            for batch_slice in gen_batches(n_samples, batch_size):\n                if self.shuffle:\n                    batch_idx = sample_idx[batch_slice]\n                    X_batch = _safe_indexing(X_train, batch_idx)\n                else:\n                    batch_idx = batch_slice\n                    X_batch = X_train[batch_idx]\n                y_batch = y_train[batch_idx]\n                if sample_weight is None:\n                    sample_weight_batch = None\n                else:\n                    sample_weight_batch = sample_weight_train[batch_idx]\n\n                activations[0] = X_batch\n                batch_loss, coef_grads, intercept_grads = self._backprop(\n                    X_batch,\n                    y_batch,\n                    sample_weight_batch,\n                    activations,\n                    deltas,\n                    coef_grads,\n                    intercept_grads,\n                )\n                accumulated_loss += batch_loss * (\n                    batch_slice.stop - batch_slice.start\n                )\n\n                # update weights\n                grads = coef_grads + intercept_grads\n                self._optimizer.update_params(params, grads)\n\n            self.n_iter_ += 1\n            self.loss_ = accumulated_loss / X_train.shape[0]\n\n            self.t_ += n_samples\n            self.loss_curve_.append(self.loss_)\n            if self.verbose:\n                print(\"Iteration %d, loss = %.8f\" % (self.n_iter_, self.loss_))\n\n            # update no_improvement_count based on training loss or\n            # validation score according to early_stopping\n            self._update_no_improvement_count(\n                early_stopping, X_val, y_val, sample_weight_val\n            )\n\n            # for learning rate that needs to be updated at iteration end\n            self._optimizer.iteration_ends(self.t_)\n\n            if self._no_improvement_count > self.n_iter_no_change:\n                # not better than last `n_iter_no_change` iterations by tol\n                # stop or decrease learning rate\n                if early_stopping:\n                    msg = (\n                        \"Validation score did not improve more than \"\n                        \"tol=%f for %d consecutive epochs.\"\n                        % (self.tol, self.n_iter_no_change)\n                    )\n                else:\n                    msg = (\n                        \"Training loss did not improve more than tol=%f\"\n                        \" for %d consecutive epochs.\"\n                        % (self.tol, self.n_iter_no_change)\n                    )\n\n                is_stopping = self._optimizer.trigger_stopping(msg, self.verbose)\n                if is_stopping:\n                    break\n                else:\n                    self._no_improvement_count = 0\n\n            if incremental:\n                break\n\n            if self.n_iter_ == self.max_iter:\n                warnings.warn(\n                    \"Stochastic Optimizer: Maximum iterations (%d) \"\n                    \"reached and the optimization hasn't converged yet.\"\n                    % self.max_iter,\n                    ConvergenceWarning,\n                )\n    except KeyboardInterrupt:\n        warnings.warn(\"Training interrupted by user.\")\n\n    if early_stopping:\n        # restore best weights\n        self.coefs_ = self._best_coefs\n        self.intercepts_ = self._best_intercepts",
    "scikit-learn.sklearn.utils.validation.check_random_state": "def check_random_state(seed):\n    \"\"\"Turn seed into an np.random.RandomState instance.\n\n    Parameters\n    ----------\n    seed : None, int or instance of RandomState\n        If seed is None, return the RandomState singleton used by np.random.\n        If seed is an int, return a new RandomState instance seeded with seed.\n        If seed is already a RandomState instance, return it.\n        Otherwise raise ValueError.\n\n    Returns\n    -------\n    :class:`numpy:numpy.random.RandomState`\n        The random state object based on `seed` parameter.\n\n    Examples\n    --------\n    >>> from sklearn.utils.validation import check_random_state\n    >>> check_random_state(42)\n    RandomState(MT19937) at 0x...\n    \"\"\"\n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, numbers.Integral):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState):\n        return seed\n    raise ValueError(\n        \"%r cannot be used to seed a numpy.random.RandomState instance\" % seed\n    )",
    "scikit-learn.sklearn.utils.validation._check_sample_weight": "def _check_sample_weight(\n    sample_weight,\n    X,\n    *,\n    dtype=None,\n    force_float_dtype=True,\n    ensure_non_negative=False,\n    ensure_same_device=True,\n    copy=False,\n):\n    \"\"\"Validate sample weights.\n\n    Note that passing sample_weight=None will output an array of ones.\n    Therefore, in some cases, you may want to protect the call with:\n    if sample_weight is not None:\n        sample_weight = _check_sample_weight(...)\n\n    Parameters\n    ----------\n    sample_weight : {ndarray, Number or None}, shape (n_samples,)\n        Input sample weights.\n\n    X : {ndarray, list, sparse matrix}\n        Input data.\n\n    dtype : dtype, default=None\n        dtype of the validated `sample_weight`.\n        If None, and `sample_weight` is an array:\n\n            - If `sample_weight.dtype` is one of `{np.float64, np.float32}`,\n              then the dtype is preserved.\n            - Else the output has NumPy's default dtype: `np.float64`.\n\n        If `dtype` is not `{np.float32, np.float64, None}`, then output will\n        be `np.float64`.\n\n    force_float_dtype : bool, default=True\n        Whether `X` should be forced to be float dtype, when `dtype` is a non-float\n        dtype or None.\n\n    ensure_non_negative : bool, default=False,\n        Whether or not the weights are expected to be non-negative.\n\n        .. versionadded:: 1.0\n\n    ensure_same_device : bool, default=True\n        Whether `sample_weight` should be forced to be on the same device as `X`.\n\n    copy : bool, default=False\n        If True, a copy of sample_weight will be created.\n\n    Returns\n    -------\n    sample_weight : ndarray of shape (n_samples,)\n        Validated sample weight. It is guaranteed to be \"C\" contiguous.\n    \"\"\"\n    xp, is_array_api, device = get_namespace_and_device(X, remove_types=(int, float))\n\n    n_samples = _num_samples(X)\n\n    max_float_type = _max_precision_float_dtype(xp, device)\n    float_dtypes = (\n        [xp.float32] if max_float_type == xp.float32 else [xp.float64, xp.float32]\n    )\n    if force_float_dtype and dtype is not None and dtype not in float_dtypes:\n        dtype = max_float_type\n\n    if sample_weight is None:\n        sample_weight = xp.ones(n_samples, dtype=dtype, device=device)\n    elif isinstance(sample_weight, numbers.Number):\n        sample_weight = xp.full(n_samples, sample_weight, dtype=dtype, device=device)\n    else:\n        if force_float_dtype and dtype is None:\n            dtype = float_dtypes\n        if is_array_api and ensure_same_device:\n            sample_weight = xp.asarray(sample_weight, device=device)\n        sample_weight = check_array(\n            sample_weight,\n            accept_sparse=False,\n            ensure_2d=False,\n            dtype=dtype,\n            order=\"C\",\n            copy=copy,\n            input_name=\"sample_weight\",\n        )\n        if sample_weight.ndim != 1:\n            raise ValueError(\n                f\"Sample weights must be 1D array or scalar, got \"\n                f\"{sample_weight.ndim}D array. Expected either a scalar value \"\n                f\"or a 1D array of length {n_samples}.\"\n            )\n\n        if sample_weight.shape != (n_samples,):\n            raise ValueError(\n                \"sample_weight.shape == {}, expected {}!\".format(\n                    sample_weight.shape, (n_samples,)\n                )\n            )\n\n    if ensure_non_negative:\n        check_non_negative(sample_weight, \"`sample_weight`\")\n\n    return sample_weight"
}