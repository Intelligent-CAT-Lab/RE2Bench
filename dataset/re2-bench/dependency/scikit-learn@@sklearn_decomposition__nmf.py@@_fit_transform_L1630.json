{
    "scikit-learn.sklearn.decomposition._nmf._check_w_h": "def _check_w_h(self, X, W, H, update_H):\n    \"\"\"Check W and H, or initialize them.\"\"\"\n    n_samples, n_features = X.shape\n\n    if self.init == \"custom\" and update_H:\n        _check_init(H, (self._n_components, n_features), \"NMF (input H)\")\n        _check_init(W, (n_samples, self._n_components), \"NMF (input W)\")\n        if self._n_components == \"auto\":\n            self._n_components = H.shape[0]\n\n        if H.dtype != X.dtype or W.dtype != X.dtype:\n            raise TypeError(\n                \"H and W should have the same dtype as X. Got \"\n                \"H.dtype = {} and W.dtype = {}.\".format(H.dtype, W.dtype)\n            )\n\n    elif not update_H:\n        if W is not None:\n            warnings.warn(\n                \"When update_H=False, the provided initial W is not used.\",\n                RuntimeWarning,\n            )\n\n        _check_init(H, (self._n_components, n_features), \"NMF (input H)\")\n        if self._n_components == \"auto\":\n            self._n_components = H.shape[0]\n\n        if H.dtype != X.dtype:\n            raise TypeError(\n                \"H should have the same dtype as X. Got H.dtype = {}.\".format(\n                    H.dtype\n                )\n            )\n\n        # 'mu' solver should not be initialized by zeros\n        if self.solver == \"mu\":\n            avg = np.sqrt(X.mean() / self._n_components)\n            W = np.full((n_samples, self._n_components), avg, dtype=X.dtype)\n        else:\n            W = np.zeros((n_samples, self._n_components), dtype=X.dtype)\n\n    else:\n        if W is not None or H is not None:\n            warnings.warn(\n                (\n                    \"When init!='custom', provided W or H are ignored. Set \"\n                    \" init='custom' to use them as initialization.\"\n                ),\n                RuntimeWarning,\n            )\n\n        if self._n_components == \"auto\":\n            self._n_components = X.shape[1]\n\n        W, H = _initialize_nmf(\n            X, self._n_components, init=self.init, random_state=self.random_state\n        )\n\n    return W, H",
    "scikit-learn.sklearn.decomposition._nmf._compute_regularization": "def _compute_regularization(self, X):\n    \"\"\"Compute scaled regularization terms.\"\"\"\n    n_samples, n_features = X.shape\n    alpha_W = self.alpha_W\n    alpha_H = self.alpha_W if self.alpha_H == \"same\" else self.alpha_H\n\n    l1_reg_W = n_features * alpha_W * self.l1_ratio\n    l1_reg_H = n_samples * alpha_H * self.l1_ratio\n    l2_reg_W = n_features * alpha_W * (1.0 - self.l1_ratio)\n    l2_reg_H = n_samples * alpha_H * (1.0 - self.l1_ratio)\n\n    return l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H",
    "scikit-learn.sklearn.decomposition._nmf._check_params": "def _check_params(self, X):\n    super()._check_params(X)\n\n    # solver\n    if self.solver != \"mu\" and self.beta_loss not in (2, \"frobenius\"):\n        # 'mu' is the only solver that handles other beta losses than 'frobenius'\n        raise ValueError(\n            f\"Invalid beta_loss parameter: solver {self.solver!r} does not handle \"\n            f\"beta_loss = {self.beta_loss!r}\"\n        )\n    if self.solver == \"mu\" and self.init == \"nndsvd\":\n        warnings.warn(\n            (\n                \"The multiplicative update ('mu') solver cannot update \"\n                \"zeros present in the initialization, and so leads to \"\n                \"poorer results when used jointly with init='nndsvd'. \"\n                \"You may try init='nndsvda' or init='nndsvdar' instead.\"\n            ),\n            UserWarning,\n        )\n\n    return self",
    "scikit-learn.sklearn.decomposition._nmf._fit_coordinate_descent": "def _fit_coordinate_descent(\n    X,\n    W,\n    H,\n    tol=1e-4,\n    max_iter=200,\n    l1_reg_W=0,\n    l1_reg_H=0,\n    l2_reg_W=0,\n    l2_reg_H=0,\n    update_H=True,\n    verbose=0,\n    shuffle=False,\n    random_state=None,\n):\n    \"\"\"Compute Non-negative Matrix Factorization (NMF) with Coordinate Descent\n\n    The objective function is minimized with an alternating minimization of W\n    and H. Each minimization is done with a cyclic (up to a permutation of the\n    features) Coordinate Descent.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Constant matrix.\n\n    W : array-like of shape (n_samples, n_components)\n        Initial guess for the solution.\n\n    H : array-like of shape (n_components, n_features)\n        Initial guess for the solution.\n\n    tol : float, default=1e-4\n        Tolerance of the stopping condition.\n\n    max_iter : int, default=200\n        Maximum number of iterations before timing out.\n\n    l1_reg_W : float, default=0.\n        L1 regularization parameter for W.\n\n    l1_reg_H : float, default=0.\n        L1 regularization parameter for H.\n\n    l2_reg_W : float, default=0.\n        L2 regularization parameter for W.\n\n    l2_reg_H : float, default=0.\n        L2 regularization parameter for H.\n\n    update_H : bool, default=True\n        Set to True, both W and H will be estimated from initial guesses.\n        Set to False, only W will be estimated.\n\n    verbose : int, default=0\n        The verbosity level.\n\n    shuffle : bool, default=False\n        If true, randomize the order of coordinates in the CD solver.\n\n    random_state : int, RandomState instance or None, default=None\n        Used to randomize the coordinates in the CD solver, when\n        ``shuffle`` is set to ``True``. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    W : ndarray of shape (n_samples, n_components)\n        Solution to the non-negative least squares problem.\n\n    H : ndarray of shape (n_components, n_features)\n        Solution to the non-negative least squares problem.\n\n    n_iter : int\n        The number of iterations done by the algorithm.\n\n    References\n    ----------\n    .. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n       factorizations\" <10.1587/transfun.E92.A.708>`\n       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n       of electronics, communications and computer sciences 92.3: 708-721, 2009.\n    \"\"\"\n    # so W and Ht are both in C order in memory\n    Ht = check_array(H.T, order=\"C\")\n    X = check_array(X, accept_sparse=\"csr\")\n\n    rng = check_random_state(random_state)\n\n    for n_iter in range(1, max_iter + 1):\n        violation = 0.0\n\n        # Update W\n        violation += _update_coordinate_descent(\n            X, W, Ht, l1_reg_W, l2_reg_W, shuffle, rng\n        )\n        # Update H\n        if update_H:\n            violation += _update_coordinate_descent(\n                X.T, Ht, W, l1_reg_H, l2_reg_H, shuffle, rng\n            )\n\n        if n_iter == 1:\n            violation_init = violation\n\n        if violation_init == 0:\n            break\n\n        if verbose:\n            print(\"violation:\", violation / violation_init)\n\n        if violation / violation_init <= tol:\n            if verbose:\n                print(\"Converged at iteration\", n_iter + 1)\n            break\n\n    return W, Ht.T, n_iter",
    "scikit-learn.sklearn.decomposition._nmf._fit_multiplicative_update": "def _fit_multiplicative_update(\n    X,\n    W,\n    H,\n    beta_loss=\"frobenius\",\n    max_iter=200,\n    tol=1e-4,\n    l1_reg_W=0,\n    l1_reg_H=0,\n    l2_reg_W=0,\n    l2_reg_H=0,\n    update_H=True,\n    verbose=0,\n):\n    \"\"\"Compute Non-negative Matrix Factorization with Multiplicative Update.\n\n    The objective function is _beta_divergence(X, WH) and is minimized with an\n    alternating minimization of W and H. Each minimization is done with a\n    Multiplicative Update.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Constant input matrix.\n\n    W : array-like of shape (n_samples, n_components)\n        Initial guess for the solution.\n\n    H : array-like of shape (n_components, n_features)\n        Initial guess for the solution.\n\n    beta_loss : float or {'frobenius', 'kullback-leibler', \\\n            'itakura-saito'}, default='frobenius'\n        String must be in {'frobenius', 'kullback-leibler', 'itakura-saito'}.\n        Beta divergence to be minimized, measuring the distance between X\n        and the dot product WH. Note that values different from 'frobenius'\n        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n        fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n        matrix X cannot contain zeros.\n\n    max_iter : int, default=200\n        Number of iterations.\n\n    tol : float, default=1e-4\n        Tolerance of the stopping condition.\n\n    l1_reg_W : float, default=0.\n        L1 regularization parameter for W.\n\n    l1_reg_H : float, default=0.\n        L1 regularization parameter for H.\n\n    l2_reg_W : float, default=0.\n        L2 regularization parameter for W.\n\n    l2_reg_H : float, default=0.\n        L2 regularization parameter for H.\n\n    update_H : bool, default=True\n        Set to True, both W and H will be estimated from initial guesses.\n        Set to False, only W will be estimated.\n\n    verbose : int, default=0\n        The verbosity level.\n\n    Returns\n    -------\n    W : ndarray of shape (n_samples, n_components)\n        Solution to the non-negative least squares problem.\n\n    H : ndarray of shape (n_components, n_features)\n        Solution to the non-negative least squares problem.\n\n    n_iter : int\n        The number of iterations done by the algorithm.\n\n    References\n    ----------\n    Lee, D. D., & Seung, H., S. (2001). Algorithms for Non-negative Matrix\n    Factorization. Adv. Neural Inform. Process. Syst.. 13.\n    Fevotte, C., & Idier, J. (2011). Algorithms for nonnegative matrix\n    factorization with the beta-divergence. Neural Computation, 23(9).\n    \"\"\"\n    start_time = time.time()\n\n    beta_loss = _beta_loss_to_float(beta_loss)\n\n    # gamma for Maximization-Minimization (MM) algorithm [Fevotte 2011]\n    if beta_loss < 1:\n        gamma = 1.0 / (2.0 - beta_loss)\n    elif beta_loss > 2:\n        gamma = 1.0 / (beta_loss - 1.0)\n    else:\n        gamma = 1.0\n\n    # used for the convergence criterion\n    error_at_init = _beta_divergence(X, W, H, beta_loss, square_root=True)\n    previous_error = error_at_init\n\n    H_sum, HHt, XHt = None, None, None\n    for n_iter in range(1, max_iter + 1):\n        # update W\n        # H_sum, HHt and XHt are saved and reused if not update_H\n        W, H_sum, HHt, XHt = _multiplicative_update_w(\n            X,\n            W,\n            H,\n            beta_loss=beta_loss,\n            l1_reg_W=l1_reg_W,\n            l2_reg_W=l2_reg_W,\n            gamma=gamma,\n            H_sum=H_sum,\n            HHt=HHt,\n            XHt=XHt,\n            update_H=update_H,\n        )\n\n        # necessary for stability with beta_loss < 1\n        if beta_loss < 1:\n            W[W < np.finfo(np.float64).eps] = 0.0\n\n        # update H (only at fit or fit_transform)\n        if update_H:\n            H = _multiplicative_update_h(\n                X,\n                W,\n                H,\n                beta_loss=beta_loss,\n                l1_reg_H=l1_reg_H,\n                l2_reg_H=l2_reg_H,\n                gamma=gamma,\n            )\n\n            # These values will be recomputed since H changed\n            H_sum, HHt, XHt = None, None, None\n\n            # necessary for stability with beta_loss < 1\n            if beta_loss <= 1:\n                H[H < np.finfo(np.float64).eps] = 0.0\n\n        # test convergence criterion every 10 iterations\n        if tol > 0 and n_iter % 10 == 0:\n            error = _beta_divergence(X, W, H, beta_loss, square_root=True)\n\n            if verbose:\n                iter_time = time.time()\n                print(\n                    \"Epoch %02d reached after %.3f seconds, error: %f\"\n                    % (n_iter, iter_time - start_time, error)\n                )\n\n            if (previous_error - error) / error_at_init < tol:\n                break\n            previous_error = error\n\n    # do not print if we have already printed in the convergence test\n    if verbose and (tol == 0 or n_iter % 10 != 0):\n        end_time = time.time()\n        print(\n            \"Epoch %02d reached after %.3f seconds.\" % (n_iter, end_time - start_time)\n        )\n\n    return W, H, n_iter"
}