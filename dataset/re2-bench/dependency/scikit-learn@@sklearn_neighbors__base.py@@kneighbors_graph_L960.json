{
    "scikit-learn.sklearn.neighbors._base.kneighbors": "def kneighbors(self, X=None, n_neighbors=None, return_distance=True):\n    \"\"\"Find the K-neighbors of a point.\n\n    Returns indices of and distances to the neighbors of each point.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}, shape (n_queries, n_features), \\\n        or (n_queries, n_indexed) if metric == 'precomputed', default=None\n        The query point or points.\n        If not provided, neighbors of each indexed point are returned.\n        In this case, the query point is not considered its own neighbor.\n\n    n_neighbors : int, default=None\n        Number of neighbors required for each sample. The default is the\n        value passed to the constructor.\n\n    return_distance : bool, default=True\n        Whether or not to return the distances.\n\n    Returns\n    -------\n    neigh_dist : ndarray of shape (n_queries, n_neighbors)\n        Array representing the lengths to points, only present if\n        return_distance=True.\n\n    neigh_ind : ndarray of shape (n_queries, n_neighbors)\n        Indices of the nearest points in the population matrix.\n\n    Examples\n    --------\n    In the following example, we construct a NearestNeighbors\n    class from an array representing our data set and ask who's\n    the closest point to [1,1,1]\n\n    >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n    >>> from sklearn.neighbors import NearestNeighbors\n    >>> neigh = NearestNeighbors(n_neighbors=1)\n    >>> neigh.fit(samples)\n    NearestNeighbors(n_neighbors=1)\n    >>> print(neigh.kneighbors([[1., 1., 1.]]))\n    (array([[0.5]]), array([[2]]))\n\n    As you can see, it returns [[0.5]], and [[2]], which means that the\n    element is at distance 0.5 and is the third element of samples\n    (indexes start at 0). You can also query for multiple points:\n\n    >>> X = [[0., 1., 0.], [1., 0., 1.]]\n    >>> neigh.kneighbors(X, return_distance=False)\n    array([[1],\n           [2]]...)\n    \"\"\"\n    check_is_fitted(self)\n\n    if n_neighbors is None:\n        n_neighbors = self.n_neighbors\n    elif n_neighbors <= 0:\n        raise ValueError(\"Expected n_neighbors > 0. Got %d\" % n_neighbors)\n    elif not isinstance(n_neighbors, numbers.Integral):\n        raise TypeError(\n            \"n_neighbors does not take %s value, enter integer value\"\n            % type(n_neighbors)\n        )\n\n    ensure_all_finite = \"allow-nan\" if get_tags(self).input_tags.allow_nan else True\n    query_is_train = X is None\n    if query_is_train:\n        X = self._fit_X\n        # Include an extra neighbor to account for the sample itself being\n        # returned, which is removed later\n        n_neighbors += 1\n    else:\n        if self.metric == \"precomputed\":\n            X = _check_precomputed(X)\n        else:\n            X = validate_data(\n                self,\n                X,\n                ensure_all_finite=ensure_all_finite,\n                accept_sparse=\"csr\",\n                reset=False,\n                order=\"C\",\n            )\n\n    n_samples_fit = self.n_samples_fit_\n    if n_neighbors > n_samples_fit:\n        if query_is_train:\n            n_neighbors -= 1  # ok to modify inplace because an error is raised\n            inequality_str = \"n_neighbors < n_samples_fit\"\n        else:\n            inequality_str = \"n_neighbors <= n_samples_fit\"\n        raise ValueError(\n            f\"Expected {inequality_str}, but \"\n            f\"n_neighbors = {n_neighbors}, n_samples_fit = {n_samples_fit}, \"\n            f\"n_samples = {X.shape[0]}\"  # include n_samples for common tests\n        )\n\n    n_jobs = effective_n_jobs(self.n_jobs)\n    chunked_results = None\n    use_pairwise_distances_reductions = (\n        self._fit_method == \"brute\"\n        and ArgKmin.is_usable_for(\n            X if X is not None else self._fit_X, self._fit_X, self.effective_metric_\n        )\n    )\n    if use_pairwise_distances_reductions:\n        results = ArgKmin.compute(\n            X=X,\n            Y=self._fit_X,\n            k=n_neighbors,\n            metric=self.effective_metric_,\n            metric_kwargs=self.effective_metric_params_,\n            strategy=\"auto\",\n            return_distance=return_distance,\n        )\n\n    elif (\n        self._fit_method == \"brute\" and self.metric == \"precomputed\" and issparse(X)\n    ):\n        results = _kneighbors_from_graph(\n            X, n_neighbors=n_neighbors, return_distance=return_distance\n        )\n\n    elif self._fit_method == \"brute\":\n        # Joblib-based backend, which is used when user-defined callable\n        # are passed for metric.\n\n        # This won't be used in the future once PairwiseDistancesReductions\n        # support:\n        #   - DistanceMetrics which work on supposedly binary data\n        #   - CSR-dense and dense-CSR case if 'euclidean' in metric.\n        reduce_func = partial(\n            self._kneighbors_reduce_func,\n            n_neighbors=n_neighbors,\n            return_distance=return_distance,\n        )\n\n        # for efficiency, use squared euclidean distances\n        if self.effective_metric_ == \"euclidean\":\n            kwds = {\"squared\": True}\n        else:\n            kwds = self.effective_metric_params_\n\n        chunked_results = list(\n            pairwise_distances_chunked(\n                X,\n                self._fit_X,\n                reduce_func=reduce_func,\n                metric=self.effective_metric_,\n                n_jobs=n_jobs,\n                **kwds,\n            )\n        )\n\n    elif self._fit_method in [\"ball_tree\", \"kd_tree\"]:\n        if issparse(X):\n            raise ValueError(\n                \"%s does not work with sparse matrices. Densify the data, \"\n                \"or set algorithm='brute'\" % self._fit_method\n            )\n        chunked_results = Parallel(n_jobs, prefer=\"threads\")(\n            delayed(self._tree.query)(X[s], n_neighbors, return_distance)\n            for s in gen_even_slices(X.shape[0], n_jobs)\n        )\n    else:\n        raise ValueError(\"internal: _fit_method not recognized\")\n\n    if chunked_results is not None:\n        if return_distance:\n            neigh_dist, neigh_ind = zip(*chunked_results)\n            results = np.vstack(neigh_dist), np.vstack(neigh_ind)\n        else:\n            results = np.vstack(chunked_results)\n\n    if not query_is_train:\n        return results\n    else:\n        # If the query data is the same as the indexed data, we would like\n        # to ignore the first nearest neighbor of every sample, i.e\n        # the sample itself.\n        if return_distance:\n            neigh_dist, neigh_ind = results\n        else:\n            neigh_ind = results\n\n        n_queries, _ = X.shape\n        sample_range = np.arange(n_queries)[:, None]\n        sample_mask = neigh_ind != sample_range\n\n        # Corner case: When the number of duplicates are more\n        # than the number of neighbors, the first NN will not\n        # be the sample, but a duplicate.\n        # In that case mask the first duplicate.\n        dup_gr_nbrs = np.all(sample_mask, axis=1)\n        sample_mask[:, 0][dup_gr_nbrs] = False\n        neigh_ind = np.reshape(neigh_ind[sample_mask], (n_queries, n_neighbors - 1))\n\n        if return_distance:\n            neigh_dist = np.reshape(\n                neigh_dist[sample_mask], (n_queries, n_neighbors - 1)\n            )\n            return neigh_dist, neigh_ind\n        return neigh_ind",
    "scikit-learn.sklearn.utils.validation.check_is_fitted": "def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):\n    \"\"\"Perform is_fitted validation for estimator.\n\n    Checks if the estimator is fitted by verifying the presence of\n    fitted attributes (ending with a trailing underscore) and otherwise\n    raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to\n    specify if the estimator is fitted or not. See\n    :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\n    for an example on how to use the API.\n\n    If no `attributes` are passed, this function will pass if an estimator is stateless.\n    An estimator can indicate it's stateless by setting the `requires_fit` tag. See\n    :ref:`estimator_tags` for more information. Note that the `requires_fit` tag\n    is ignored if `attributes` are passed.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Raises\n    ------\n    TypeError\n        If the estimator is a class or not an estimator instance\n\n    NotFittedError\n        If the attributes are not found.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.utils.validation import check_is_fitted\n    >>> from sklearn.exceptions import NotFittedError\n    >>> lr = LogisticRegression()\n    >>> try:\n    ...     check_is_fitted(lr)\n    ... except NotFittedError as exc:\n    ...     print(f\"Model is not fitted yet.\")\n    Model is not fitted yet.\n    >>> lr.fit([[1, 2], [1, 3]], [1, 0])\n    LogisticRegression()\n    >>> check_is_fitted(lr)\n    \"\"\"\n    if isclass(estimator):\n        raise TypeError(\"{} is a class, not an instance.\".format(estimator))\n    if msg is None:\n        msg = (\n            \"This %(name)s instance is not fitted yet. Call 'fit' with \"\n            \"appropriate arguments before using this estimator.\"\n        )\n\n    if not hasattr(estimator, \"fit\"):\n        raise TypeError(\"%s is not an estimator instance.\" % (estimator))\n\n    tags = get_tags(estimator)\n\n    if not tags.requires_fit and attributes is None:\n        return\n\n    if not _is_fitted(estimator, attributes, all_or_any):\n        raise NotFittedError(msg % {\"name\": type(estimator).__name__})"
}