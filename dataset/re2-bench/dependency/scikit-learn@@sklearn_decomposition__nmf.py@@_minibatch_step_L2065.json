{
    "scikit-learn.sklearn.decomposition._nmf._compute_regularization": "def _compute_regularization(self, X):\n    \"\"\"Compute scaled regularization terms.\"\"\"\n    n_samples, n_features = X.shape\n    alpha_W = self.alpha_W\n    alpha_H = self.alpha_W if self.alpha_H == \"same\" else self.alpha_H\n\n    l1_reg_W = n_features * alpha_W * self.l1_ratio\n    l1_reg_H = n_samples * alpha_H * self.l1_ratio\n    l2_reg_W = n_features * alpha_W * (1.0 - self.l1_ratio)\n    l2_reg_H = n_samples * alpha_H * (1.0 - self.l1_ratio)\n\n    return l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H",
    "scikit-learn.sklearn.decomposition._nmf._solve_W": "def _solve_W(self, X, H, max_iter):\n    \"\"\"Minimize the objective function w.r.t W.\n\n    Update W with H being fixed, until convergence. This is the heart\n    of `transform` but it's also used during `fit` when doing fresh restarts.\n    \"\"\"\n    avg = np.sqrt(X.mean() / self._n_components)\n    W = np.full((X.shape[0], self._n_components), avg, dtype=X.dtype)\n    W_buffer = W.copy()\n\n    # Get scaled regularization terms. Done for each minibatch to take into account\n    # variable sizes of minibatches.\n    l1_reg_W, _, l2_reg_W, _ = self._compute_regularization(X)\n\n    for _ in range(max_iter):\n        W, *_ = _multiplicative_update_w(\n            X, W, H, self._beta_loss, l1_reg_W, l2_reg_W, self._gamma\n        )\n\n        W_diff = linalg.norm(W - W_buffer) / linalg.norm(W)\n        if self.tol > 0 and W_diff <= self.tol:\n            break\n\n        W_buffer[:] = W\n\n    return W",
    "scikit-learn.sklearn.decomposition._nmf._multiplicative_update_w": "def _multiplicative_update_w(\n    X,\n    W,\n    H,\n    beta_loss,\n    l1_reg_W,\n    l2_reg_W,\n    gamma,\n    H_sum=None,\n    HHt=None,\n    XHt=None,\n    update_H=True,\n):\n    \"\"\"Update W in Multiplicative Update NMF.\"\"\"\n    if beta_loss == 2:\n        # Numerator\n        if XHt is None:\n            XHt = safe_sparse_dot(X, H.T)\n        if update_H:\n            # avoid a copy of XHt, which will be re-computed (update_H=True)\n            numerator = XHt\n        else:\n            # preserve the XHt, which is not re-computed (update_H=False)\n            numerator = XHt.copy()\n\n        # Denominator\n        if HHt is None:\n            HHt = np.dot(H, H.T)\n        denominator = np.dot(W, HHt)\n\n    else:\n        # Numerator\n        # if X is sparse, compute WH only where X is non zero\n        WH_safe_X = _special_sparse_dot(W, H, X)\n        if sp.issparse(X):\n            WH_safe_X_data = WH_safe_X.data\n            X_data = X.data\n        else:\n            WH_safe_X_data = WH_safe_X\n            X_data = X\n            # copy used in the Denominator\n            WH = WH_safe_X.copy()\n            if beta_loss - 1.0 < 0:\n                WH[WH < EPSILON] = EPSILON\n\n        # to avoid taking a negative power of zero\n        if beta_loss - 2.0 < 0:\n            WH_safe_X_data[WH_safe_X_data < EPSILON] = EPSILON\n\n        if beta_loss == 1:\n            np.divide(X_data, WH_safe_X_data, out=WH_safe_X_data)\n        elif beta_loss == 0:\n            # speeds up computation time\n            # refer to /numpy/numpy/issues/9363\n            WH_safe_X_data **= -1\n            WH_safe_X_data **= 2\n            # element-wise multiplication\n            WH_safe_X_data *= X_data\n        else:\n            WH_safe_X_data **= beta_loss - 2\n            # element-wise multiplication\n            WH_safe_X_data *= X_data\n\n        # here numerator = dot(X * (dot(W, H) ** (beta_loss - 2)), H.T)\n        numerator = safe_sparse_dot(WH_safe_X, H.T)\n\n        # Denominator\n        if beta_loss == 1:\n            if H_sum is None:\n                H_sum = np.sum(H, axis=1)  # shape(n_components, )\n            denominator = H_sum[np.newaxis, :]\n\n        else:\n            # computation of WHHt = dot(dot(W, H) ** beta_loss - 1, H.T)\n            if sp.issparse(X):\n                # memory efficient computation\n                # (compute row by row, avoiding the dense matrix WH)\n                WHHt = np.empty(W.shape)\n                for i in range(X.shape[0]):\n                    WHi = np.dot(W[i, :], H)\n                    if beta_loss - 1 < 0:\n                        WHi[WHi < EPSILON] = EPSILON\n                    WHi **= beta_loss - 1\n                    WHHt[i, :] = np.dot(WHi, H.T)\n            else:\n                WH **= beta_loss - 1\n                WHHt = np.dot(WH, H.T)\n            denominator = WHHt\n\n    # Add L1 and L2 regularization\n    if l1_reg_W > 0:\n        denominator += l1_reg_W\n    if l2_reg_W > 0:\n        denominator = denominator + l2_reg_W * W\n    denominator[denominator == 0] = EPSILON\n\n    numerator /= denominator\n    delta_W = numerator\n\n    # gamma is in ]0, 1]\n    if gamma != 1:\n        delta_W **= gamma\n\n    W *= delta_W\n\n    return W, H_sum, HHt, XHt",
    "scikit-learn.sklearn.decomposition._nmf._multiplicative_update_h": "def _multiplicative_update_h(\n    X, W, H, beta_loss, l1_reg_H, l2_reg_H, gamma, A=None, B=None, rho=None\n):\n    \"\"\"update H in Multiplicative Update NMF.\"\"\"\n    if beta_loss == 2:\n        numerator = safe_sparse_dot(W.T, X)\n        denominator = np.linalg.multi_dot([W.T, W, H])\n\n    else:\n        # Numerator\n        WH_safe_X = _special_sparse_dot(W, H, X)\n        if sp.issparse(X):\n            WH_safe_X_data = WH_safe_X.data\n            X_data = X.data\n        else:\n            WH_safe_X_data = WH_safe_X\n            X_data = X\n            # copy used in the Denominator\n            WH = WH_safe_X.copy()\n            if beta_loss - 1.0 < 0:\n                WH[WH < EPSILON] = EPSILON\n\n        # to avoid division by zero\n        if beta_loss - 2.0 < 0:\n            WH_safe_X_data[WH_safe_X_data < EPSILON] = EPSILON\n\n        if beta_loss == 1:\n            np.divide(X_data, WH_safe_X_data, out=WH_safe_X_data)\n        elif beta_loss == 0:\n            # speeds up computation time\n            # refer to /numpy/numpy/issues/9363\n            WH_safe_X_data **= -1\n            WH_safe_X_data **= 2\n            # element-wise multiplication\n            WH_safe_X_data *= X_data\n        else:\n            WH_safe_X_data **= beta_loss - 2\n            # element-wise multiplication\n            WH_safe_X_data *= X_data\n\n        # here numerator = dot(W.T, (dot(W, H) ** (beta_loss - 2)) * X)\n        numerator = safe_sparse_dot(W.T, WH_safe_X)\n\n        # Denominator\n        if beta_loss == 1:\n            W_sum = np.sum(W, axis=0)  # shape(n_components, )\n            W_sum[W_sum == 0] = 1.0\n            denominator = W_sum[:, np.newaxis]\n\n        # beta_loss not in (1, 2)\n        else:\n            # computation of WtWH = dot(W.T, dot(W, H) ** beta_loss - 1)\n            if sp.issparse(X):\n                # memory efficient computation\n                # (compute column by column, avoiding the dense matrix WH)\n                WtWH = np.empty(H.shape)\n                for i in range(X.shape[1]):\n                    WHi = np.dot(W, H[:, i])\n                    if beta_loss - 1 < 0:\n                        WHi[WHi < EPSILON] = EPSILON\n                    WHi **= beta_loss - 1\n                    WtWH[:, i] = np.dot(W.T, WHi)\n            else:\n                WH **= beta_loss - 1\n                WtWH = np.dot(W.T, WH)\n            denominator = WtWH\n\n    # Add L1 and L2 regularization\n    if l1_reg_H > 0:\n        denominator += l1_reg_H\n    if l2_reg_H > 0:\n        denominator = denominator + l2_reg_H * H\n    denominator[denominator == 0] = EPSILON\n\n    if A is not None and B is not None:\n        # Updates for the online nmf\n        if gamma != 1:\n            H **= 1 / gamma\n        numerator *= H\n        A *= rho\n        B *= rho\n        A += numerator\n        B += denominator\n        H = A / B\n\n        if gamma != 1:\n            H **= gamma\n    else:\n        delta_H = numerator\n        delta_H /= denominator\n        if gamma != 1:\n            delta_H **= gamma\n        H *= delta_H\n\n    return H",
    "scikit-learn.sklearn.decomposition._nmf._beta_divergence": "def _beta_divergence(X, W, H, beta, square_root=False):\n    \"\"\"Compute the beta-divergence of X and dot(W, H).\n\n    Parameters\n    ----------\n    X : float or array-like of shape (n_samples, n_features)\n\n    W : float or array-like of shape (n_samples, n_components)\n\n    H : float or array-like of shape (n_components, n_features)\n\n    beta : float or {'frobenius', 'kullback-leibler', 'itakura-saito'}\n        Parameter of the beta-divergence.\n        If beta == 2, this is half the Frobenius *squared* norm.\n        If beta == 1, this is the generalized Kullback-Leibler divergence.\n        If beta == 0, this is the Itakura-Saito divergence.\n        Else, this is the general beta-divergence.\n\n    square_root : bool, default=False\n        If True, return np.sqrt(2 * res)\n        For beta == 2, it corresponds to the Frobenius norm.\n\n    Returns\n    -------\n        res : float\n            Beta divergence of X and np.dot(X, H).\n    \"\"\"\n    beta = _beta_loss_to_float(beta)\n\n    # The method can be called with scalars\n    if not sp.issparse(X):\n        X = np.atleast_2d(X)\n    W = np.atleast_2d(W)\n    H = np.atleast_2d(H)\n\n    # Frobenius norm\n    if beta == 2:\n        # Avoid the creation of the dense np.dot(W, H) if X is sparse.\n        if sp.issparse(X):\n            norm_X = np.dot(X.data, X.data)\n            norm_WH = trace_dot(np.linalg.multi_dot([W.T, W, H]), H)\n            cross_prod = trace_dot((X @ H.T), W)\n            res = (norm_X + norm_WH - 2.0 * cross_prod) / 2.0\n        else:\n            res = squared_norm(X - np.dot(W, H)) / 2.0\n\n        if square_root:\n            return np.sqrt(res * 2)\n        else:\n            return res\n\n    if sp.issparse(X):\n        # compute np.dot(W, H) only where X is nonzero\n        WH_data = _special_sparse_dot(W, H, X).data\n        X_data = X.data\n    else:\n        WH = np.dot(W, H)\n        WH_data = WH.ravel()\n        X_data = X.ravel()\n\n    # do not affect the zeros: here 0 ** (-1) = 0 and not infinity\n    indices = X_data > EPSILON\n    WH_data = WH_data[indices]\n    X_data = X_data[indices]\n\n    # used to avoid division by zero\n    WH_data[WH_data < EPSILON] = EPSILON\n\n    # generalized Kullback-Leibler divergence\n    if beta == 1:\n        # fast and memory efficient computation of np.sum(np.dot(W, H))\n        sum_WH = np.dot(np.sum(W, axis=0), np.sum(H, axis=1))\n        # computes np.sum(X * log(X / WH)) only where X is nonzero\n        div = X_data / WH_data\n        res = np.dot(X_data, np.log(div))\n        # add full np.sum(np.dot(W, H)) - np.sum(X)\n        res += sum_WH - X_data.sum()\n\n    # Itakura-Saito divergence\n    elif beta == 0:\n        div = X_data / WH_data\n        res = np.sum(div) - np.prod(X.shape) - np.sum(np.log(div))\n\n    # beta-divergence, beta not in (0, 1, 2)\n    else:\n        if sp.issparse(X):\n            # slow loop, but memory efficient computation of :\n            # np.sum(np.dot(W, H) ** beta)\n            sum_WH_beta = 0\n            for i in range(X.shape[1]):\n                sum_WH_beta += np.sum(np.dot(W, H[:, i]) ** beta)\n\n        else:\n            sum_WH_beta = np.sum(WH**beta)\n\n        sum_X_WH = np.dot(X_data, WH_data ** (beta - 1))\n        res = (X_data**beta).sum() - beta * sum_X_WH\n        res += sum_WH_beta * (beta - 1)\n        res /= beta * (beta - 1)\n\n    if square_root:\n        res = max(res, 0)  # avoid negative number due to rounding errors\n        return np.sqrt(2 * res)\n    else:\n        return res"
}