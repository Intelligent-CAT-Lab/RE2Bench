{
    "scikit-learn.sklearn.base.is_regressor": "def is_regressor(estimator):\n    \"\"\"Return True if the given estimator is (probably) a regressor.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator object to test.\n\n    Returns\n    -------\n    out : bool\n        True if estimator is a regressor and False otherwise.\n\n    Examples\n    --------\n    >>> from sklearn.base import is_regressor\n    >>> from sklearn.cluster import KMeans\n    >>> from sklearn.svm import SVC, SVR\n    >>> classifier = SVC()\n    >>> regressor = SVR()\n    >>> kmeans = KMeans()\n    >>> is_regressor(classifier)\n    False\n    >>> is_regressor(regressor)\n    True\n    >>> is_regressor(kmeans)\n    False\n    \"\"\"\n    return get_tags(estimator).estimator_type == \"regressor\"",
    "scikit-learn.sklearn.gaussian_process._gpc.predict_proba": "def predict_proba(self, X):\n    \"\"\"Return probability estimates for the test vector X.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features) or list of object\n        Query points where the GP is evaluated for classification.\n\n    Returns\n    -------\n    C : array-like of shape (n_samples, n_classes)\n        Returns the probability of the samples for each class in\n        the model. The columns correspond to the classes in sorted\n        order, as they appear in the attribute ``classes_``.\n    \"\"\"\n    check_is_fitted(self)\n\n    # Compute the mean and variance of the latent function\n    # (Lines 4-6 of Algorithm 3.2 of GPML)\n    latent_mean, latent_var = self.latent_mean_and_variance(X)\n\n    # Line 7:\n    # Approximate \\int log(z) * N(z | f_star, var_f_star)\n    # Approximation is due to Williams & Barber, \"Bayesian Classification\n    # with Gaussian Processes\", Appendix A: Approximate the logistic\n    # sigmoid by a linear combination of 5 error functions.\n    # For information on how this integral can be computed see\n    # blitiri.blogspot.de/2012/11/gaussian-integral-of-error-function.html\n    alpha = 1 / (2 * latent_var)\n    gamma = LAMBDAS * latent_mean\n    integrals = (\n        np.sqrt(np.pi / alpha)\n        * erf(gamma * np.sqrt(alpha / (alpha + LAMBDAS**2)))\n        / (2 * np.sqrt(latent_var * 2 * np.pi))\n    )\n    pi_star = (COEFS * integrals).sum(axis=0) + 0.5 * COEFS.sum()\n\n    return np.vstack((1 - pi_star, pi_star)).T",
    "scikit-learn.sklearn.linear_model._base.predict": "def predict(self, X):\n    \"\"\"\n    Predict using the linear model.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape (n_samples, n_features)\n        Samples.\n\n    Returns\n    -------\n    C : array, shape (n_samples,)\n        Returns predicted values.\n    \"\"\"\n    return self._decision_function(X)",
    "scikit-learn.sklearn.linear_model._base.decision_function": "def decision_function(self, X):\n    \"\"\"\n    Predict confidence scores for samples.\n\n    The confidence score for a sample is proportional to the signed\n    distance of that sample to the hyperplane.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The data matrix for which we want to get the confidence scores.\n\n    Returns\n    -------\n    scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n        Confidence scores per `(n_samples, n_classes)` combination. In the\n        binary case, confidence score for `self.classes_[1]` where >0 means\n        this class would be predicted.\n    \"\"\"\n    check_is_fitted(self)\n    xp, _ = get_namespace(X)\n\n    X = validate_data(self, X, accept_sparse=\"csr\", reset=False)\n    coef_T = self.coef_.T if self.coef_.ndim == 2 else self.coef_\n    scores = safe_sparse_dot(X, coef_T, dense_output=True) + self.intercept_\n    return (\n        xp.reshape(scores, (-1,))\n        if (scores.ndim > 1 and scores.shape[1] == 1)\n        else scores\n    )",
    "scikit-learn.sklearn.multiclass.decision_function": "def decision_function(self, X):\n    check_is_fitted(self)\n    validate_data(\n        self,\n        X,\n        ensure_all_finite=False,\n        dtype=None,\n        accept_sparse=True,\n        ensure_2d=False,\n        reset=False,\n    )\n\n    return np.repeat(self.y_, _num_samples(X))",
    "scikit-learn.sklearn.naive_bayes.predict_proba": "def predict_proba(self, X):\n    \"\"\"\n    Return probability estimates for the test vector X.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input samples.\n\n    Returns\n    -------\n    C : array-like of shape (n_samples, n_classes)\n        Returns the probability of the samples for each class in\n        the model. The columns correspond to the classes in sorted\n        order, as they appear in the attribute :term:`classes_`.\n    \"\"\"\n    xp, _ = get_namespace(X)\n    return xp.exp(self.predict_log_proba(X))",
    "scikit-learn.sklearn.neighbors._classification.predict_proba": "def predict_proba(self, X):\n    \"\"\"Return probability estimates for the test data X.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_queries, n_features), \\\n            or (n_queries, n_indexed) if metric == 'precomputed', or None\n        Test samples. If `None`, predictions for all indexed points are\n        returned; in this case, points are not considered their own\n        neighbors.\n\n    Returns\n    -------\n    p : ndarray of shape (n_queries, n_classes), or a list of n_outputs \\\n            of such arrays if n_outputs > 1.\n        The class probabilities of the input samples. Classes are ordered\n        by lexicographic order.\n    \"\"\"\n    check_is_fitted(self, \"_fit_method\")\n    if self.weights == \"uniform\":\n        # TODO: systematize this mapping of metric for\n        # PairwiseDistancesReductions.\n        metric, metric_kwargs = _adjusted_metric(\n            metric=self.metric, metric_kwargs=self.metric_params, p=self.p\n        )\n        if (\n            self._fit_method == \"brute\"\n            and ArgKminClassMode.is_usable_for(X, self._fit_X, metric)\n            # TODO: Implement efficient multi-output solution\n            and not self.outputs_2d_\n        ):\n            if self.metric == \"precomputed\":\n                X = _check_precomputed(X)\n            else:\n                X = validate_data(\n                    self, X, accept_sparse=\"csr\", reset=False, order=\"C\"\n                )\n\n            probabilities = ArgKminClassMode.compute(\n                X,\n                self._fit_X,\n                k=self.n_neighbors,\n                weights=self.weights,\n                Y_labels=self._y,\n                unique_Y_labels=self.classes_,\n                metric=metric,\n                metric_kwargs=metric_kwargs,\n                # `strategy=\"parallel_on_X\"` has in practice be shown\n                # to be more efficient than `strategy=\"parallel_on_Y``\n                # on many combination of datasets.\n                # Hence, we choose to enforce it here.\n                # For more information, see:\n                # https://github.com/scikit-learn/scikit-learn/pull/24076#issuecomment-1445258342\n                # TODO: adapt the heuristic for `strategy=\"auto\"` for\n                # `ArgKminClassMode` and use `strategy=\"auto\"`.\n                strategy=\"parallel_on_X\",\n            )\n            return probabilities\n\n        # In that case, we do not need the distances to perform\n        # the weighting so we do not compute them.\n        neigh_ind = self.kneighbors(X, return_distance=False)\n        neigh_dist = None\n    else:\n        neigh_dist, neigh_ind = self.kneighbors(X)\n\n    classes_ = self.classes_\n    _y = self._y\n    if not self.outputs_2d_:\n        _y = self._y.reshape((-1, 1))\n        classes_ = [self.classes_]\n\n    n_queries = _num_samples(self._fit_X if X is None else X)\n\n    weights = _get_weights(neigh_dist, self.weights)\n    if weights is None:\n        weights = np.ones_like(neigh_ind)\n    elif _all_with_any_reduction_axis_1(weights, value=0):\n        raise ValueError(\n            \"All neighbors of some sample is getting zero weights. \"\n            \"Please modify 'weights' to avoid this case if you are \"\n            \"using a user-defined function.\"\n        )\n\n    all_rows = np.arange(n_queries)\n    probabilities = []\n    for k, classes_k in enumerate(classes_):\n        pred_labels = _y[:, k][neigh_ind]\n        proba_k = np.zeros((n_queries, classes_k.size))\n\n        # a simple ':' index doesn't work right\n        for i, idx in enumerate(pred_labels.T):  # loop is O(n_neighbors)\n            proba_k[all_rows, idx] += weights[:, i]\n\n        # normalize 'votes' into real [0,1] probabilities\n        normalizer = proba_k.sum(axis=1)[:, np.newaxis]\n        proba_k /= normalizer\n\n        probabilities.append(proba_k)\n\n    if not self.outputs_2d_:\n        probabilities = probabilities[0]\n\n    return probabilities",
    "scikit-learn.sklearn.pipeline.predict_proba": "@available_if(_final_estimator_has(\"predict_proba\"))\ndef predict_proba(self, X, **params):\n    \"\"\"Transform the data, and apply `predict_proba` with the final estimator.\n\n    Call `transform` of each transformer in the pipeline. The transformed\n    data are finally passed to the final estimator that calls\n    `predict_proba` method. Only valid if the final estimator implements\n    `predict_proba`.\n\n    Parameters\n    ----------\n    X : iterable\n        Data to predict on. Must fulfill input requirements of first step\n        of the pipeline.\n\n    **params : dict of str -> object\n        - If `enable_metadata_routing=False` (default): Parameters to the\n          `predict_proba` called at the end of all transformations in the pipeline.\n\n        - If `enable_metadata_routing=True`: Parameters requested and accepted by\n          steps. Each step must have requested certain metadata for these parameters\n          to be forwarded to them.\n\n        .. versionadded:: 0.20\n\n        .. versionchanged:: 1.4\n            Parameters are now passed to the ``transform`` method of the\n            intermediate steps as well, if requested, and if\n            `enable_metadata_routing=True`.\n\n        See :ref:`Metadata Routing User Guide <metadata_routing>` for more\n        details.\n\n    Returns\n    -------\n    y_proba : ndarray of shape (n_samples, n_classes)\n        Result of calling `predict_proba` on the final estimator.\n    \"\"\"\n    check_is_fitted(self)\n    Xt = X\n\n    if not _routing_enabled():\n        for _, name, transform in self._iter(with_final=False):\n            Xt = transform.transform(Xt)\n        return self.steps[-1][1].predict_proba(Xt, **params)\n\n    # metadata routing enabled\n    routed_params = process_routing(self, \"predict_proba\", **params)\n    for _, name, transform in self._iter(with_final=False):\n        Xt = transform.transform(Xt, **routed_params[name].transform)\n    return self.steps[-1][1].predict_proba(\n        Xt, **routed_params[self.steps[-1][0]].predict_proba\n    )",
    "scikit-learn.sklearn.pipeline.decision_function": "@available_if(_final_estimator_has(\"decision_function\"))\ndef decision_function(self, X, **params):\n    \"\"\"Transform the data, and apply `decision_function` with the final estimator.\n\n    Call `transform` of each transformer in the pipeline. The transformed\n    data are finally passed to the final estimator that calls\n    `decision_function` method. Only valid if the final estimator\n    implements `decision_function`.\n\n    Parameters\n    ----------\n    X : iterable\n        Data to predict on. Must fulfill input requirements of first step\n        of the pipeline.\n\n    **params : dict of string -> object\n        Parameters requested and accepted by steps. Each step must have\n        requested certain metadata for these parameters to be forwarded to\n        them.\n\n        .. versionadded:: 1.4\n            Only available if `enable_metadata_routing=True`. See\n            :ref:`Metadata Routing User Guide <metadata_routing>` for more\n            details.\n\n    Returns\n    -------\n    y_score : ndarray of shape (n_samples, n_classes)\n        Result of calling `decision_function` on the final estimator.\n    \"\"\"\n    check_is_fitted(self)\n    _raise_for_params(params, self, \"decision_function\")\n\n    # not branching here since params is only available if\n    # enable_metadata_routing=True\n    routed_params = process_routing(self, \"decision_function\", **params)\n\n    Xt = X\n    for _, name, transform in self._iter(with_final=False):\n        Xt = transform.transform(\n            Xt, **routed_params.get(name, {}).get(\"transform\", {})\n        )\n    return self.steps[-1][1].decision_function(\n        Xt,\n        **routed_params.get(self.steps[-1][0], {}).get(\"decision_function\", {}),\n    )",
    "scikit-learn.sklearn.svm._base.decision_function": "def decision_function(self, X):\n    \"\"\"Evaluate the decision function for the samples in X.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input samples.\n\n    Returns\n    -------\n    X : ndarray of shape (n_samples, n_classes * (n_classes-1) / 2)\n        Returns the decision function of the sample for each class\n        in the model.\n        If decision_function_shape='ovr', the shape is (n_samples,\n        n_classes).\n\n    Notes\n    -----\n    If decision_function_shape='ovo', the function values are proportional\n    to the distance of the samples X to the separating hyperplane. If the\n    exact distances are required, divide the function values by the norm of\n    the weight vector (``coef_``). See also `this question\n    <https://stats.stackexchange.com/questions/14876/\n    interpreting-distance-from-hyperplane-in-svm>`_ for further details.\n    If decision_function_shape='ovr', the decision function is a monotonic\n    transformation of ovo decision function.\n    \"\"\"\n    dec = self._decision_function(X)\n    if self.decision_function_shape == \"ovr\" and len(self.classes_) > 2:\n        return _ovr_decision_function(dec < 0, -dec, len(self.classes_))\n    return dec",
    "scikit-learn.sklearn.tree._classes.predict_proba": "def predict_proba(self, X, check_input=True):\n    \"\"\"Predict class probabilities of the input samples X.\n\n    The predicted class probability is the fraction of samples of the same\n    class in a leaf.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The input samples. Internally, it will be converted to\n        ``dtype=np.float32`` and if a sparse matrix is provided\n        to a sparse ``csr_matrix``.\n\n    check_input : bool, default=True\n        Allow to bypass several input checking.\n        Don't use this parameter unless you know what you're doing.\n\n    Returns\n    -------\n    proba : ndarray of shape (n_samples, n_classes) or list of n_outputs \\\n        such arrays if n_outputs > 1\n        The class probabilities of the input samples. The order of the\n        classes corresponds to that in the attribute :term:`classes_`.\n    \"\"\"\n    check_is_fitted(self)\n    X = self._validate_X_predict(X, check_input)\n    proba = self.tree_.predict(X)\n\n    if self.n_outputs_ == 1:\n        return proba[:, : self.n_classes_]\n    else:\n        all_proba = []\n        for k in range(self.n_outputs_):\n            proba_k = proba[:, k, : self.n_classes_[k]]\n            all_proba.append(proba_k)\n        return all_proba",
    "scikit-learn.sklearn.tree._classes.predict": "def predict(self, X, check_input=True):\n    \"\"\"Predict class or regression value for X.\n\n    For a classification model, the predicted class for each sample in X is\n    returned. For a regression model, the predicted value based on X is\n    returned.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The input samples. Internally, it will be converted to\n        ``dtype=np.float32`` and if a sparse matrix is provided\n        to a sparse ``csr_matrix``.\n\n    check_input : bool, default=True\n        Allow to bypass several input checking.\n        Don't use this parameter unless you know what you're doing.\n\n    Returns\n    -------\n    y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n        The predicted classes, or the predict values.\n    \"\"\"\n    check_is_fitted(self)\n    X = self._validate_X_predict(X, check_input)\n    proba = self.tree_.predict(X)\n    n_samples = X.shape[0]\n\n    # Classification\n    if is_classifier(self):\n        if self.n_outputs_ == 1:\n            return self.classes_.take(np.argmax(proba, axis=1), axis=0)\n\n        else:\n            class_type = self.classes_[0].dtype\n            predictions = np.zeros((n_samples, self.n_outputs_), dtype=class_type)\n            for k in range(self.n_outputs_):\n                predictions[:, k] = self.classes_[k].take(\n                    np.argmax(proba[:, k], axis=1), axis=0\n                )\n\n            return predictions\n\n    # Regression\n    else:\n        if self.n_outputs_ == 1:\n            return proba[:, 0]\n\n        else:\n            return proba[:, :, 0]",
    "scikit-learn.sklearn.utils._available_if.__get__": "def __get__(self, obj, owner=None):\n    if obj is not None:\n        # delegate only on instances, not the classes.\n        # this is to allow access to the docstrings.\n        self._check(obj, owner=owner)\n        out = MethodType(self.fn, obj)\n\n    else:\n        # This makes it possible to use the decorated method as an unbound method,\n        # for instance when monkeypatching.\n        @wraps(self.fn)\n        def out(*args, **kwargs):\n            self._check(args[0], owner=owner)\n            return self.fn(*args, **kwargs)\n\n    return out",
    "scikit-learn.sklearn.utils._mocking.decision_function": "def decision_function(self, X):\n    \"\"\"Confidence score.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input data.\n\n    Returns\n    -------\n    decision : ndarray of shape (n_samples,) if n_classes == 2\\\n            else (n_samples, n_classes)\n        Confidence score.\n    \"\"\"\n    if (\n        self.methods_to_check == \"all\"\n        or \"decision_function\" in self.methods_to_check\n    ):\n        X, y = self._check_X_y(X)\n    rng = check_random_state(self.random_state)\n    if len(self.classes_) == 2:\n        # for binary classifier, the confidence score is related to\n        # classes_[1] and therefore should be null.\n        return rng.randn(_num_samples(X))\n    else:\n        return rng.randn(_num_samples(X), len(self.classes_))"
}