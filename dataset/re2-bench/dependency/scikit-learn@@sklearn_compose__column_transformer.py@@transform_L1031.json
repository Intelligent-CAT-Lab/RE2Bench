{
    "scikit-learn.sklearn.compose._column_transformer._hstack": "def _hstack(self, Xs, *, n_samples):\n    \"\"\"Stacks Xs horizontally.\n\n    This allows subclasses to control the stacking behavior, while reusing\n    everything else from ColumnTransformer.\n\n    Parameters\n    ----------\n    Xs : list of {array-like, sparse matrix, dataframe}\n        The container to concatenate.\n    n_samples : int\n        The number of samples in the input data to checking the transformation\n        consistency.\n    \"\"\"\n    if self.sparse_output_:\n        try:\n            # since all columns should be numeric before stacking them\n            # in a sparse matrix, `check_array` is used for the\n            # dtype conversion if necessary.\n            converted_Xs = [\n                check_array(X, accept_sparse=True, ensure_all_finite=False)\n                for X in Xs\n            ]\n        except ValueError as e:\n            raise ValueError(\n                \"For a sparse output, all columns should \"\n                \"be a numeric or convertible to a numeric.\"\n            ) from e\n\n        return sparse.hstack(converted_Xs).tocsr()\n    else:\n        Xs = [f.toarray() if sparse.issparse(f) else f for f in Xs]\n        adapter = _get_container_adapter(\"transform\", self)\n        if adapter and all(adapter.is_supported_container(X) for X in Xs):\n            # rename before stacking as it avoids to error on temporary duplicated\n            # columns\n            transformer_names = [\n                t[0]\n                for t in self._iter(\n                    fitted=True,\n                    column_as_labels=False,\n                    skip_drop=True,\n                    skip_empty_columns=True,\n                )\n            ]\n            feature_names_outs = [X.columns for X in Xs if X.shape[1] != 0]\n            if self.verbose_feature_names_out:\n                # `_add_prefix_for_feature_names_out` takes care about raising\n                # an error if there are duplicated columns.\n                feature_names_outs = self._add_prefix_for_feature_names_out(\n                    list(zip(transformer_names, feature_names_outs))\n                )\n            else:\n                # check for duplicated columns and raise if any\n                feature_names_outs = list(chain.from_iterable(feature_names_outs))\n                feature_names_count = Counter(feature_names_outs)\n                if any(count > 1 for count in feature_names_count.values()):\n                    duplicated_feature_names = sorted(\n                        name\n                        for name, count in feature_names_count.items()\n                        if count > 1\n                    )\n                    err_msg = (\n                        \"Duplicated feature names found before concatenating the\"\n                        \" outputs of the transformers:\"\n                        f\" {duplicated_feature_names}.\\n\"\n                    )\n                    for transformer_name, X in zip(transformer_names, Xs):\n                        if X.shape[1] == 0:\n                            continue\n                        dup_cols_in_transformer = sorted(\n                            set(X.columns).intersection(duplicated_feature_names)\n                        )\n                        if len(dup_cols_in_transformer):\n                            err_msg += (\n                                f\"Transformer {transformer_name} has conflicting \"\n                                f\"columns names: {dup_cols_in_transformer}.\\n\"\n                            )\n                    raise ValueError(\n                        err_msg\n                        + \"Either make sure that the transformers named above \"\n                        \"do not generate columns with conflicting names or set \"\n                        \"verbose_feature_names_out=True to automatically \"\n                        \"prefix to the output feature names with the name \"\n                        \"of the transformer to prevent any conflicting \"\n                        \"names.\"\n                    )\n\n            names_idx = 0\n            for X in Xs:\n                if X.shape[1] == 0:\n                    continue\n                names_out = feature_names_outs[names_idx : names_idx + X.shape[1]]\n                adapter.rename_columns(X, names_out)\n                names_idx += X.shape[1]\n\n            output = adapter.hstack(Xs)\n            output_samples = output.shape[0]\n            if output_samples != n_samples:\n                raise ValueError(\n                    \"Concatenating DataFrames from the transformer's output lead to\"\n                    \" an inconsistent number of samples. The output may have Pandas\"\n                    \" Indexes that do not match, or that transformers are returning\"\n                    \" number of samples which are not the same as the number input\"\n                    \" samples.\"\n                )\n\n            return output\n\n        return np.hstack(Xs)",
    "scikit-learn.sklearn.compose._column_transformer._get_empty_routing": "def _get_empty_routing(self):\n    \"\"\"Return empty routing.\n\n    Used while routing can be disabled.\n\n    TODO: Remove when ``set_config(enable_metadata_routing=False)`` is no\n    more an option.\n    \"\"\"\n    return Bunch(\n        **{\n            name: Bunch(**{method: {} for method in METHODS})\n            for name, step, _, _ in self._iter(\n                fitted=False,\n                column_as_labels=False,\n                skip_drop=True,\n                skip_empty_columns=True,\n            )\n        }\n    )",
    "scikit-learn.sklearn.compose._column_transformer._check_X": "def _check_X(X):\n    \"\"\"Use check_array only when necessary, e.g. on lists and other non-array-likes.\"\"\"\n    if (\n        (hasattr(X, \"__array__\") and hasattr(X, \"shape\"))\n        or hasattr(X, \"__dataframe__\")\n        or sparse.issparse(X)\n    ):\n        return X\n    return check_array(X, ensure_all_finite=\"allow-nan\", dtype=object)",
    "scikit-learn.sklearn.compose._column_transformer._validate_output": "def _validate_output(self, result):\n    \"\"\"\n    Ensure that the output of each transformer is 2D. Otherwise\n    hstack can raise an error or produce incorrect results.\n    \"\"\"\n    names = [\n        name\n        for name, _, _, _ in self._iter(\n            fitted=True,\n            column_as_labels=False,\n            skip_drop=True,\n            skip_empty_columns=True,\n        )\n    ]\n    for Xs, name in zip(result, names):\n        if not getattr(Xs, \"ndim\", 0) == 2 and not hasattr(Xs, \"__dataframe__\"):\n            raise ValueError(\n                \"The output of the '{0}' transformer should be 2D (numpy array, \"\n                \"scipy sparse array, dataframe).\".format(name)\n            )\n    if _get_output_config(\"transform\", self)[\"dense\"] == \"pandas\":\n        return\n    try:\n        import pandas as pd\n    except ImportError:\n        return\n    for Xs, name in zip(result, names):\n        if not _is_pandas_df(Xs):\n            continue\n        for col_name, dtype in Xs.dtypes.to_dict().items():\n            if getattr(dtype, \"na_value\", None) is not pd.NA:\n                continue\n            if pd.NA not in Xs[col_name].values:\n                continue\n            class_name = self.__class__.__name__\n            raise ValueError(\n                f\"The output of the '{name}' transformer for column\"\n                f\" '{col_name}' has dtype {dtype} and uses pandas.NA to\"\n                \" represent null values. Storing this output in a numpy array\"\n                \" can cause errors in downstream scikit-learn estimators, and\"\n                \" inefficiencies. To avoid this problem you can (i)\"\n                \" store the output in a pandas DataFrame by using\"\n                f\" {class_name}.set_output(transform='pandas') or (ii) modify\"\n                f\" the input data or the '{name}' transformer to avoid the\"\n                \" presence of pandas.NA (for example by using\"\n                \" pandas.DataFrame.astype).\"\n            )",
    "scikit-learn.sklearn.compose._column_transformer._call_func_on_transformers": "def _call_func_on_transformers(self, X, y, func, column_as_labels, routed_params):\n    \"\"\"\n    Private function to fit and/or transform on demand.\n\n    Parameters\n    ----------\n    X : {array-like, dataframe} of shape (n_samples, n_features)\n        The data to be used in fit and/or transform.\n\n    y : array-like of shape (n_samples,)\n        Targets.\n\n    func : callable\n        Function to call, which can be _fit_transform_one or\n        _transform_one.\n\n    column_as_labels : bool\n        Used to iterate through transformers. If True, columns are returned\n        as strings. If False, columns are returned as they were given by\n        the user. Can be True only if the ``ColumnTransformer`` is already\n        fitted.\n\n    routed_params : dict\n        The routed parameters as the output from ``process_routing``.\n\n    Returns\n    -------\n    Return value (transformers and/or transformed X data) depends\n    on the passed function.\n    \"\"\"\n    if func is _fit_transform_one:\n        fitted = False\n    else:  # func is _transform_one\n        fitted = True\n\n    transformers = list(\n        self._iter(\n            fitted=fitted,\n            column_as_labels=column_as_labels,\n            skip_drop=True,\n            skip_empty_columns=True,\n        )\n    )\n    try:\n        jobs = []\n        for idx, (name, trans, columns, weight) in enumerate(transformers, start=1):\n            if func is _fit_transform_one:\n                if trans == \"passthrough\":\n                    output_config = _get_output_config(\"transform\", self)\n                    trans = FunctionTransformer(\n                        accept_sparse=True,\n                        check_inverse=False,\n                        feature_names_out=\"one-to-one\",\n                    ).set_output(transform=output_config[\"dense\"])\n\n                extra_args = dict(\n                    message_clsname=\"ColumnTransformer\",\n                    message=self._log_message(name, idx, len(transformers)),\n                )\n            else:  # func is _transform_one\n                extra_args = {}\n            jobs.append(\n                delayed(func)(\n                    transformer=clone(trans) if not fitted else trans,\n                    X=_safe_indexing(X, columns, axis=1),\n                    y=y,\n                    weight=weight,\n                    **extra_args,\n                    params=routed_params[name],\n                )\n            )\n\n        return Parallel(n_jobs=self.n_jobs)(jobs)\n\n    except ValueError as e:\n        if \"Expected 2D array, got 1D array instead\" in str(e):\n            raise ValueError(_ERR_MSG_1DCOLUMN) from e\n        else:\n            raise",
    "scikit-learn.sklearn.utils._metadata_requests.process_routing": "def process_routing(_obj, _method, /, **kwargs):\n    \"\"\"Validate and route metadata.\n\n    This function is used inside a :term:`router`'s method, e.g. :term:`fit`,\n    to validate the metadata and handle the routing.\n\n    Assuming this signature of a router's fit method:\n    ``fit(self, X, y, sample_weight=None, **fit_params)``,\n    a call to this function would be:\n    ``process_routing(self, \"fit\", sample_weight=sample_weight, **fit_params)``.\n\n    Note that if routing is not enabled and ``kwargs`` is empty, then it\n    returns an empty routing where ``process_routing(...).ANYTHING.ANY_METHOD``\n    is always an empty dictionary.\n\n    .. versionadded:: 1.3\n\n    Parameters\n    ----------\n    _obj : object\n        An object implementing ``get_metadata_routing``. Typically a\n        :term:`meta-estimator`.\n\n    _method : str\n        The name of the router's method in which this function is called.\n\n    **kwargs : dict\n        Metadata to be routed.\n\n    Returns\n    -------\n    routed_params : Bunch\n        A :class:`~utils.Bunch` of the form ``{\"object_name\": {\"method_name\":\n        {metadata: value}}}`` which can be used to pass the required metadata to\n        A :class:`~sklearn.utils.Bunch` of the form ``{\"object_name\": {\"method_name\":\n        {metadata: value}}}`` which can be used to pass the required metadata to\n        corresponding methods or corresponding child objects. The object names\n        are those defined in `obj.get_metadata_routing()`.\n    \"\"\"\n    if not kwargs:\n        # If routing is not enabled and kwargs are empty, then we don't have to\n        # try doing any routing, we can simply return a structure which returns\n        # an empty dict on routed_params.ANYTHING.ANY_METHOD.\n        class EmptyRequest:\n            def get(self, name, default=None):\n                return Bunch(**{method: dict() for method in METHODS})\n\n            def __getitem__(self, name):\n                return Bunch(**{method: dict() for method in METHODS})\n\n            def __getattr__(self, name):\n                return Bunch(**{method: dict() for method in METHODS})\n\n        return EmptyRequest()\n\n    if not (hasattr(_obj, \"get_metadata_routing\") or isinstance(_obj, MetadataRouter)):\n        raise AttributeError(\n            f\"The given object ({_routing_repr(_obj)}) needs to either\"\n            \" implement the routing method `get_metadata_routing` or be a\"\n            \" `MetadataRouter` instance.\"\n        )\n    if _method not in METHODS:\n        raise TypeError(\n            f\"Can only route and process input on these methods: {METHODS}, \"\n            f\"while the passed method is: {_method}.\"\n        )\n\n    request_routing = get_routing_for_object(_obj)\n    request_routing.validate_metadata(params=kwargs, method=_method)\n    routed_params = request_routing.route_params(params=kwargs, caller=_method)\n\n    return routed_params",
    "scikit-learn.sklearn.utils._metadata_requests._routing_enabled": "def _routing_enabled():\n    \"\"\"Return whether metadata routing is enabled.\n\n    .. versionadded:: 1.3\n\n    Returns\n    -------\n    enabled : bool\n        Whether metadata routing is enabled. If the config is not set, it\n        defaults to False.\n    \"\"\"\n    return get_config().get(\"enable_metadata_routing\", False)",
    "scikit-learn.sklearn.utils._metadata_requests._raise_for_params": "def _raise_for_params(params, owner, method, allow=None):\n    \"\"\"Raise an error if metadata routing is not enabled and params are passed.\n\n    .. versionadded:: 1.4\n\n    Parameters\n    ----------\n    params : dict\n        The metadata passed to a method.\n\n    owner : object\n        The object to which the method belongs.\n\n    method : str\n        The name of the method, e.g. \"fit\".\n\n    allow : list of str, default=None\n        A list of parameters which are allowed to be passed even if metadata\n        routing is not enabled.\n\n    Raises\n    ------\n    ValueError\n        If metadata routing is not enabled and params are passed.\n    \"\"\"\n    caller = f\"{_routing_repr(owner)}.{method}\" if method else _routing_repr(owner)\n\n    allow = allow if allow is not None else {}\n\n    if not _routing_enabled() and (params.keys() - allow):\n        raise ValueError(\n            f\"Passing extra keyword arguments to {caller} is only supported if\"\n            \" enable_metadata_routing=True, which you can set using\"\n            \" `sklearn.set_config`. See the User Guide\"\n            \" <https://scikit-learn.org/stable/metadata_routing.html> for more\"\n            f\" details. Extra parameters passed are: {set(params)}\"\n        )",
    "scikit-learn.sklearn.utils.validation.check_is_fitted": "def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):\n    \"\"\"Perform is_fitted validation for estimator.\n\n    Checks if the estimator is fitted by verifying the presence of\n    fitted attributes (ending with a trailing underscore) and otherwise\n    raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to\n    specify if the estimator is fitted or not. See\n    :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\n    for an example on how to use the API.\n\n    If no `attributes` are passed, this function will pass if an estimator is stateless.\n    An estimator can indicate it's stateless by setting the `requires_fit` tag. See\n    :ref:`estimator_tags` for more information. Note that the `requires_fit` tag\n    is ignored if `attributes` are passed.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Raises\n    ------\n    TypeError\n        If the estimator is a class or not an estimator instance\n\n    NotFittedError\n        If the attributes are not found.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.utils.validation import check_is_fitted\n    >>> from sklearn.exceptions import NotFittedError\n    >>> lr = LogisticRegression()\n    >>> try:\n    ...     check_is_fitted(lr)\n    ... except NotFittedError as exc:\n    ...     print(f\"Model is not fitted yet.\")\n    Model is not fitted yet.\n    >>> lr.fit([[1, 2], [1, 3]], [1, 0])\n    LogisticRegression()\n    >>> check_is_fitted(lr)\n    \"\"\"\n    if isclass(estimator):\n        raise TypeError(\"{} is a class, not an instance.\".format(estimator))\n    if msg is None:\n        msg = (\n            \"This %(name)s instance is not fitted yet. Call 'fit' with \"\n            \"appropriate arguments before using this estimator.\"\n        )\n\n    if not hasattr(estimator, \"fit\"):\n        raise TypeError(\"%s is not an estimator instance.\" % (estimator))\n\n    tags = get_tags(estimator)\n\n    if not tags.requires_fit and attributes is None:\n        return\n\n    if not _is_fitted(estimator, attributes, all_or_any):\n        raise NotFittedError(msg % {\"name\": type(estimator).__name__})",
    "scikit-learn.sklearn.utils.validation._get_feature_names": "def _get_feature_names(X):\n    \"\"\"Get feature names from X.\n\n    Support for other array containers should place its implementation here.\n\n    Parameters\n    ----------\n    X : {ndarray, dataframe} of shape (n_samples, n_features)\n        Array container to extract feature names.\n\n        - pandas dataframe : The columns will be considered to be feature\n          names. If the dataframe contains non-string feature names, `None` is\n          returned.\n        - All other array containers will return `None`.\n\n    Returns\n    -------\n    names: ndarray or None\n        Feature names of `X`. Unrecognized array containers will return `None`.\n    \"\"\"\n    feature_names = None\n\n    # extract feature names for support array containers\n    if _is_pandas_df(X):\n        # Make sure we can inspect columns names from pandas, even with\n        # versions too old to expose a working implementation of\n        # __dataframe__.column_names() and avoid introducing any\n        # additional copy.\n        # TODO: remove the pandas-specific branch once the minimum supported\n        # version of pandas has a working implementation of\n        # __dataframe__.column_names() that is guaranteed to not introduce any\n        # additional copy of the data without having to impose allow_copy=False\n        # that could fail with other libraries. Note: in the longer term, we\n        # could decide to instead rely on the __dataframe_namespace__ API once\n        # adopted by our minimally supported pandas version.\n        feature_names = np.asarray(X.columns, dtype=object)\n    elif hasattr(X, \"__dataframe__\"):\n        df_protocol = X.__dataframe__()\n        feature_names = np.asarray(list(df_protocol.column_names()), dtype=object)\n\n    if feature_names is None or len(feature_names) == 0:\n        return\n\n    types = sorted(t.__qualname__ for t in set(type(v) for v in feature_names))\n\n    # mixed type of string and non-string is not supported\n    if len(types) > 1 and \"str\" in types:\n        raise TypeError(\n            \"Feature names are only supported if all input features have string names, \"\n            f\"but your input has {types} as feature name / column name types. \"\n            \"If you want feature names to be stored and validated, you must convert \"\n            \"them all to strings, by using X.columns = X.columns.astype(str) for \"\n            \"example. Otherwise you can remove feature / column names from your input \"\n            \"data, or convert them all to a non-string data type.\"\n        )\n\n    # Only feature names of all strings are supported\n    if len(types) == 1 and types[0] == \"str\":\n        return feature_names",
    "scikit-learn.sklearn.utils.validation._check_n_features": "def _check_n_features(estimator, X, reset):\n    \"\"\"Set the `n_features_in_` attribute, or check against it on an estimator.\n\n    .. note::\n        To only check n_features without conducting a full data validation, prefer\n        using `validate_data(..., skip_check_array=True)` if possible.\n\n    .. versionchanged:: 1.6\n        Moved from :class:`~sklearn.base.BaseEstimator` to\n        :mod:`~sklearn.utils.validation`.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        The estimator to validate the input for.\n\n    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        The input samples.\n\n    reset : bool\n        Whether to reset the `n_features_in_` attribute.\n        If True, the `n_features_in_` attribute is set to `X.shape[1]`.\n        If False and the attribute exists, then check that it is equal to\n        `X.shape[1]`. If False and the attribute does *not* exist, then\n        the check is skipped.\n\n        .. note::\n           It is recommended to call `reset=True` in `fit` and in the first\n           call to `partial_fit`. All other methods that validate `X`\n           should set `reset=False`.\n    \"\"\"\n    try:\n        n_features = _num_features(X)\n    except TypeError as e:\n        if not reset and hasattr(estimator, \"n_features_in_\"):\n            raise ValueError(\n                \"X does not contain any features, but \"\n                f\"{estimator.__class__.__name__} is expecting \"\n                f\"{estimator.n_features_in_} features\"\n            ) from e\n        # If the number of features is not defined and reset=True,\n        # then we skip this check\n        return\n\n    if reset:\n        estimator.n_features_in_ = n_features\n        return\n\n    if not hasattr(estimator, \"n_features_in_\"):\n        # Skip this check if the expected number of expected input features\n        # was not recorded by calling fit first. This is typically the case\n        # for stateless transformers.\n        return\n\n    if n_features != estimator.n_features_in_:\n        raise ValueError(\n            f\"X has {n_features} features, but {estimator.__class__.__name__} \"\n            f\"is expecting {estimator.n_features_in_} features as input.\"\n        )",
    "scikit-learn.sklearn.utils.validation._num_samples": "def _num_samples(x):\n    \"\"\"Return number of samples in array-like x.\"\"\"\n    message = \"Expected sequence or array-like, got %s\" % type(x)\n    if hasattr(x, \"fit\") and callable(x.fit):\n        # Don't get num_samples from an ensembles length!\n        raise TypeError(message)\n\n    if _use_interchange_protocol(x):\n        return x.__dataframe__().num_rows()\n\n    if not hasattr(x, \"__len__\") and not hasattr(x, \"shape\"):\n        if hasattr(x, \"__array__\"):\n            xp, _ = get_namespace(x)\n            x = xp.asarray(x)\n        else:\n            raise TypeError(message)\n\n    if hasattr(x, \"shape\") and x.shape is not None:\n        if len(x.shape) == 0:\n            raise TypeError(\n                \"Input should have at least 1 dimension i.e. satisfy \"\n                f\"`len(x.shape) > 0`, got scalar `{x!r}` instead.\"\n            )\n        # Check that shape is returning an integer or default to len\n        # Dask dataframes may not return numeric shape[0] value\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n\n    try:\n        return len(x)\n    except TypeError as type_error:\n        raise TypeError(message) from type_error"
}