{
    "scikit-learn.sklearn.base.wrapper": "@functools.wraps(fit_method)\ndef wrapper(estimator, *args, **kwargs):\n    global_skip_validation = get_config()[\"skip_parameter_validation\"]\n\n    # we don't want to validate again for each call to partial_fit\n    partial_fit_and_fitted = (\n        fit_method.__name__ == \"partial_fit\" and _is_fitted(estimator)\n    )\n\n    if not global_skip_validation and not partial_fit_and_fitted:\n        estimator._validate_params()\n\n    with config_context(\n        skip_parameter_validation=(\n            prefer_skip_nested_validation or global_skip_validation\n        )\n    ):\n        return fit_method(estimator, *args, **kwargs)",
    "scikit-learn.sklearn.dummy.predict": "def predict(self, X, return_std=False):\n    \"\"\"Perform classification on test vectors X.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Test data.\n\n    return_std : bool, default=False\n        Whether to return the standard deviation of posterior prediction.\n        All zeros in this case.\n\n        .. versionadded:: 0.20\n\n    Returns\n    -------\n    y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n        Predicted target values for X.\n\n    y_std : array-like of shape (n_samples,) or (n_samples, n_outputs)\n        Standard deviation of predictive distribution of query points.\n    \"\"\"\n    check_is_fitted(self)\n    n_samples = _num_samples(X)\n\n    y = np.full(\n        (n_samples, self.n_outputs_),\n        self.constant_,\n        dtype=np.array(self.constant_).dtype,\n    )\n    y_std = np.zeros((n_samples, self.n_outputs_))\n\n    if self.n_outputs_ == 1:\n        y = np.ravel(y)\n        y_std = np.ravel(y_std)\n\n    return (y, y_std) if return_std else y",
    "scikit-learn.sklearn.ensemble._base._make_estimator": "def _make_estimator(self, append=True, random_state=None):\n    \"\"\"Make and configure a copy of the `estimator_` attribute.\n\n    Warning: This method should be used to properly instantiate new\n    sub-estimators.\n    \"\"\"\n    estimator = clone(self.estimator_)\n    estimator.set_params(**{p: getattr(self, p) for p in self.estimator_params})\n\n    if random_state is not None:\n        _set_random_states(estimator, random_state)\n\n    if append:\n        self.estimators_.append(estimator)\n\n    return estimator",
    "scikit-learn.sklearn.ensemble._forest.predict": "def predict(self, X):\n    \"\"\"\n    Predict regression target for X.\n\n    The predicted regression target of an input sample is computed as the\n    mean predicted regression targets of the trees in the forest.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The input samples. Internally, its dtype will be converted to\n        ``dtype=np.float32``. If a sparse matrix is provided, it will be\n        converted into a sparse ``csr_matrix``.\n\n    Returns\n    -------\n    y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n        The predicted values.\n    \"\"\"\n    check_is_fitted(self)\n    # Check data\n    X = self._validate_X_predict(X)\n\n    # Assign chunk of trees to jobs\n    n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)\n\n    # avoid storing the output of every estimator by summing them here\n    if self.n_outputs_ > 1:\n        y_hat = np.zeros((X.shape[0], self.n_outputs_), dtype=np.float64)\n    else:\n        y_hat = np.zeros((X.shape[0]), dtype=np.float64)\n\n    # Parallel loop\n    lock = threading.Lock()\n    Parallel(n_jobs=n_jobs, verbose=self.verbose, require=\"sharedmem\")(\n        delayed(_accumulate_prediction)(e.predict, X, [y_hat], lock)\n        for e in self.estimators_\n    )\n\n    y_hat /= len(self.estimators_)\n\n    return y_hat",
    "scikit-learn.sklearn.linear_model._base.predict": "def predict(self, X):\n    \"\"\"\n    Predict using the linear model.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape (n_samples, n_features)\n        Samples.\n\n    Returns\n    -------\n    C : array, shape (n_samples,)\n        Returns predicted values.\n    \"\"\"\n    return self._decision_function(X)",
    "scikit-learn.sklearn.svm._base.predict": "def predict(self, X):\n    \"\"\"Perform regression on samples in X.\n\n    For a one-class model, +1 (inlier) or -1 (outlier) is returned.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        For kernel=\"precomputed\", the expected shape of X is\n        (n_samples_test, n_samples_train).\n\n    Returns\n    -------\n    y_pred : ndarray of shape (n_samples,)\n        The predicted values.\n    \"\"\"\n    X = self._validate_for_predict(X)\n    predict = self._sparse_predict if self._sparse else self._dense_predict\n    return predict(X)",
    "scikit-learn.sklearn.tree._classes.predict": "def predict(self, X, check_input=True):\n    \"\"\"Predict class or regression value for X.\n\n    For a classification model, the predicted class for each sample in X is\n    returned. For a regression model, the predicted value based on X is\n    returned.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The input samples. Internally, it will be converted to\n        ``dtype=np.float32`` and if a sparse matrix is provided\n        to a sparse ``csr_matrix``.\n\n    check_input : bool, default=True\n        Allow to bypass several input checking.\n        Don't use this parameter unless you know what you're doing.\n\n    Returns\n    -------\n    y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n        The predicted classes, or the predict values.\n    \"\"\"\n    check_is_fitted(self)\n    X = self._validate_X_predict(X, check_input)\n    proba = self.tree_.predict(X)\n    n_samples = X.shape[0]\n\n    # Classification\n    if is_classifier(self):\n        if self.n_outputs_ == 1:\n            return self.classes_.take(np.argmax(proba, axis=1), axis=0)\n\n        else:\n            class_type = self.classes_[0].dtype\n            predictions = np.zeros((n_samples, self.n_outputs_), dtype=class_type)\n            for k in range(self.n_outputs_):\n                predictions[:, k] = self.classes_[k].take(\n                    np.argmax(proba[:, k], axis=1), axis=0\n                )\n\n            return predictions\n\n    # Regression\n    else:\n        if self.n_outputs_ == 1:\n            return proba[:, 0]\n\n        else:\n            return proba[:, :, 0]",
    "scikit-learn.sklearn.utils._indexing._safe_indexing": "def _safe_indexing(X, indices, *, axis=0):\n    \"\"\"Return rows, items or columns of X using indices.\n\n    .. warning::\n\n        This utility is documented, but **private**. This means that\n        backward compatibility might be broken without any deprecation\n        cycle.\n\n    Parameters\n    ----------\n    X : array-like, sparse-matrix, list, pandas.DataFrame, pandas.Series\n        Data from which to sample rows, items or columns. `list` are only\n        supported when `axis=0`.\n    indices : bool, int, str, slice, array-like\n        - If `axis=0`, boolean and integer array-like, integer slice,\n          and scalar integer are supported.\n        - If `axis=1`:\n            - to select a single column, `indices` can be of `int` type for\n              all `X` types and `str` only for dataframe. The selected subset\n              will be 1D, unless `X` is a sparse matrix in which case it will\n              be 2D.\n            - to select multiples columns, `indices` can be one of the\n              following: `list`, `array`, `slice`. The type used in\n              these containers can be one of the following: `int`, 'bool' and\n              `str`. However, `str` is only supported when `X` is a dataframe.\n              The selected subset will be 2D.\n    axis : int, default=0\n        The axis along which `X` will be subsampled. `axis=0` will select\n        rows while `axis=1` will select columns.\n\n    Returns\n    -------\n    subset\n        Subset of X on axis 0 or 1.\n\n    Notes\n    -----\n    CSR, CSC, and LIL sparse matrices are supported. COO sparse matrices are\n    not supported.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.utils import _safe_indexing\n    >>> data = np.array([[1, 2], [3, 4], [5, 6]])\n    >>> _safe_indexing(data, 0, axis=0)  # select the first row\n    array([1, 2])\n    >>> _safe_indexing(data, 0, axis=1)  # select the first column\n    array([1, 3, 5])\n    \"\"\"\n    if indices is None:\n        return X\n\n    if axis not in (0, 1):\n        raise ValueError(\n            \"'axis' should be either 0 (to index rows) or 1 (to index \"\n            \" column). Got {} instead.\".format(axis)\n        )\n\n    indices_dtype = _determine_key_type(indices)\n\n    if axis == 0 and indices_dtype == \"str\":\n        raise ValueError(\n            f\"String indexing (indices={indices}) is not supported with 'axis=0'. \"\n            \"Did you mean to use axis=1 for column selection?\"\n        )\n\n    if axis == 1 and isinstance(X, list):\n        raise ValueError(\"axis=1 is not supported for lists\")\n\n    if axis == 1 and (ndim := len(getattr(X, \"shape\", [0]))) != 2:\n        raise ValueError(\n            \"'X' should be a 2D NumPy array, 2D sparse matrix or \"\n            \"dataframe when indexing the columns (i.e. 'axis=1'). \"\n            f\"Got {type(X)} instead with {ndim} dimension(s).\"\n        )\n\n    if (\n        axis == 1\n        and indices_dtype == \"str\"\n        and not (_is_pandas_df(X) or _use_interchange_protocol(X))\n    ):\n        raise ValueError(\n            \"Specifying the columns using strings is only supported for dataframes.\"\n        )\n\n    if hasattr(X, \"iloc\"):\n        # TODO: we should probably use _is_pandas_df_or_series(X) instead but:\n        # 1) Currently, it (probably) works for dataframes compliant to pandas' API.\n        # 2) Updating would require updating some tests such as\n        #    test_train_test_split_mock_pandas.\n        return _pandas_indexing(X, indices, indices_dtype, axis=axis)\n    elif _is_polars_df_or_series(X):\n        return _polars_indexing(X, indices, indices_dtype, axis=axis)\n    elif _is_pyarrow_data(X):\n        return _pyarrow_indexing(X, indices, indices_dtype, axis=axis)\n    elif _use_interchange_protocol(X):  # pragma: no cover\n        # Once the dataframe X is converted into its dataframe interchange protocol\n        # version by calling X.__dataframe__(), it becomes very hard to turn it back\n        # into its original type, e.g., a pyarrow.Table, see\n        # https://github.com/data-apis/dataframe-api/issues/85.\n        raise warnings.warn(\n            message=\"A data object with support for the dataframe interchange protocol\"\n            \"was passed, but scikit-learn does currently not know how to handle this \"\n            \"kind of data. Some array/list indexing will be tried.\",\n            category=UserWarning,\n        )\n\n    if hasattr(X, \"shape\"):\n        return _array_indexing(X, indices, indices_dtype, axis=axis)\n    else:\n        return _list_indexing(X, indices, indices_dtype)",
    "scikit-learn.sklearn.utils.validation._num_samples": "def _num_samples(x):\n    \"\"\"Return number of samples in array-like x.\"\"\"\n    message = \"Expected sequence or array-like, got %s\" % type(x)\n    if hasattr(x, \"fit\") and callable(x.fit):\n        # Don't get num_samples from an ensembles length!\n        raise TypeError(message)\n\n    if _use_interchange_protocol(x):\n        return x.__dataframe__().num_rows()\n\n    if not hasattr(x, \"__len__\") and not hasattr(x, \"shape\"):\n        if hasattr(x, \"__array__\"):\n            xp, _ = get_namespace(x)\n            x = xp.asarray(x)\n        else:\n            raise TypeError(message)\n\n    if hasattr(x, \"shape\") and x.shape is not None:\n        if len(x.shape) == 0:\n            raise TypeError(\n                \"Input should have at least 1 dimension i.e. satisfy \"\n                f\"`len(x.shape) > 0`, got scalar `{x!r}` instead.\"\n            )\n        # Check that shape is returning an integer or default to len\n        # Dask dataframes may not return numeric shape[0] value\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n\n    try:\n        return len(x)\n    except TypeError as type_error:\n        raise TypeError(message) from type_error"
}