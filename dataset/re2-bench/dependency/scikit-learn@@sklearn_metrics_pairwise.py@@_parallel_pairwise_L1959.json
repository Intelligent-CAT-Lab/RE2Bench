{
    "scikit-learn.sklearn.externals.array_api_compat._internal.wrapped_f": "@wraps(f)\ndef wrapped_f(*args: object, **kwargs: object) -> object:\n    return f(*args, xp=xp, **kwargs)",
    "scikit-learn.sklearn.gaussian_process.kernels.__call__": "def __call__(self, X, Y=None, eval_gradient=False):\n    \"\"\"Return the kernel k(X, Y) and optionally its gradient.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples_X, n_features) or list of object\n        Left argument of the returned kernel k(X, Y)\n\n    Y : array-like of shape (n_samples_Y, n_features) or list of object,\\\n        default=None\n        Right argument of the returned kernel k(X, Y). If None, k(X, X)\n        is evaluated instead.\n\n    eval_gradient : bool, default=False\n        Determines whether the gradient with respect to the log of\n        the kernel hyperparameter is computed.\n\n    Returns\n    -------\n    K : ndarray of shape (n_samples_X, n_samples_Y)\n        Kernel k(X, Y)\n\n    K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims), \\\n            optional\n        The gradient of the kernel k(X, X) with respect to the log of the\n        hyperparameter of the kernel. Only returned when `eval_gradient`\n        is True.\n    \"\"\"\n    if eval_gradient:\n        K1, K1_gradient = self.k1(X, Y, eval_gradient=True)\n        K2, K2_gradient = self.k2(X, Y, eval_gradient=True)\n        return K1 * K2, np.dstack(\n            (K1_gradient * K2[:, :, np.newaxis], K2_gradient * K1[:, :, np.newaxis])\n        )\n    else:\n        return self.k1(X, Y) * self.k2(X, Y)",
    "scikit-learn.sklearn.metrics.pairwise._pairwise_callable": "def _pairwise_callable(X, Y, metric, ensure_all_finite=True, **kwds):\n    \"\"\"Handle the callable case for pairwise_{distances,kernels}.\"\"\"\n    xp, _, device = get_namespace_and_device(X)\n    X, Y = check_pairwise_arrays(\n        X,\n        Y,\n        dtype=None,\n        ensure_all_finite=ensure_all_finite,\n        # No input dimension checking done for custom metrics (left to user)\n        ensure_2d=False,\n    )\n    _, _, dtype_float = _find_floating_dtype_allow_sparse(X, Y, xp=xp)\n\n    def _get_slice(array, index):\n        # TODO: below 2 lines can be removed once min scipy >= 1.14. Support for\n        # 1D shapes in scipy sparse arrays (COO, DOK and CSR formats) only\n        # added in 1.14. We must return 2D array until min scipy 1.14.\n        if issparse(array):\n            return array[[index], :]\n        # When `metric` is a callable, 1D input arrays allowed, in which case\n        # scalar should be returned.\n        if array.ndim == 1:\n            return array[index]\n        else:\n            return array[index, ...]\n\n    if X is Y:\n        # Only calculate metric for upper triangle\n        out = xp.zeros((X.shape[0], Y.shape[0]), dtype=dtype_float, device=device)\n        iterator = itertools.combinations(range(X.shape[0]), 2)\n        for i, j in iterator:\n            x = _get_slice(X, i)\n            y = _get_slice(Y, j)\n            out[i, j] = metric(x, y, **kwds)\n\n        # Make symmetric\n        # NB: out += out.T will produce incorrect results\n        out = out + out.T\n\n        # Calculate diagonal\n        # NB: nonzero diagonals are allowed for both metrics and kernels\n        for i in range(X.shape[0]):\n            x = _get_slice(X, i)\n            out[i, i] = metric(x, x, **kwds)\n\n    else:\n        # Calculate all cells\n        out = xp.empty((X.shape[0], Y.shape[0]), dtype=dtype_float)\n        iterator = itertools.product(range(X.shape[0]), range(Y.shape[0]))\n        for i, j in iterator:\n            x = _get_slice(X, i)\n            y = _get_slice(Y, j)\n            out[i, j] = metric(x, y, **kwds)\n\n    return out",
    "scikit-learn.sklearn.metrics.pairwise._find_floating_dtype_allow_sparse": "def _find_floating_dtype_allow_sparse(X, Y, xp=None):\n    \"\"\"Find matching floating type, allowing for sparse input.\"\"\"\n    if any([issparse(X), issparse(Y)]) or _is_numpy_namespace(xp):\n        X, Y, dtype_float = _return_float_dtype(X, Y)\n    else:\n        dtype_float = _find_matching_floating_dtype(X, Y, xp=xp)\n    return X, Y, dtype_float",
    "scikit-learn.sklearn.utils._array_api.get_namespace_and_device": "def get_namespace_and_device(\n    *array_list, remove_none=True, remove_types=(str,), xp=None\n):\n    \"\"\"Combination into one single function of `get_namespace` and `device`.\n\n    Parameters\n    ----------\n    *array_list : array objects\n        Array objects.\n    remove_none : bool, default=True\n        Whether to ignore None objects passed in arrays.\n    remove_types : tuple or list, default=(str,)\n        Types to ignore in the arrays.\n    xp : module, default=None\n        Precomputed array namespace module. When passed, typically from a caller\n        that has already performed inspection of its own inputs, skips array\n        namespace inspection.\n\n    Returns\n    -------\n    namespace : module\n        Namespace shared by array objects. If any of the `arrays` are not arrays,\n        the namespace defaults to NumPy.\n    is_array_api_compliant : bool\n        True if the arrays are containers that implement the Array API spec.\n        Always False when array_api_dispatch=False.\n    device : device\n        `device` object (see the \"Device Support\" section of the array API spec).\n    \"\"\"\n    skip_remove_kwargs = dict(remove_none=False, remove_types=[])\n\n    array_list = _remove_non_arrays(\n        *array_list,\n        remove_none=remove_none,\n        remove_types=remove_types,\n    )\n    arrays_device = device(*array_list, **skip_remove_kwargs)\n\n    if xp is None:\n        xp, is_array_api = get_namespace(*array_list, **skip_remove_kwargs)\n    else:\n        xp, is_array_api = xp, True\n\n    if is_array_api:\n        return xp, is_array_api, arrays_device\n    else:\n        return xp, False, arrays_device",
    "scikit-learn.sklearn.utils._array_api._fill_diagonal": "def _fill_diagonal(array, value, xp):\n    \"\"\"Minimal implementation of `numpy.fill_diagonal`.\n\n    `wrap` is not supported (i.e. always False). `value` should be a scalar or\n    1D of greater or equal length as the diagonal (i.e., `value` is never repeated\n    when shorter).\n\n    Note `array` is altered in place.\n    \"\"\"\n    value, min_rows_columns = _validate_diagonal_args(array, value, xp)\n\n    if _is_numpy_namespace(xp):\n        xp.fill_diagonal(array, value, wrap=False)\n    else:\n        # TODO: when array libraries support `reshape(copy)`, use\n        # `reshape(array, (-1,), copy=False)`, then fill with `[:end:step]` (within\n        # `try/except`). This is faster than for loop, when no copy needs to be\n        # made within `reshape`. See #31445 for details.\n        if value.ndim == 0:\n            for i in range(min_rows_columns):\n                array[i, i] = value\n        else:\n            for i in range(min_rows_columns):\n                array[i, i] = value[i]",
    "scikit-learn.sklearn.utils._param_validation.wrapper": "@functools.wraps(func)\ndef wrapper(*args, **kwargs):\n    global_skip_validation = get_config()[\"skip_parameter_validation\"]\n    if global_skip_validation:\n        return func(*args, **kwargs)\n\n    func_sig = signature(func)\n\n    # Map *args/**kwargs to the function signature\n    params = func_sig.bind(*args, **kwargs)\n    params.apply_defaults()\n\n    # ignore self/cls and positional/keyword markers\n    to_ignore = [\n        p.name\n        for p in func_sig.parameters.values()\n        if p.kind in (p.VAR_POSITIONAL, p.VAR_KEYWORD)\n    ]\n    to_ignore += [\"self\", \"cls\"]\n    params = {k: v for k, v in params.arguments.items() if k not in to_ignore}\n\n    validate_parameter_constraints(\n        parameter_constraints, params, caller_name=func.__qualname__\n    )\n\n    try:\n        with config_context(\n            skip_parameter_validation=(\n                prefer_skip_nested_validation or global_skip_validation\n            )\n        ):\n            return func(*args, **kwargs)\n    except InvalidParameterError as e:\n        # When the function is just a wrapper around an estimator, we allow\n        # the function to delegate validation to the estimator, but we replace\n        # the name of the estimator by the name of the function in the error\n        # message to avoid confusion.\n        msg = re.sub(\n            r\"parameter of \\w+ must be\",\n            f\"parameter of {func.__qualname__} must be\",\n            str(e),\n        )\n        raise InvalidParameterError(msg) from e",
    "scikit-learn.sklearn.utils.parallel.__call__": "def __call__(self, iterable):\n    \"\"\"Dispatch the tasks and return the results.\n\n    Parameters\n    ----------\n    iterable : iterable\n        Iterable containing tuples of (delayed_function, args, kwargs) that should\n        be consumed.\n\n    Returns\n    -------\n    results : list\n        List of results of the tasks.\n    \"\"\"\n    # Capture the thread-local scikit-learn configuration at the time\n    # Parallel.__call__ is issued since the tasks can be dispatched\n    # in a different thread depending on the backend and on the value of\n    # pre_dispatch and n_jobs.\n    config = get_config()\n    # In free-threading Python >= 3.14, warnings filters are managed through a\n    # ContextVar and warnings.filters is not modified inside a\n    # warnings.catch_warnings context. You need to use warnings._get_filters().\n    # For more details, see\n    # https://docs.python.org/3.14/whatsnew/3.14.html#concurrent-safe-warnings-control\n    filters_func = getattr(warnings, \"_get_filters\", None)\n    warning_filters = (\n        filters_func() if filters_func is not None else warnings.filters\n    )\n\n    iterable_with_config_and_warning_filters = (\n        (\n            _with_config_and_warning_filters(delayed_func, config, warning_filters),\n            args,\n            kwargs,\n        )\n        for delayed_func, args, kwargs in iterable\n    )\n    return super().__call__(iterable_with_config_and_warning_filters)",
    "scikit-learn.sklearn.utils.parallel.delayed": "def delayed(function):\n    \"\"\"Decorator used to capture the arguments of a function.\n\n    This alternative to `joblib.delayed` is meant to be used in conjunction\n    with `sklearn.utils.parallel.Parallel`. The latter captures the scikit-\n    learn configuration by calling `sklearn.get_config()` in the current\n    thread, prior to dispatching the first task. The captured configuration is\n    then propagated and enabled for the duration of the execution of the\n    delayed function in the joblib workers.\n\n    .. versionchanged:: 1.3\n       `delayed` was moved from `sklearn.utils.fixes` to `sklearn.utils.parallel`\n       in scikit-learn 1.3.\n\n    Parameters\n    ----------\n    function : callable\n        The function to be delayed.\n\n    Returns\n    -------\n    output: tuple\n        Tuple containing the delayed function, the positional arguments, and the\n        keyword arguments.\n    \"\"\"\n\n    @functools.wraps(function)\n    def delayed_function(*args, **kwargs):\n        return _FuncWrapper(function), args, kwargs\n\n    return delayed_function",
    "scikit-learn.sklearn.utils.validation._num_samples": "def _num_samples(x):\n    \"\"\"Return number of samples in array-like x.\"\"\"\n    message = \"Expected sequence or array-like, got %s\" % type(x)\n    if hasattr(x, \"fit\") and callable(x.fit):\n        # Don't get num_samples from an ensembles length!\n        raise TypeError(message)\n\n    if _use_interchange_protocol(x):\n        return x.__dataframe__().num_rows()\n\n    if not hasattr(x, \"__len__\") and not hasattr(x, \"shape\"):\n        if hasattr(x, \"__array__\"):\n            xp, _ = get_namespace(x)\n            x = xp.asarray(x)\n        else:\n            raise TypeError(message)\n\n    if hasattr(x, \"shape\") and x.shape is not None:\n        if len(x.shape) == 0:\n            raise TypeError(\n                \"Input should have at least 1 dimension i.e. satisfy \"\n                f\"`len(x.shape) > 0`, got scalar `{x!r}` instead.\"\n            )\n        # Check that shape is returning an integer or default to len\n        # Dask dataframes may not return numeric shape[0] value\n        if isinstance(x.shape[0], numbers.Integral):\n            return x.shape[0]\n\n    try:\n        return len(x)\n    except TypeError as type_error:\n        raise TypeError(message) from type_error"
}