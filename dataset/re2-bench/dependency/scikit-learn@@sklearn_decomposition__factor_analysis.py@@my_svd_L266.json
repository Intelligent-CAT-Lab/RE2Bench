{
    "scikit-learn.sklearn.utils.extmath.squared_norm": "def squared_norm(x):\n    \"\"\"Squared Euclidean or Frobenius norm of x.\n\n    Faster than norm(x) ** 2.\n\n    Parameters\n    ----------\n    x : array-like\n        The input array which could be either be a vector or a 2 dimensional array.\n\n    Returns\n    -------\n    float\n        The Euclidean norm when x is a vector, the Frobenius norm when x\n        is a matrix (2-d array).\n    \"\"\"\n    x = np.ravel(x, order=\"K\")\n    if np.issubdtype(x.dtype, np.integer):\n        warnings.warn(\n            (\n                \"Array type is integer, np.dot may overflow. \"\n                \"Data should be float type to avoid this issue\"\n            ),\n            UserWarning,\n        )\n    return np.dot(x, x)",
    "scikit-learn.sklearn.utils.extmath._randomized_svd": "def _randomized_svd(\n    M,\n    n_components,\n    *,\n    n_oversamples=10,\n    n_iter=\"auto\",\n    power_iteration_normalizer=\"auto\",\n    transpose=\"auto\",\n    flip_sign=True,\n    random_state=None,\n    svd_lapack_driver=\"gesdd\",\n):\n    \"\"\"Body of randomized_svd without input validation.\"\"\"\n    xp, is_array_api_compliant = get_namespace(M)\n\n    if sparse.issparse(M) and M.format in (\"lil\", \"dok\"):\n        warnings.warn(\n            \"Calculating SVD of a {} is expensive. \"\n            \"csr_matrix is more efficient.\".format(type(M).__name__),\n            sparse.SparseEfficiencyWarning,\n        )\n\n    random_state = check_random_state(random_state)\n    n_random = n_components + n_oversamples\n    n_samples, n_features = M.shape\n\n    if n_iter == \"auto\":\n        # Checks if the number of iterations is explicitly specified\n        # Adjust n_iter. 7 was found a good compromise for PCA. See #5299\n        n_iter = 7 if n_components < 0.1 * min(M.shape) else 4\n\n    if transpose == \"auto\":\n        transpose = n_samples < n_features\n    if transpose:\n        # this implementation is a bit faster with smaller shape[1]\n        M = M.T\n\n    Q = _randomized_range_finder(\n        M,\n        size=n_random,\n        n_iter=n_iter,\n        power_iteration_normalizer=power_iteration_normalizer,\n        random_state=random_state,\n    )\n\n    # project M to the (k + p) dimensional space using the basis vectors\n    B = Q.T @ M\n\n    # compute the SVD on the thin matrix: (k + p) wide\n    if is_array_api_compliant:\n        Uhat, s, Vt = xp.linalg.svd(B, full_matrices=False)\n    else:\n        # When array_api_dispatch is disabled, rely on scipy.linalg\n        # instead of numpy.linalg to avoid introducing a behavior change w.r.t.\n        # previous versions of scikit-learn.\n        Uhat, s, Vt = linalg.svd(\n            B, full_matrices=False, lapack_driver=svd_lapack_driver\n        )\n    del B\n    U = Q @ Uhat\n\n    if flip_sign:\n        if not transpose:\n            U, Vt = svd_flip(U, Vt)\n        else:\n            # In case of transpose u_based_decision=false\n            # to actually flip based on u and not v.\n            U, Vt = svd_flip(U, Vt, u_based_decision=False)\n\n    if transpose:\n        # transpose back the results according to the input convention\n        return Vt[:n_components, :].T, s[:n_components], U[:, :n_components].T\n    else:\n        return U[:, :n_components], s[:n_components], Vt[:n_components, :]"
}