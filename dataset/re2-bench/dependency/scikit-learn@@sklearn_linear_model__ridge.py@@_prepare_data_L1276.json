{
    "scikit-learn.sklearn.linear_model._ridge._get_valid_accept_sparse": "def _get_valid_accept_sparse(is_X_sparse, solver):\n    if is_X_sparse and solver in [\"auto\", \"sag\", \"saga\"]:\n        return \"csr\"\n    else:\n        return [\"csr\", \"csc\", \"coo\"]",
    "scikit-learn.sklearn.preprocessing._label.__init__": "def __init__(self, *, neg_label=0, pos_label=1, sparse_output=False):\n    self.neg_label = neg_label\n    self.pos_label = pos_label\n    self.sparse_output = sparse_output",
    "scikit-learn.sklearn.preprocessing._label.fit_transform": "def fit_transform(self, y):\n    \"\"\"Fit label binarizer/transform multi-class labels to binary labels.\n\n    The output of transform is sometimes referred to as\n    the 1-of-K coding scheme.\n\n    Parameters\n    ----------\n    y : {ndarray, sparse matrix} of shape (n_samples,) or \\\n            (n_samples, n_classes)\n        Target values. The 2-d matrix should only contain 0 and 1,\n        represents multilabel classification. Sparse matrix can be\n        CSR, CSC, COO, DOK, or LIL.\n\n    Returns\n    -------\n    Y : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n        Shape will be (n_samples, 1) for binary problems. Sparse matrix\n        will be of CSR format.\n    \"\"\"\n    return self.fit(y).transform(y)",
    "scikit-learn.sklearn.utils._array_api.get_namespace": "def get_namespace(*arrays, remove_none=True, remove_types=(str,), xp=None):\n    \"\"\"Get namespace of arrays.\n\n    Introspect `arrays` arguments and return their common Array API compatible\n    namespace object, if any.\n\n    Note that sparse arrays are filtered by default.\n\n    See: https://numpy.org/neps/nep-0047-array-api-standard.html\n\n    If `arrays` are regular numpy arrays, `array_api_compat.numpy` is returned instead.\n\n    Namespace support is not enabled by default. To enabled it call:\n\n      sklearn.set_config(array_api_dispatch=True)\n\n    or:\n\n      with sklearn.config_context(array_api_dispatch=True):\n          # your code here\n\n    Otherwise `array_api_compat.numpy` is\n    always returned irrespective of the fact that arrays implement the\n    `__array_namespace__` protocol or not.\n\n    Note that if no arrays pass the set filters, ``_NUMPY_API_WRAPPER_INSTANCE, False``\n    is returned.\n\n    Parameters\n    ----------\n    *arrays : array objects\n        Array objects.\n\n    remove_none : bool, default=True\n        Whether to ignore None objects passed in arrays.\n\n    remove_types : tuple or list, default=(str,)\n        Types to ignore in the arrays.\n\n    xp : module, default=None\n        Precomputed array namespace module. When passed, typically from a caller\n        that has already performed inspection of its own inputs, skips array\n        namespace inspection.\n\n    Returns\n    -------\n    namespace : module\n        Namespace shared by array objects. If any of the `arrays` are not arrays,\n        the namespace defaults to the NumPy namespace.\n\n    is_array_api_compliant : bool\n        True if the arrays are containers that implement the array API spec (see\n        https://data-apis.org/array-api/latest/index.html).\n        Always False when array_api_dispatch=False.\n    \"\"\"\n    array_api_dispatch = get_config()[\"array_api_dispatch\"]\n    if not array_api_dispatch:\n        if xp is not None:\n            return xp, False\n        else:\n            return np_compat, False\n\n    if xp is not None:\n        return xp, True\n\n    arrays = _remove_non_arrays(\n        *arrays,\n        remove_none=remove_none,\n        remove_types=remove_types,\n    )\n\n    if not arrays:\n        return np_compat, False\n\n    _check_array_api_dispatch(array_api_dispatch)\n\n    namespace, is_array_api_compliant = array_api_compat.get_namespace(*arrays), True\n\n    if namespace.__name__ == \"array_api_strict\" and hasattr(\n        namespace, \"set_array_api_strict_flags\"\n    ):\n        namespace.set_array_api_strict_flags(api_version=\"2024.12\")\n\n    return namespace, is_array_api_compliant",
    "scikit-learn.sklearn.utils._array_api.get_namespace_and_device": "def get_namespace_and_device(\n    *array_list, remove_none=True, remove_types=(str,), xp=None\n):\n    \"\"\"Combination into one single function of `get_namespace` and `device`.\n\n    Parameters\n    ----------\n    *array_list : array objects\n        Array objects.\n    remove_none : bool, default=True\n        Whether to ignore None objects passed in arrays.\n    remove_types : tuple or list, default=(str,)\n        Types to ignore in the arrays.\n    xp : module, default=None\n        Precomputed array namespace module. When passed, typically from a caller\n        that has already performed inspection of its own inputs, skips array\n        namespace inspection.\n\n    Returns\n    -------\n    namespace : module\n        Namespace shared by array objects. If any of the `arrays` are not arrays,\n        the namespace defaults to NumPy.\n    is_array_api_compliant : bool\n        True if the arrays are containers that implement the Array API spec.\n        Always False when array_api_dispatch=False.\n    device : device\n        `device` object (see the \"Device Support\" section of the array API spec).\n    \"\"\"\n    skip_remove_kwargs = dict(remove_none=False, remove_types=[])\n\n    array_list = _remove_non_arrays(\n        *array_list,\n        remove_none=remove_none,\n        remove_types=remove_types,\n    )\n    arrays_device = device(*array_list, **skip_remove_kwargs)\n\n    if xp is None:\n        xp, is_array_api = get_namespace(*array_list, **skip_remove_kwargs)\n    else:\n        xp, is_array_api = xp, True\n\n    if is_array_api:\n        return xp, is_array_api, arrays_device\n    else:\n        return xp, False, arrays_device",
    "scikit-learn.sklearn.utils._array_api.move_to": "def move_to(*arrays, xp, device):\n    \"\"\"Move all arrays to `xp` and `device`.\n\n    Each array will be moved to the reference namespace and device if\n    it is not already using it. Otherwise the array is left unchanged.\n\n    `array` may contain `None` entries, these are left unchanged.\n\n    Sparse arrays are accepted (as pass through) if the reference namespace is\n    Numpy, in which case they are returned unchanged. Otherwise a `TypeError`\n    is raised.\n\n    Parameters\n    ----------\n    *arrays : iterable of arrays\n        Arrays to (potentially) move.\n\n    xp : namespace\n        Array API namespace to move arrays to.\n\n    device : device\n        Array API device to move arrays to.\n\n    Returns\n    -------\n    arrays : tuple or array\n        Tuple of arrays with the same namespace and device as reference. Single array\n        returned if only one `arrays` input.\n    \"\"\"\n    sparse_mask = [sp.issparse(array) for array in arrays]\n    none_mask = [array is None for array in arrays]\n    if any(sparse_mask) and not _is_numpy_namespace(xp):\n        raise TypeError(\n            \"Sparse arrays are only accepted (and passed through) when the target \"\n            \"namespace is Numpy\"\n        )\n\n    converted_arrays = []\n\n    for array, is_sparse, is_none in zip(arrays, sparse_mask, none_mask):\n        if is_none:\n            converted_arrays.append(None)\n        elif is_sparse:\n            converted_arrays.append(array)\n        else:\n            xp_array, _, device_array = get_namespace_and_device(array)\n            if xp == xp_array and device == device_array:\n                converted_arrays.append(array)\n            else:\n                try:\n                    # The dlpack protocol is the future proof and library agnostic\n                    # method to transfer arrays across namespace and device boundaries\n                    # hence this method is attempted first and going through NumPy is\n                    # only used as fallback in case of failure.\n                    # Note: copy=None is the default since array-api 2023.12. Namespace\n                    # libraries should only trigger a copy automatically if needed.\n                    array_converted = xp.from_dlpack(array, device=device)\n                    # `AttributeError` occurs when `__dlpack__` and `__dlpack_device__`\n                    # methods are not present on the input array\n                    # `TypeError` and `NotImplementedError` for packages that do not\n                    # yet support dlpack 1.0\n                    # (i.e. the `device`/`copy` kwargs, e.g., torch <= 2.8.0)\n                    # See https://github.com/data-apis/array-api/pull/741 for\n                    # more details about the introduction of the `copy` and `device`\n                    # kwargs in the from_dlpack method and their expected\n                    # meaning by namespaces implementing the array API spec.\n                    # TODO: try removing this once DLPack v1 more widely supported\n                    # TODO: ValueError should not be needed but is in practice:\n                    # https://github.com/numpy/numpy/issues/30341\n                except (\n                    AttributeError,\n                    TypeError,\n                    NotImplementedError,\n                    BufferError,\n                    ValueError,\n                ):\n                    # Converting to numpy is tricky, handle this via dedicated function\n                    if _is_numpy_namespace(xp):\n                        array_converted = _convert_to_numpy(array, xp_array)\n                    # Convert from numpy, all array libraries can do this\n                    elif _is_numpy_namespace(xp_array):\n                        array_converted = xp.asarray(array, device=device)\n                    else:\n                        # There is no generic way to convert from namespace A to B\n                        # So we first convert from A to numpy and then from numpy to B\n                        # The way to avoid this round trip is to lobby for DLpack\n                        # support in libraries A and B\n                        array_np = _convert_to_numpy(array, xp_array)\n                        array_converted = xp.asarray(array_np, device=device)\n                converted_arrays.append(array_converted)\n\n    return (\n        converted_arrays[0] if len(converted_arrays) == 1 else tuple(converted_arrays)\n    )",
    "scikit-learn.sklearn.utils._param_validation.wrapper": "@functools.wraps(func)\ndef wrapper(*args, **kwargs):\n    global_skip_validation = get_config()[\"skip_parameter_validation\"]\n    if global_skip_validation:\n        return func(*args, **kwargs)\n\n    func_sig = signature(func)\n\n    # Map *args/**kwargs to the function signature\n    params = func_sig.bind(*args, **kwargs)\n    params.apply_defaults()\n\n    # ignore self/cls and positional/keyword markers\n    to_ignore = [\n        p.name\n        for p in func_sig.parameters.values()\n        if p.kind in (p.VAR_POSITIONAL, p.VAR_KEYWORD)\n    ]\n    to_ignore += [\"self\", \"cls\"]\n    params = {k: v for k, v in params.arguments.items() if k not in to_ignore}\n\n    validate_parameter_constraints(\n        parameter_constraints, params, caller_name=func.__qualname__\n    )\n\n    try:\n        with config_context(\n            skip_parameter_validation=(\n                prefer_skip_nested_validation or global_skip_validation\n            )\n        ):\n            return func(*args, **kwargs)\n    except InvalidParameterError as e:\n        # When the function is just a wrapper around an estimator, we allow\n        # the function to delegate validation to the estimator, but we replace\n        # the name of the estimator by the name of the function in the error\n        # message to avoid confusion.\n        msg = re.sub(\n            r\"parameter of \\w+ must be\",\n            f\"parameter of {func.__qualname__} must be\",\n            str(e),\n        )\n        raise InvalidParameterError(msg) from e",
    "scikit-learn.sklearn.utils.validation.column_or_1d": "def column_or_1d(y, *, dtype=None, input_name=\"y\", warn=False, device=None):\n    \"\"\"Ravel column or 1d numpy array, else raises an error.\n\n    Parameters\n    ----------\n    y : array-like\n       Input data.\n\n    dtype : data-type, default=None\n        Data type for `y`.\n\n        .. versionadded:: 1.2\n\n    input_name : str, default=\"y\"\n        The data name used to construct the error message.\n\n        .. versionadded:: 1.8\n\n    warn : bool, default=False\n       To control display of warnings.\n\n    device : device, default=None\n        `device` object.\n        See the :ref:`Array API User Guide <array_api>` for more details.\n\n        .. versionadded:: 1.6\n\n    Returns\n    -------\n    y : ndarray\n       Output data.\n\n    Raises\n    ------\n    ValueError\n        If `y` is not a 1D array or a 2D array with a single row or column.\n\n    Examples\n    --------\n    >>> from sklearn.utils.validation import column_or_1d\n    >>> column_or_1d([1, 1])\n    array([1, 1])\n    \"\"\"\n    xp, _ = get_namespace(y)\n    y = check_array(\n        y,\n        ensure_2d=False,\n        dtype=dtype,\n        input_name=input_name,\n        ensure_all_finite=False,\n        ensure_min_samples=0,\n    )\n\n    shape = y.shape\n    if len(shape) == 1:\n        return _asarray_with_order(\n            xp.reshape(y, (-1,)), order=\"C\", xp=xp, device=device\n        )\n    if len(shape) == 2 and shape[1] == 1:\n        if warn:\n            warnings.warn(\n                (\n                    \"A column-vector y was passed when a 1d array was\"\n                    \" expected. Please change the shape of y to \"\n                    \"(n_samples, ), for example using ravel().\"\n                ),\n                DataConversionWarning,\n                stacklevel=2,\n            )\n        return _asarray_with_order(\n            xp.reshape(y, (-1,)), order=\"C\", xp=xp, device=device\n        )\n\n    raise ValueError(\n        \"y should be a 1d array, got an array of shape {} instead.\".format(shape)\n    )",
    "scikit-learn.sklearn.utils.validation._check_sample_weight": "def _check_sample_weight(\n    sample_weight,\n    X,\n    *,\n    dtype=None,\n    force_float_dtype=True,\n    ensure_non_negative=False,\n    ensure_same_device=True,\n    copy=False,\n):\n    \"\"\"Validate sample weights.\n\n    Note that passing sample_weight=None will output an array of ones.\n    Therefore, in some cases, you may want to protect the call with:\n    if sample_weight is not None:\n        sample_weight = _check_sample_weight(...)\n\n    Parameters\n    ----------\n    sample_weight : {ndarray, Number or None}, shape (n_samples,)\n        Input sample weights.\n\n    X : {ndarray, list, sparse matrix}\n        Input data.\n\n    dtype : dtype, default=None\n        dtype of the validated `sample_weight`.\n        If None, and `sample_weight` is an array:\n\n            - If `sample_weight.dtype` is one of `{np.float64, np.float32}`,\n              then the dtype is preserved.\n            - Else the output has NumPy's default dtype: `np.float64`.\n\n        If `dtype` is not `{np.float32, np.float64, None}`, then output will\n        be `np.float64`.\n\n    force_float_dtype : bool, default=True\n        Whether `X` should be forced to be float dtype, when `dtype` is a non-float\n        dtype or None.\n\n    ensure_non_negative : bool, default=False,\n        Whether or not the weights are expected to be non-negative.\n\n        .. versionadded:: 1.0\n\n    ensure_same_device : bool, default=True\n        Whether `sample_weight` should be forced to be on the same device as `X`.\n\n    copy : bool, default=False\n        If True, a copy of sample_weight will be created.\n\n    Returns\n    -------\n    sample_weight : ndarray of shape (n_samples,)\n        Validated sample weight. It is guaranteed to be \"C\" contiguous.\n    \"\"\"\n    xp, is_array_api, device = get_namespace_and_device(X, remove_types=(int, float))\n\n    n_samples = _num_samples(X)\n\n    max_float_type = _max_precision_float_dtype(xp, device)\n    float_dtypes = (\n        [xp.float32] if max_float_type == xp.float32 else [xp.float64, xp.float32]\n    )\n    if force_float_dtype and dtype is not None and dtype not in float_dtypes:\n        dtype = max_float_type\n\n    if sample_weight is None:\n        sample_weight = xp.ones(n_samples, dtype=dtype, device=device)\n    elif isinstance(sample_weight, numbers.Number):\n        sample_weight = xp.full(n_samples, sample_weight, dtype=dtype, device=device)\n    else:\n        if force_float_dtype and dtype is None:\n            dtype = float_dtypes\n        if is_array_api and ensure_same_device:\n            sample_weight = xp.asarray(sample_weight, device=device)\n        sample_weight = check_array(\n            sample_weight,\n            accept_sparse=False,\n            ensure_2d=False,\n            dtype=dtype,\n            order=\"C\",\n            copy=copy,\n            input_name=\"sample_weight\",\n        )\n        if sample_weight.ndim != 1:\n            raise ValueError(\n                f\"Sample weights must be 1D array or scalar, got \"\n                f\"{sample_weight.ndim}D array. Expected either a scalar value \"\n                f\"or a 1D array of length {n_samples}.\"\n            )\n\n        if sample_weight.shape != (n_samples,):\n            raise ValueError(\n                \"sample_weight.shape == {}, expected {}!\".format(\n                    sample_weight.shape, (n_samples,)\n                )\n            )\n\n    if ensure_non_negative:\n        check_non_negative(sample_weight, \"`sample_weight`\")\n\n    return sample_weight",
    "scikit-learn.sklearn.utils.validation.validate_data": "def validate_data(\n    _estimator,\n    /,\n    X=\"no_validation\",\n    y=\"no_validation\",\n    reset=True,\n    validate_separately=False,\n    skip_check_array=False,\n    **check_params,\n):\n    \"\"\"Validate input data and set or check feature names and counts of the input.\n\n    This helper function should be used in an estimator that requires input\n    validation. This mutates the estimator and sets the `n_features_in_` and\n    `feature_names_in_` attributes if `reset=True`.\n\n    .. versionadded:: 1.6\n\n    Parameters\n    ----------\n    _estimator : estimator instance\n        The estimator to validate the input for.\n\n    X : {array-like, sparse matrix, dataframe} of shape \\\n            (n_samples, n_features), default='no validation'\n        The input samples.\n        If `'no_validation'`, no validation is performed on `X`. This is\n        useful for meta-estimator which can delegate input validation to\n        their underlying estimator(s). In that case `y` must be passed and\n        the only accepted `check_params` are `multi_output` and\n        `y_numeric`.\n\n    y : array-like of shape (n_samples,), default='no_validation'\n        The targets.\n\n        - If `None`, :func:`~sklearn.utils.check_array` is called on `X`. If\n          the estimator's `requires_y` tag is True, then an error will be raised.\n        - If `'no_validation'`, :func:`~sklearn.utils.check_array` is called\n          on `X` and the estimator's `requires_y` tag is ignored. This is a default\n          placeholder and is never meant to be explicitly set. In that case `X` must be\n          passed.\n        - Otherwise, only `y` with `_check_y` or both `X` and `y` are checked with\n          either :func:`~sklearn.utils.check_array` or\n          :func:`~sklearn.utils.check_X_y` depending on `validate_separately`.\n\n    reset : bool, default=True\n        Whether to reset the `n_features_in_` attribute.\n        If False, the input will be checked for consistency with data\n        provided when reset was last True.\n\n        .. note::\n\n           It is recommended to call `reset=True` in `fit` and in the first\n           call to `partial_fit`. All other methods that validate `X`\n           should set `reset=False`.\n\n    validate_separately : False or tuple of dicts, default=False\n        Only used if `y` is not `None`.\n        If `False`, call :func:`~sklearn.utils.check_X_y`. Else, it must be a tuple of\n        kwargs to be used for calling :func:`~sklearn.utils.check_array` on `X` and `y`\n        respectively.\n\n        `estimator=self` is automatically added to these dicts to generate\n        more informative error message in case of invalid input data.\n\n    skip_check_array : bool, default=False\n        If `True`, `X` and `y` are unchanged and only `feature_names_in_` and\n        `n_features_in_` are checked. Otherwise, :func:`~sklearn.utils.check_array`\n        is called on `X` and `y`.\n\n    **check_params : kwargs\n        Parameters passed to :func:`~sklearn.utils.check_array` or\n        :func:`~sklearn.utils.check_X_y`. Ignored if validate_separately\n        is not False.\n\n        `estimator=self` is automatically added to these params to generate\n        more informative error message in case of invalid input data.\n\n    Returns\n    -------\n    out : {ndarray, sparse matrix} or tuple of these\n        The validated input. A tuple is returned if both `X` and `y` are\n        validated.\n    \"\"\"\n    _check_feature_names(_estimator, X, reset=reset)\n    tags = get_tags(_estimator)\n    if y is None and tags.target_tags.required:\n        raise ValueError(\n            f\"This {_estimator.__class__.__name__} estimator \"\n            \"requires y to be passed, but the target y is None.\"\n        )\n\n    no_val_X = isinstance(X, str) and X == \"no_validation\"\n    no_val_y = y is None or (isinstance(y, str) and y == \"no_validation\")\n\n    if no_val_X and no_val_y:\n        raise ValueError(\"Validation should be done on X, y or both.\")\n\n    default_check_params = {\"estimator\": _estimator}\n    check_params = {**default_check_params, **check_params}\n\n    if skip_check_array:\n        if not no_val_X and no_val_y:\n            out = X\n        elif no_val_X and not no_val_y:\n            out = y\n        else:\n            out = X, y\n    elif not no_val_X and no_val_y:\n        out = check_array(X, input_name=\"X\", **check_params)\n    elif no_val_X and not no_val_y:\n        out = _check_y(y, **check_params)\n    else:\n        if validate_separately:\n            # We need this because some estimators validate X and y\n            # separately, and in general, separately calling check_array()\n            # on X and y isn't equivalent to just calling check_X_y()\n            # :(\n            check_X_params, check_y_params = validate_separately\n            if \"estimator\" not in check_X_params:\n                check_X_params = {**default_check_params, **check_X_params}\n            X = check_array(X, input_name=\"X\", **check_X_params)\n            if \"estimator\" not in check_y_params:\n                check_y_params = {**default_check_params, **check_y_params}\n            y = check_array(y, input_name=\"y\", **check_y_params)\n        else:\n            X, y = check_X_y(X, y, **check_params)\n        out = X, y\n\n    if not no_val_X and check_params.get(\"ensure_2d\", True):\n        _check_n_features(_estimator, X, reset=reset)\n\n    return out"
}