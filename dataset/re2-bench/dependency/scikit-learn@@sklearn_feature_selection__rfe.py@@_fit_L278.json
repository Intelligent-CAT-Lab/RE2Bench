{
    "scikit-learn.sklearn.base.wrapper": "@functools.wraps(fit_method)\ndef wrapper(estimator, *args, **kwargs):\n    global_skip_validation = get_config()[\"skip_parameter_validation\"]\n\n    # we don't want to validate again for each call to partial_fit\n    partial_fit_and_fitted = (\n        fit_method.__name__ == \"partial_fit\" and _is_fitted(estimator)\n    )\n\n    if not global_skip_validation and not partial_fit_and_fitted:\n        estimator._validate_params()\n\n    with config_context(\n        skip_parameter_validation=(\n            prefer_skip_nested_validation or global_skip_validation\n        )\n    ):\n        return fit_method(estimator, *args, **kwargs)",
    "scikit-learn.sklearn.base.clone": "def clone(estimator, *, safe=True):\n    \"\"\"Construct a new unfitted estimator with the same parameters.\n\n    Clone does a deep copy of the model in an estimator\n    without actually copying attached data. It returns a new estimator\n    with the same parameters that has not been fitted on any data.\n\n    .. versionchanged:: 1.3\n        Delegates to `estimator.__sklearn_clone__` if the method exists.\n\n    Parameters\n    ----------\n    estimator : {list, tuple, set} of estimator instance or a single \\\n            estimator instance\n        The estimator or group of estimators to be cloned.\n    safe : bool, default=True\n        If safe is False, clone will fall back to a deep copy on objects\n        that are not estimators. Ignored if `estimator.__sklearn_clone__`\n        exists.\n\n    Returns\n    -------\n    estimator : object\n        The deep copy of the input, an estimator if input is an estimator.\n\n    Notes\n    -----\n    If the estimator's `random_state` parameter is an integer (or if the\n    estimator doesn't have a `random_state` parameter), an *exact clone* is\n    returned: the clone and the original estimator will give the exact same\n    results. Otherwise, *statistical clone* is returned: the clone might\n    return different results from the original estimator. More details can be\n    found in :ref:`randomness`.\n\n    Examples\n    --------\n    >>> from sklearn.base import clone\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> X = [[-1, 0], [0, 1], [0, -1], [1, 0]]\n    >>> y = [0, 0, 1, 1]\n    >>> classifier = LogisticRegression().fit(X, y)\n    >>> cloned_classifier = clone(classifier)\n    >>> hasattr(classifier, \"classes_\")\n    True\n    >>> hasattr(cloned_classifier, \"classes_\")\n    False\n    >>> classifier is cloned_classifier\n    False\n    \"\"\"\n    if hasattr(estimator, \"__sklearn_clone__\") and not inspect.isclass(estimator):\n        return estimator.__sklearn_clone__()\n    return _clone_parametrized(estimator, safe=safe)",
    "scikit-learn.sklearn.cross_decomposition._pls.fit": "def fit(self, X, y):\n    \"\"\"Fit model to data.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Training vectors, where `n_samples` is the number of samples and\n        `n_features` is the number of predictors.\n\n    y : array-like of shape (n_samples,) or (n_samples, n_targets)\n        Target vectors, where `n_samples` is the number of samples and\n        `n_targets` is the number of response variables.\n\n    Returns\n    -------\n    self : object\n        Fitted model.\n    \"\"\"\n    super().fit(X, y)\n    # expose the fitted attributes `x_scores_` and `y_scores_`\n    self.x_scores_ = self._x_scores\n    self.y_scores_ = self._y_scores\n    return self",
    "scikit-learn.sklearn.feature_selection._base._get_feature_importances": "def _get_feature_importances(estimator, getter, transform_func=None, norm_order=1):\n    \"\"\"\n    Retrieve and aggregate (ndim > 1)  the feature importances\n    from an estimator. Also optionally applies transformation.\n\n    Parameters\n    ----------\n    estimator : estimator\n        A scikit-learn estimator from which we want to get the feature\n        importances.\n\n    getter : \"auto\", str or callable\n        An attribute or a callable to get the feature importance. If `\"auto\"`,\n        `estimator` is expected to expose `coef_` or `feature_importances`.\n\n    transform_func : {\"norm\", \"square\"}, default=None\n        The transform to apply to the feature importances. By default (`None`)\n        no transformation is applied.\n\n    norm_order : int, default=1\n        The norm order to apply when `transform_func=\"norm\"`. Only applied\n        when `importances.ndim > 1`.\n\n    Returns\n    -------\n    importances : ndarray of shape (n_features,)\n        The features importances, optionally transformed.\n    \"\"\"\n    if isinstance(getter, str):\n        if getter == \"auto\":\n            if hasattr(estimator, \"coef_\"):\n                getter = attrgetter(\"coef_\")\n            elif hasattr(estimator, \"feature_importances_\"):\n                getter = attrgetter(\"feature_importances_\")\n            else:\n                raise ValueError(\n                    \"when `importance_getter=='auto'`, the underlying \"\n                    f\"estimator {estimator.__class__.__name__} should have \"\n                    \"`coef_` or `feature_importances_` attribute. Either \"\n                    \"pass a fitted estimator to feature selector or call fit \"\n                    \"before calling transform.\"\n                )\n        else:\n            getter = attrgetter(getter)\n    elif not callable(getter):\n        raise ValueError(\"`importance_getter` has to be a string or `callable`\")\n\n    importances = getter(estimator)\n\n    if transform_func is None:\n        return importances\n    elif transform_func == \"norm\":\n        if importances.ndim == 1:\n            importances = np.abs(importances)\n        else:\n            importances = np.linalg.norm(importances, axis=0, ord=norm_order)\n    elif transform_func == \"square\":\n        if importances.ndim == 1:\n            importances = safe_sqr(importances)\n        else:\n            importances = safe_sqr(importances).sum(axis=0)\n    else:\n        raise ValueError(\n            \"Valid values for `transform_func` are \"\n            \"None, 'norm' and 'square'. Those two \"\n            \"transformation are only supported now\"\n        )\n\n    return importances",
    "scikit-learn.sklearn.feature_selection._rfe.<lambda>": "lambda estimator, features: _score(\n    estimator,\n    X_test[:, features],\n    y_test,\n    scorer,\n    score_params=score_params,\n",
    "scikit-learn.sklearn.utils.validation.validate_data": "def validate_data(\n    _estimator,\n    /,\n    X=\"no_validation\",\n    y=\"no_validation\",\n    reset=True,\n    validate_separately=False,\n    skip_check_array=False,\n    **check_params,\n):\n    \"\"\"Validate input data and set or check feature names and counts of the input.\n\n    This helper function should be used in an estimator that requires input\n    validation. This mutates the estimator and sets the `n_features_in_` and\n    `feature_names_in_` attributes if `reset=True`.\n\n    .. versionadded:: 1.6\n\n    Parameters\n    ----------\n    _estimator : estimator instance\n        The estimator to validate the input for.\n\n    X : {array-like, sparse matrix, dataframe} of shape \\\n            (n_samples, n_features), default='no validation'\n        The input samples.\n        If `'no_validation'`, no validation is performed on `X`. This is\n        useful for meta-estimator which can delegate input validation to\n        their underlying estimator(s). In that case `y` must be passed and\n        the only accepted `check_params` are `multi_output` and\n        `y_numeric`.\n\n    y : array-like of shape (n_samples,), default='no_validation'\n        The targets.\n\n        - If `None`, :func:`~sklearn.utils.check_array` is called on `X`. If\n          the estimator's `requires_y` tag is True, then an error will be raised.\n        - If `'no_validation'`, :func:`~sklearn.utils.check_array` is called\n          on `X` and the estimator's `requires_y` tag is ignored. This is a default\n          placeholder and is never meant to be explicitly set. In that case `X` must be\n          passed.\n        - Otherwise, only `y` with `_check_y` or both `X` and `y` are checked with\n          either :func:`~sklearn.utils.check_array` or\n          :func:`~sklearn.utils.check_X_y` depending on `validate_separately`.\n\n    reset : bool, default=True\n        Whether to reset the `n_features_in_` attribute.\n        If False, the input will be checked for consistency with data\n        provided when reset was last True.\n\n        .. note::\n\n           It is recommended to call `reset=True` in `fit` and in the first\n           call to `partial_fit`. All other methods that validate `X`\n           should set `reset=False`.\n\n    validate_separately : False or tuple of dicts, default=False\n        Only used if `y` is not `None`.\n        If `False`, call :func:`~sklearn.utils.check_X_y`. Else, it must be a tuple of\n        kwargs to be used for calling :func:`~sklearn.utils.check_array` on `X` and `y`\n        respectively.\n\n        `estimator=self` is automatically added to these dicts to generate\n        more informative error message in case of invalid input data.\n\n    skip_check_array : bool, default=False\n        If `True`, `X` and `y` are unchanged and only `feature_names_in_` and\n        `n_features_in_` are checked. Otherwise, :func:`~sklearn.utils.check_array`\n        is called on `X` and `y`.\n\n    **check_params : kwargs\n        Parameters passed to :func:`~sklearn.utils.check_array` or\n        :func:`~sklearn.utils.check_X_y`. Ignored if validate_separately\n        is not False.\n\n        `estimator=self` is automatically added to these params to generate\n        more informative error message in case of invalid input data.\n\n    Returns\n    -------\n    out : {ndarray, sparse matrix} or tuple of these\n        The validated input. A tuple is returned if both `X` and `y` are\n        validated.\n    \"\"\"\n    _check_feature_names(_estimator, X, reset=reset)\n    tags = get_tags(_estimator)\n    if y is None and tags.target_tags.required:\n        raise ValueError(\n            f\"This {_estimator.__class__.__name__} estimator \"\n            \"requires y to be passed, but the target y is None.\"\n        )\n\n    no_val_X = isinstance(X, str) and X == \"no_validation\"\n    no_val_y = y is None or (isinstance(y, str) and y == \"no_validation\")\n\n    if no_val_X and no_val_y:\n        raise ValueError(\"Validation should be done on X, y or both.\")\n\n    default_check_params = {\"estimator\": _estimator}\n    check_params = {**default_check_params, **check_params}\n\n    if skip_check_array:\n        if not no_val_X and no_val_y:\n            out = X\n        elif no_val_X and not no_val_y:\n            out = y\n        else:\n            out = X, y\n    elif not no_val_X and no_val_y:\n        out = check_array(X, input_name=\"X\", **check_params)\n    elif no_val_X and not no_val_y:\n        out = _check_y(y, **check_params)\n    else:\n        if validate_separately:\n            # We need this because some estimators validate X and y\n            # separately, and in general, separately calling check_array()\n            # on X and y isn't equivalent to just calling check_X_y()\n            # :(\n            check_X_params, check_y_params = validate_separately\n            if \"estimator\" not in check_X_params:\n                check_X_params = {**default_check_params, **check_X_params}\n            X = check_array(X, input_name=\"X\", **check_X_params)\n            if \"estimator\" not in check_y_params:\n                check_y_params = {**default_check_params, **check_y_params}\n            y = check_array(y, input_name=\"y\", **check_y_params)\n        else:\n            X, y = check_X_y(X, y, **check_params)\n        out = X, y\n\n    if not no_val_X and check_params.get(\"ensure_2d\", True):\n        _check_n_features(_estimator, X, reset=reset)\n\n    return out"
}