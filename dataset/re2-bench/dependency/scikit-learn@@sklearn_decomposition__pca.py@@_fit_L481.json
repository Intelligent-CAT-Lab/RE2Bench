{
    "scikit-learn.sklearn.decomposition._pca._fit_full": "def _fit_full(self, X, n_components, xp, is_array_api_compliant):\n    \"\"\"Fit the model by computing full SVD on X.\"\"\"\n    n_samples, n_features = X.shape\n\n    if n_components == \"mle\":\n        if n_samples < n_features:\n            raise ValueError(\n                \"n_components='mle' is only supported if n_samples >= n_features\"\n            )\n    elif not 0 <= n_components <= min(n_samples, n_features):\n        raise ValueError(\n            f\"n_components={n_components} must be between 0 and \"\n            f\"min(n_samples, n_features)={min(n_samples, n_features)} with \"\n            f\"svd_solver={self._fit_svd_solver!r}\"\n        )\n\n    self.mean_ = xp.mean(X, axis=0)\n    # When X is a scipy sparse matrix, self.mean_ is a numpy matrix, so we need\n    # to transform it to a 1D array. Note that this is not the case when X\n    # is a scipy sparse array.\n    # TODO: remove the following two lines when scikit-learn only depends\n    # on scipy versions that no longer support scipy.sparse matrices.\n    self.mean_ = xp.reshape(xp.asarray(self.mean_), (-1,))\n\n    if self._fit_svd_solver == \"full\":\n        X_centered = xp.asarray(X, copy=True) if self.copy else X\n        X_centered -= self.mean_\n        x_is_centered = not self.copy\n\n        if not is_array_api_compliant:\n            # Use scipy.linalg with NumPy/SciPy inputs for the sake of not\n            # introducing unanticipated behavior changes. In the long run we\n            # could instead decide to always use xp.linalg.svd for all inputs,\n            # but that would make this code rely on numpy's SVD instead of\n            # scipy's. It's not 100% clear whether they use the same LAPACK\n            # solver by default though (assuming both are built against the\n            # same BLAS).\n            U, S, Vt = linalg.svd(X_centered, full_matrices=False)\n        else:\n            U, S, Vt = xp.linalg.svd(X_centered, full_matrices=False)\n        explained_variance_ = (S**2) / (n_samples - 1)\n\n    else:\n        assert self._fit_svd_solver == \"covariance_eigh\"\n        # In the following, we center the covariance matrix C afterwards\n        # (without centering the data X first) to avoid an unnecessary copy\n        # of X. Note that the mean_ attribute is still needed to center\n        # test data in the transform method.\n        #\n        # Note: at the time of writing, `xp.cov` does not exist in the\n        # Array API standard:\n        # https://github.com/data-apis/array-api/issues/43\n        #\n        # Besides, using `numpy.cov`, as of numpy 1.26.0, would not be\n        # memory efficient for our use case when `n_samples >> n_features`:\n        # `numpy.cov` centers a copy of the data before computing the\n        # matrix product instead of subtracting a small `(n_features,\n        # n_features)` square matrix from the gram matrix X.T @ X, as we do\n        # below.\n        x_is_centered = False\n        C = X.T @ X\n        C -= (\n            n_samples\n            * xp.reshape(self.mean_, (-1, 1))\n            * xp.reshape(self.mean_, (1, -1))\n        )\n        C /= n_samples - 1\n        eigenvals, eigenvecs = xp.linalg.eigh(C)\n\n        # When X is a scipy sparse matrix, the following two datastructures\n        # are returned as instances of the soft-deprecated numpy.matrix\n        # class. Note that this problem does not occur when X is a scipy\n        # sparse array (or another other kind of supported array).\n        # TODO: remove the following two lines when scikit-learn only\n        # depends on scipy versions that no longer support scipy.sparse\n        # matrices.\n        eigenvals = xp.reshape(xp.asarray(eigenvals), (-1,))\n        eigenvecs = xp.asarray(eigenvecs)\n\n        eigenvals = xp.flip(eigenvals, axis=0)\n        eigenvecs = xp.flip(eigenvecs, axis=1)\n\n        # The covariance matrix C is positive semi-definite by\n        # construction. However, the eigenvalues returned by xp.linalg.eigh\n        # can be slightly negative due to numerical errors. This would be\n        # an issue for the subsequent sqrt, hence the manual clipping.\n        eigenvals[eigenvals < 0.0] = 0.0\n        explained_variance_ = eigenvals\n\n        # Re-construct SVD of centered X indirectly and make it consistent\n        # with the other solvers.\n        S = xp.sqrt(eigenvals * (n_samples - 1))\n        Vt = eigenvecs.T\n        U = None\n\n    # flip eigenvectors' sign to enforce deterministic output\n    U, Vt = svd_flip(U, Vt, u_based_decision=False)\n\n    components_ = Vt\n\n    # Get variance explained by singular values\n    total_var = xp.sum(explained_variance_)\n    explained_variance_ratio_ = explained_variance_ / total_var\n    singular_values_ = xp.asarray(S, copy=True)  # Store the singular values.\n\n    # Postprocess the number of components required\n    if n_components == \"mle\":\n        n_components = _infer_dimension(explained_variance_, n_samples)\n    elif 0 < n_components < 1.0:\n        # number of components for which the cumulated explained\n        # variance percentage is superior to the desired threshold\n        # side='right' ensures that number of features selected\n        # their variance is always greater than n_components float\n        # passed. More discussion in issue: #15669\n        ratio_cumsum = xp.cumulative_sum(explained_variance_ratio_)\n        n_components = (\n            xp.searchsorted(\n                ratio_cumsum,\n                xp.asarray(n_components, device=device(ratio_cumsum)),\n                side=\"right\",\n            )\n            + 1\n        )\n\n    # Compute noise covariance using Probabilistic PCA model\n    # The sigma2 maximum likelihood (cf. eq. 12.46)\n    if n_components < min(n_features, n_samples):\n        self.noise_variance_ = xp.mean(explained_variance_[n_components:])\n    else:\n        self.noise_variance_ = 0.0\n\n    self.n_samples_ = n_samples\n    self.n_components_ = n_components\n    # Assign a copy of the result of the truncation of the components in\n    # order to:\n    # - release the memory used by the discarded components,\n    # - ensure that the kept components are allocated contiguously in\n    #   memory to make the transform method faster by leveraging cache\n    #   locality.\n    self.components_ = xp.asarray(components_[:n_components, :], copy=True)\n\n    # We do the same for the other arrays for the sake of consistency.\n    self.explained_variance_ = xp.asarray(\n        explained_variance_[:n_components], copy=True\n    )\n    self.explained_variance_ratio_ = xp.asarray(\n        explained_variance_ratio_[:n_components], copy=True\n    )\n    self.singular_values_ = xp.asarray(singular_values_[:n_components], copy=True)\n\n    return U, S, Vt, X, x_is_centered, xp",
    "scikit-learn.sklearn.decomposition._pca._fit_truncated": "def _fit_truncated(self, X, n_components, xp):\n    \"\"\"Fit the model by computing truncated SVD (by ARPACK or randomized)\n    on X.\n    \"\"\"\n    n_samples, n_features = X.shape\n\n    svd_solver = self._fit_svd_solver\n    if isinstance(n_components, str):\n        raise ValueError(\n            \"n_components=%r cannot be a string with svd_solver='%s'\"\n            % (n_components, svd_solver)\n        )\n    elif not 1 <= n_components <= min(n_samples, n_features):\n        raise ValueError(\n            \"n_components=%r must be between 1 and \"\n            \"min(n_samples, n_features)=%r with \"\n            \"svd_solver='%s'\"\n            % (n_components, min(n_samples, n_features), svd_solver)\n        )\n    elif svd_solver == \"arpack\" and n_components == min(n_samples, n_features):\n        raise ValueError(\n            \"n_components=%r must be strictly less than \"\n            \"min(n_samples, n_features)=%r with \"\n            \"svd_solver='%s'\"\n            % (n_components, min(n_samples, n_features), svd_solver)\n        )\n\n    random_state = check_random_state(self.random_state)\n\n    # Center data\n    total_var = None\n    if issparse(X):\n        self.mean_, var = mean_variance_axis(X, axis=0)\n        total_var = var.sum() * n_samples / (n_samples - 1)  # ddof=1\n        X_centered = _implicit_column_offset(X, self.mean_)\n        x_is_centered = False\n    else:\n        self.mean_ = xp.mean(X, axis=0)\n        X_centered = xp.asarray(X, copy=True) if self.copy else X\n        X_centered -= self.mean_\n        x_is_centered = not self.copy\n\n    if svd_solver == \"arpack\":\n        v0 = _init_arpack_v0(min(X.shape), random_state)\n        U, S, Vt = svds(X_centered, k=n_components, tol=self.tol, v0=v0)\n        # svds doesn't abide by scipy.linalg.svd/randomized_svd\n        # conventions, so reverse its outputs.\n        S = S[::-1]\n        # flip eigenvectors' sign to enforce deterministic output\n        U, Vt = svd_flip(U[:, ::-1], Vt[::-1], u_based_decision=False)\n\n    elif svd_solver == \"randomized\":\n        # sign flipping is done inside\n        U, S, Vt = _randomized_svd(\n            X_centered,\n            n_components=n_components,\n            n_oversamples=self.n_oversamples,\n            n_iter=self.iterated_power,\n            power_iteration_normalizer=self.power_iteration_normalizer,\n            flip_sign=False,\n            random_state=random_state,\n        )\n        U, Vt = svd_flip(U, Vt, u_based_decision=False)\n\n    self.n_samples_ = n_samples\n    self.components_ = Vt\n    self.n_components_ = n_components\n\n    # Get variance explained by singular values\n    self.explained_variance_ = (S**2) / (n_samples - 1)\n\n    # Workaround in-place variance calculation since at the time numpy\n    # did not have a way to calculate variance in-place.\n    #\n    # TODO: update this code to either:\n    # * Use the array-api variance calculation, unless memory usage suffers\n    # * Update sklearn.utils.extmath._incremental_mean_and_var to support array-api\n    # See: https://github.com/scikit-learn/scikit-learn/pull/18689#discussion_r1335540991\n    if total_var is None:\n        N = X.shape[0] - 1\n        X_centered **= 2\n        total_var = xp.sum(X_centered) / N\n\n    self.explained_variance_ratio_ = self.explained_variance_ / total_var\n    self.singular_values_ = xp.asarray(S, copy=True)  # Store the singular values.\n\n    if self.n_components_ < min(n_features, n_samples):\n        self.noise_variance_ = total_var - xp.sum(self.explained_variance_)\n        self.noise_variance_ /= min(n_features, n_samples) - n_components\n    else:\n        self.noise_variance_ = 0.0\n\n    return U, S, Vt, X, x_is_centered, xp",
    "scikit-learn.sklearn.utils._array_api.get_namespace": "def get_namespace(*arrays, remove_none=True, remove_types=(str,), xp=None):\n    \"\"\"Get namespace of arrays.\n\n    Introspect `arrays` arguments and return their common Array API compatible\n    namespace object, if any.\n\n    Note that sparse arrays are filtered by default.\n\n    See: https://numpy.org/neps/nep-0047-array-api-standard.html\n\n    If `arrays` are regular numpy arrays, `array_api_compat.numpy` is returned instead.\n\n    Namespace support is not enabled by default. To enabled it call:\n\n      sklearn.set_config(array_api_dispatch=True)\n\n    or:\n\n      with sklearn.config_context(array_api_dispatch=True):\n          # your code here\n\n    Otherwise `array_api_compat.numpy` is\n    always returned irrespective of the fact that arrays implement the\n    `__array_namespace__` protocol or not.\n\n    Note that if no arrays pass the set filters, ``_NUMPY_API_WRAPPER_INSTANCE, False``\n    is returned.\n\n    Parameters\n    ----------\n    *arrays : array objects\n        Array objects.\n\n    remove_none : bool, default=True\n        Whether to ignore None objects passed in arrays.\n\n    remove_types : tuple or list, default=(str,)\n        Types to ignore in the arrays.\n\n    xp : module, default=None\n        Precomputed array namespace module. When passed, typically from a caller\n        that has already performed inspection of its own inputs, skips array\n        namespace inspection.\n\n    Returns\n    -------\n    namespace : module\n        Namespace shared by array objects. If any of the `arrays` are not arrays,\n        the namespace defaults to the NumPy namespace.\n\n    is_array_api_compliant : bool\n        True if the arrays are containers that implement the array API spec (see\n        https://data-apis.org/array-api/latest/index.html).\n        Always False when array_api_dispatch=False.\n    \"\"\"\n    array_api_dispatch = get_config()[\"array_api_dispatch\"]\n    if not array_api_dispatch:\n        if xp is not None:\n            return xp, False\n        else:\n            return np_compat, False\n\n    if xp is not None:\n        return xp, True\n\n    arrays = _remove_non_arrays(\n        *arrays,\n        remove_none=remove_none,\n        remove_types=remove_types,\n    )\n\n    if not arrays:\n        return np_compat, False\n\n    _check_array_api_dispatch(array_api_dispatch)\n\n    namespace, is_array_api_compliant = array_api_compat.get_namespace(*arrays), True\n\n    if namespace.__name__ == \"array_api_strict\" and hasattr(\n        namespace, \"set_array_api_strict_flags\"\n    ):\n        namespace.set_array_api_strict_flags(api_version=\"2024.12\")\n\n    return namespace, is_array_api_compliant",
    "scikit-learn.sklearn.utils.validation.validate_data": "def validate_data(\n    _estimator,\n    /,\n    X=\"no_validation\",\n    y=\"no_validation\",\n    reset=True,\n    validate_separately=False,\n    skip_check_array=False,\n    **check_params,\n):\n    \"\"\"Validate input data and set or check feature names and counts of the input.\n\n    This helper function should be used in an estimator that requires input\n    validation. This mutates the estimator and sets the `n_features_in_` and\n    `feature_names_in_` attributes if `reset=True`.\n\n    .. versionadded:: 1.6\n\n    Parameters\n    ----------\n    _estimator : estimator instance\n        The estimator to validate the input for.\n\n    X : {array-like, sparse matrix, dataframe} of shape \\\n            (n_samples, n_features), default='no validation'\n        The input samples.\n        If `'no_validation'`, no validation is performed on `X`. This is\n        useful for meta-estimator which can delegate input validation to\n        their underlying estimator(s). In that case `y` must be passed and\n        the only accepted `check_params` are `multi_output` and\n        `y_numeric`.\n\n    y : array-like of shape (n_samples,), default='no_validation'\n        The targets.\n\n        - If `None`, :func:`~sklearn.utils.check_array` is called on `X`. If\n          the estimator's `requires_y` tag is True, then an error will be raised.\n        - If `'no_validation'`, :func:`~sklearn.utils.check_array` is called\n          on `X` and the estimator's `requires_y` tag is ignored. This is a default\n          placeholder and is never meant to be explicitly set. In that case `X` must be\n          passed.\n        - Otherwise, only `y` with `_check_y` or both `X` and `y` are checked with\n          either :func:`~sklearn.utils.check_array` or\n          :func:`~sklearn.utils.check_X_y` depending on `validate_separately`.\n\n    reset : bool, default=True\n        Whether to reset the `n_features_in_` attribute.\n        If False, the input will be checked for consistency with data\n        provided when reset was last True.\n\n        .. note::\n\n           It is recommended to call `reset=True` in `fit` and in the first\n           call to `partial_fit`. All other methods that validate `X`\n           should set `reset=False`.\n\n    validate_separately : False or tuple of dicts, default=False\n        Only used if `y` is not `None`.\n        If `False`, call :func:`~sklearn.utils.check_X_y`. Else, it must be a tuple of\n        kwargs to be used for calling :func:`~sklearn.utils.check_array` on `X` and `y`\n        respectively.\n\n        `estimator=self` is automatically added to these dicts to generate\n        more informative error message in case of invalid input data.\n\n    skip_check_array : bool, default=False\n        If `True`, `X` and `y` are unchanged and only `feature_names_in_` and\n        `n_features_in_` are checked. Otherwise, :func:`~sklearn.utils.check_array`\n        is called on `X` and `y`.\n\n    **check_params : kwargs\n        Parameters passed to :func:`~sklearn.utils.check_array` or\n        :func:`~sklearn.utils.check_X_y`. Ignored if validate_separately\n        is not False.\n\n        `estimator=self` is automatically added to these params to generate\n        more informative error message in case of invalid input data.\n\n    Returns\n    -------\n    out : {ndarray, sparse matrix} or tuple of these\n        The validated input. A tuple is returned if both `X` and `y` are\n        validated.\n    \"\"\"\n    _check_feature_names(_estimator, X, reset=reset)\n    tags = get_tags(_estimator)\n    if y is None and tags.target_tags.required:\n        raise ValueError(\n            f\"This {_estimator.__class__.__name__} estimator \"\n            \"requires y to be passed, but the target y is None.\"\n        )\n\n    no_val_X = isinstance(X, str) and X == \"no_validation\"\n    no_val_y = y is None or (isinstance(y, str) and y == \"no_validation\")\n\n    if no_val_X and no_val_y:\n        raise ValueError(\"Validation should be done on X, y or both.\")\n\n    default_check_params = {\"estimator\": _estimator}\n    check_params = {**default_check_params, **check_params}\n\n    if skip_check_array:\n        if not no_val_X and no_val_y:\n            out = X\n        elif no_val_X and not no_val_y:\n            out = y\n        else:\n            out = X, y\n    elif not no_val_X and no_val_y:\n        out = check_array(X, input_name=\"X\", **check_params)\n    elif no_val_X and not no_val_y:\n        out = _check_y(y, **check_params)\n    else:\n        if validate_separately:\n            # We need this because some estimators validate X and y\n            # separately, and in general, separately calling check_array()\n            # on X and y isn't equivalent to just calling check_X_y()\n            # :(\n            check_X_params, check_y_params = validate_separately\n            if \"estimator\" not in check_X_params:\n                check_X_params = {**default_check_params, **check_X_params}\n            X = check_array(X, input_name=\"X\", **check_X_params)\n            if \"estimator\" not in check_y_params:\n                check_y_params = {**default_check_params, **check_y_params}\n            y = check_array(y, input_name=\"y\", **check_y_params)\n        else:\n            X, y = check_X_y(X, y, **check_params)\n        out = X, y\n\n    if not no_val_X and check_params.get(\"ensure_2d\", True):\n        _check_n_features(_estimator, X, reset=reset)\n\n    return out"
}