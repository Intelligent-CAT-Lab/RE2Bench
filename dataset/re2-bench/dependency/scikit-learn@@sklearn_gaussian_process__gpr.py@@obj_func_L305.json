{
    "scikit-learn.sklearn.gaussian_process._gpr.log_marginal_likelihood": "def log_marginal_likelihood(\n    self, theta=None, eval_gradient=False, clone_kernel=True\n):\n    \"\"\"Return log-marginal likelihood of theta for training data.\n\n    Parameters\n    ----------\n    theta : array-like of shape (n_kernel_params,) default=None\n        Kernel hyperparameters for which the log-marginal likelihood is\n        evaluated. If None, the precomputed log_marginal_likelihood\n        of ``self.kernel_.theta`` is returned.\n\n    eval_gradient : bool, default=False\n        If True, the gradient of the log-marginal likelihood with respect\n        to the kernel hyperparameters at position theta is returned\n        additionally. If True, theta must not be None.\n\n    clone_kernel : bool, default=True\n        If True, the kernel attribute is copied. If False, the kernel\n        attribute is modified, but may result in a performance improvement.\n\n    Returns\n    -------\n    log_likelihood : float\n        Log-marginal likelihood of theta for training data.\n\n    log_likelihood_gradient : ndarray of shape (n_kernel_params,), optional\n        Gradient of the log-marginal likelihood with respect to the kernel\n        hyperparameters at position theta.\n        Only returned when eval_gradient is True.\n    \"\"\"\n    if theta is None:\n        if eval_gradient:\n            raise ValueError(\"Gradient can only be evaluated for theta!=None\")\n        return self.log_marginal_likelihood_value_\n\n    if clone_kernel:\n        kernel = self.kernel_.clone_with_theta(theta)\n    else:\n        kernel = self.kernel_\n        kernel.theta = theta\n\n    if eval_gradient:\n        K, K_gradient = kernel(self.X_train_, eval_gradient=True)\n    else:\n        K = kernel(self.X_train_)\n\n    # Alg. 2.1, page 19, line 2 -> L = cholesky(K + sigma^2 I)\n    K[np.diag_indices_from(K)] += self.alpha\n    try:\n        L = cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n    except np.linalg.LinAlgError:\n        return (-np.inf, np.zeros_like(theta)) if eval_gradient else -np.inf\n\n    # Support multi-dimensional output of self.y_train_\n    y_train = self.y_train_\n    if y_train.ndim == 1:\n        y_train = y_train[:, np.newaxis]\n\n    # Alg 2.1, page 19, line 3 -> alpha = L^T \\ (L \\ y)\n    alpha = cho_solve((L, GPR_CHOLESKY_LOWER), y_train, check_finite=False)\n\n    # Alg 2.1, page 19, line 7\n    # -0.5 . y^T . alpha - sum(log(diag(L))) - n_samples / 2 log(2*pi)\n    # y is originally thought to be a (1, n_samples) row vector. However,\n    # in multioutputs, y is of shape (n_samples, 2) and we need to compute\n    # y^T . alpha for each output, independently using einsum. Thus, it\n    # is equivalent to:\n    # for output_idx in range(n_outputs):\n    #     log_likelihood_dims[output_idx] = (\n    #         y_train[:, [output_idx]] @ alpha[:, [output_idx]]\n    #     )\n    log_likelihood_dims = -0.5 * np.einsum(\"ik,ik->k\", y_train, alpha)\n    log_likelihood_dims -= np.log(np.diag(L)).sum()\n    log_likelihood_dims -= K.shape[0] / 2 * np.log(2 * np.pi)\n    # the log likelihood is sum-up across the outputs\n    log_likelihood = log_likelihood_dims.sum(axis=-1)\n\n    if eval_gradient:\n        # Eq. 5.9, p. 114, and footnote 5 in p. 114\n        # 0.5 * trace((alpha . alpha^T - K^-1) . K_gradient)\n        # alpha is supposed to be a vector of (n_samples,) elements. With\n        # multioutputs, alpha is a matrix of size (n_samples, n_outputs).\n        # Therefore, we want to construct a matrix of\n        # (n_samples, n_samples, n_outputs) equivalent to\n        # for output_idx in range(n_outputs):\n        #     output_alpha = alpha[:, [output_idx]]\n        #     inner_term[..., output_idx] = output_alpha @ output_alpha.T\n        inner_term = np.einsum(\"ik,jk->ijk\", alpha, alpha)\n        # compute K^-1 of shape (n_samples, n_samples)\n        K_inv = cho_solve(\n            (L, GPR_CHOLESKY_LOWER), np.eye(K.shape[0]), check_finite=False\n        )\n        # create a new axis to use broadcasting between inner_term and\n        # K_inv\n        inner_term -= K_inv[..., np.newaxis]\n        # Since we are interested about the trace of\n        # inner_term @ K_gradient, we don't explicitly compute the\n        # matrix-by-matrix operation and instead use an einsum. Therefore\n        # it is equivalent to:\n        # for param_idx in range(n_kernel_params):\n        #     for output_idx in range(n_output):\n        #         log_likehood_gradient_dims[param_idx, output_idx] = (\n        #             inner_term[..., output_idx] @\n        #             K_gradient[..., param_idx]\n        #         )\n        log_likelihood_gradient_dims = 0.5 * np.einsum(\n            \"ijl,jik->kl\", inner_term, K_gradient\n        )\n        # the log likelihood gradient is the sum-up across the outputs\n        log_likelihood_gradient = log_likelihood_gradient_dims.sum(axis=-1)\n\n    if eval_gradient:\n        return log_likelihood, log_likelihood_gradient\n    else:\n        return log_likelihood"
}