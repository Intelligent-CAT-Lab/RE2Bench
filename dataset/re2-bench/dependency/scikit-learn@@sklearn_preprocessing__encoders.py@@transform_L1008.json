{
    "scikit-learn.sklearn.preprocessing._encoders._transform": "def _transform(\n    self,\n    X,\n    handle_unknown=\"error\",\n    ensure_all_finite=True,\n    warn_on_unknown=False,\n    ignore_category_indices=None,\n):\n    X_list, n_samples, n_features = self._check_X(\n        X, ensure_all_finite=ensure_all_finite\n    )\n    validate_data(self, X=X, reset=False, skip_check_array=True)\n\n    X_int = np.zeros((n_samples, n_features), dtype=int)\n    X_mask = np.ones((n_samples, n_features), dtype=bool)\n\n    columns_with_unknown = []\n    for i in range(n_features):\n        Xi = X_list[i]\n        diff, valid_mask = _check_unknown(Xi, self.categories_[i], return_mask=True)\n\n        if not np.all(valid_mask):\n            if handle_unknown == \"error\":\n                msg = (\n                    \"Found unknown categories {0} in column {1}\"\n                    \" during transform\".format(diff, i)\n                )\n                raise ValueError(msg)\n            else:\n                if warn_on_unknown:\n                    columns_with_unknown.append(i)\n                # Set the problematic rows to an acceptable value and\n                # continue `The rows are marked `X_mask` and will be\n                # removed later.\n                X_mask[:, i] = valid_mask\n                # cast Xi into the largest string type necessary\n                # to handle different lengths of numpy strings\n                if (\n                    self.categories_[i].dtype.kind in (\"U\", \"S\")\n                    and self.categories_[i].itemsize > Xi.itemsize\n                ):\n                    Xi = Xi.astype(self.categories_[i].dtype)\n                elif self.categories_[i].dtype.kind == \"O\" and Xi.dtype.kind == \"U\":\n                    # categories are objects and Xi are numpy strings.\n                    # Cast Xi to an object dtype to prevent truncation\n                    # when setting invalid values.\n                    Xi = Xi.astype(\"O\")\n                else:\n                    Xi = Xi.copy()\n\n                Xi[~valid_mask] = self.categories_[i][0]\n        # We use check_unknown=False, since _check_unknown was\n        # already called above.\n        X_int[:, i] = _encode(Xi, uniques=self.categories_[i], check_unknown=False)\n    if columns_with_unknown:\n        if handle_unknown == \"infrequent_if_exist\":\n            msg = (\n                \"Found unknown categories in columns \"\n                f\"{columns_with_unknown} during transform. These \"\n                \"unknown categories will be encoded as the \"\n                \"infrequent category.\"\n            )\n        else:\n            msg = (\n                \"Found unknown categories in columns \"\n                f\"{columns_with_unknown} during transform. These \"\n                \"unknown categories will be encoded as all zeros\"\n            )\n        warnings.warn(msg, UserWarning)\n\n    self._map_infrequent_categories(X_int, X_mask, ignore_category_indices)\n    return X_int, X_mask",
    "scikit-learn.sklearn.utils._set_output._get_output_config": "def _get_output_config(method, estimator=None):\n    \"\"\"Get output config based on estimator and global configuration.\n\n    Parameters\n    ----------\n    method : {\"transform\"}\n        Estimator's method for which the output container is looked up.\n\n    estimator : estimator instance or None\n        Estimator to get the output configuration from. If `None`, check global\n        configuration is used.\n\n    Returns\n    -------\n    config : dict\n        Dictionary with keys:\n\n        - \"dense\": specifies the dense container for `method`. This can be\n          `\"default\"` or `\"pandas\"`.\n    \"\"\"\n    est_sklearn_output_config = getattr(estimator, \"_sklearn_output_config\", {})\n    if method in est_sklearn_output_config:\n        dense_config = est_sklearn_output_config[method]\n    else:\n        dense_config = get_config()[f\"{method}_output\"]\n\n    supported_outputs = ADAPTERS_MANAGER.supported_outputs\n    if dense_config not in supported_outputs:\n        raise ValueError(\n            f\"output config must be in {sorted(supported_outputs)}, got {dense_config}\"\n        )\n\n    return {\"dense\": dense_config}",
    "scikit-learn.sklearn.utils.validation.check_is_fitted": "def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):\n    \"\"\"Perform is_fitted validation for estimator.\n\n    Checks if the estimator is fitted by verifying the presence of\n    fitted attributes (ending with a trailing underscore) and otherwise\n    raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to\n    specify if the estimator is fitted or not. See\n    :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\n    for an example on how to use the API.\n\n    If no `attributes` are passed, this function will pass if an estimator is stateless.\n    An estimator can indicate it's stateless by setting the `requires_fit` tag. See\n    :ref:`estimator_tags` for more information. Note that the `requires_fit` tag\n    is ignored if `attributes` are passed.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Raises\n    ------\n    TypeError\n        If the estimator is a class or not an estimator instance\n\n    NotFittedError\n        If the attributes are not found.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.utils.validation import check_is_fitted\n    >>> from sklearn.exceptions import NotFittedError\n    >>> lr = LogisticRegression()\n    >>> try:\n    ...     check_is_fitted(lr)\n    ... except NotFittedError as exc:\n    ...     print(f\"Model is not fitted yet.\")\n    Model is not fitted yet.\n    >>> lr.fit([[1, 2], [1, 3]], [1, 0])\n    LogisticRegression()\n    >>> check_is_fitted(lr)\n    \"\"\"\n    if isclass(estimator):\n        raise TypeError(\"{} is a class, not an instance.\".format(estimator))\n    if msg is None:\n        msg = (\n            \"This %(name)s instance is not fitted yet. Call 'fit' with \"\n            \"appropriate arguments before using this estimator.\"\n        )\n\n    if not hasattr(estimator, \"fit\"):\n        raise TypeError(\"%s is not an estimator instance.\" % (estimator))\n\n    tags = get_tags(estimator)\n\n    if not tags.requires_fit and attributes is None:\n        return\n\n    if not _is_fitted(estimator, attributes, all_or_any):\n        raise NotFittedError(msg % {\"name\": type(estimator).__name__})"
}