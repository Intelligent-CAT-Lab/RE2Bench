{
    "scikit-learn.sklearn.cluster._kmeans._check_test_data": "def _check_test_data(self, X):\n    X = validate_data(\n        self,\n        X,\n        accept_sparse=\"csr\",\n        reset=False,\n        dtype=[np.float64, np.float32],\n        order=\"C\",\n        accept_large_sparse=False,\n    )\n    return X",
    "scikit-learn.sklearn.utils.parallel.wrapper": "@functools.wraps(func)\ndef wrapper(*args, **kwargs):\n    controller = _get_threadpool_controller()\n    with controller.limit(limits=limits, user_api=user_api):\n        return func(*args, **kwargs)",
    "scikit-learn.sklearn.utils.validation.check_is_fitted": "def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):\n    \"\"\"Perform is_fitted validation for estimator.\n\n    Checks if the estimator is fitted by verifying the presence of\n    fitted attributes (ending with a trailing underscore) and otherwise\n    raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to\n    specify if the estimator is fitted or not. See\n    :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\n    for an example on how to use the API.\n\n    If no `attributes` are passed, this function will pass if an estimator is stateless.\n    An estimator can indicate it's stateless by setting the `requires_fit` tag. See\n    :ref:`estimator_tags` for more information. Note that the `requires_fit` tag\n    is ignored if `attributes` are passed.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Raises\n    ------\n    TypeError\n        If the estimator is a class or not an estimator instance\n\n    NotFittedError\n        If the attributes are not found.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.utils.validation import check_is_fitted\n    >>> from sklearn.exceptions import NotFittedError\n    >>> lr = LogisticRegression()\n    >>> try:\n    ...     check_is_fitted(lr)\n    ... except NotFittedError as exc:\n    ...     print(f\"Model is not fitted yet.\")\n    Model is not fitted yet.\n    >>> lr.fit([[1, 2], [1, 3]], [1, 0])\n    LogisticRegression()\n    >>> check_is_fitted(lr)\n    \"\"\"\n    if isclass(estimator):\n        raise TypeError(\"{} is a class, not an instance.\".format(estimator))\n    if msg is None:\n        msg = (\n            \"This %(name)s instance is not fitted yet. Call 'fit' with \"\n            \"appropriate arguments before using this estimator.\"\n        )\n\n    if not hasattr(estimator, \"fit\"):\n        raise TypeError(\"%s is not an estimator instance.\" % (estimator))\n\n    tags = get_tags(estimator)\n\n    if not tags.requires_fit and attributes is None:\n        return\n\n    if not _is_fitted(estimator, attributes, all_or_any):\n        raise NotFittedError(msg % {\"name\": type(estimator).__name__})",
    "scikit-learn.sklearn.utils.validation._check_sample_weight": "def _check_sample_weight(\n    sample_weight,\n    X,\n    *,\n    dtype=None,\n    force_float_dtype=True,\n    ensure_non_negative=False,\n    ensure_same_device=True,\n    copy=False,\n):\n    \"\"\"Validate sample weights.\n\n    Note that passing sample_weight=None will output an array of ones.\n    Therefore, in some cases, you may want to protect the call with:\n    if sample_weight is not None:\n        sample_weight = _check_sample_weight(...)\n\n    Parameters\n    ----------\n    sample_weight : {ndarray, Number or None}, shape (n_samples,)\n        Input sample weights.\n\n    X : {ndarray, list, sparse matrix}\n        Input data.\n\n    dtype : dtype, default=None\n        dtype of the validated `sample_weight`.\n        If None, and `sample_weight` is an array:\n\n            - If `sample_weight.dtype` is one of `{np.float64, np.float32}`,\n              then the dtype is preserved.\n            - Else the output has NumPy's default dtype: `np.float64`.\n\n        If `dtype` is not `{np.float32, np.float64, None}`, then output will\n        be `np.float64`.\n\n    force_float_dtype : bool, default=True\n        Whether `X` should be forced to be float dtype, when `dtype` is a non-float\n        dtype or None.\n\n    ensure_non_negative : bool, default=False,\n        Whether or not the weights are expected to be non-negative.\n\n        .. versionadded:: 1.0\n\n    ensure_same_device : bool, default=True\n        Whether `sample_weight` should be forced to be on the same device as `X`.\n\n    copy : bool, default=False\n        If True, a copy of sample_weight will be created.\n\n    Returns\n    -------\n    sample_weight : ndarray of shape (n_samples,)\n        Validated sample weight. It is guaranteed to be \"C\" contiguous.\n    \"\"\"\n    xp, is_array_api, device = get_namespace_and_device(X, remove_types=(int, float))\n\n    n_samples = _num_samples(X)\n\n    max_float_type = _max_precision_float_dtype(xp, device)\n    float_dtypes = (\n        [xp.float32] if max_float_type == xp.float32 else [xp.float64, xp.float32]\n    )\n    if force_float_dtype and dtype is not None and dtype not in float_dtypes:\n        dtype = max_float_type\n\n    if sample_weight is None:\n        sample_weight = xp.ones(n_samples, dtype=dtype, device=device)\n    elif isinstance(sample_weight, numbers.Number):\n        sample_weight = xp.full(n_samples, sample_weight, dtype=dtype, device=device)\n    else:\n        if force_float_dtype and dtype is None:\n            dtype = float_dtypes\n        if is_array_api and ensure_same_device:\n            sample_weight = xp.asarray(sample_weight, device=device)\n        sample_weight = check_array(\n            sample_weight,\n            accept_sparse=False,\n            ensure_2d=False,\n            dtype=dtype,\n            order=\"C\",\n            copy=copy,\n            input_name=\"sample_weight\",\n        )\n        if sample_weight.ndim != 1:\n            raise ValueError(\n                f\"Sample weights must be 1D array or scalar, got \"\n                f\"{sample_weight.ndim}D array. Expected either a scalar value \"\n                f\"or a 1D array of length {n_samples}.\"\n            )\n\n        if sample_weight.shape != (n_samples,):\n            raise ValueError(\n                \"sample_weight.shape == {}, expected {}!\".format(\n                    sample_weight.shape, (n_samples,)\n                )\n            )\n\n    if ensure_non_negative:\n        check_non_negative(sample_weight, \"`sample_weight`\")\n\n    return sample_weight"
}