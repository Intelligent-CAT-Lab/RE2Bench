{
    "scikit-learn.sklearn.base.wrapper": "@functools.wraps(fit_method)\ndef wrapper(estimator, *args, **kwargs):\n    global_skip_validation = get_config()[\"skip_parameter_validation\"]\n\n    # we don't want to validate again for each call to partial_fit\n    partial_fit_and_fitted = (\n        fit_method.__name__ == \"partial_fit\" and _is_fitted(estimator)\n    )\n\n    if not global_skip_validation and not partial_fit_and_fitted:\n        estimator._validate_params()\n\n    with config_context(\n        skip_parameter_validation=(\n            prefer_skip_nested_validation or global_skip_validation\n        )\n    ):\n        return fit_method(estimator, *args, **kwargs)",
    "scikit-learn.sklearn.ensemble._hist_gradient_boosting.binning.fit": "def fit(self, X, y=None):\n    \"\"\"Fit data X by computing the binning thresholds.\n\n    The last bin is reserved for missing values, whether missing values\n    are present in the data or not.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to bin.\n    y: None\n        Ignored.\n\n    Returns\n    -------\n    self : object\n    \"\"\"\n    if not (3 <= self.n_bins <= 256):\n        # min is 3: at least 2 distinct bins and a missing values bin\n        raise ValueError(\n            \"n_bins={} should be no smaller than 3 and no larger than 256.\".format(\n                self.n_bins\n            )\n        )\n\n    X = check_array(X, dtype=[X_DTYPE], ensure_all_finite=False)\n    max_bins = self.n_bins - 1\n\n    rng = check_random_state(self.random_state)\n    if self.subsample is not None and X.shape[0] > self.subsample:\n        subset = rng.choice(X.shape[0], self.subsample, replace=False)\n        X = X.take(subset, axis=0)\n\n    if self.is_categorical is None:\n        self.is_categorical_ = np.zeros(X.shape[1], dtype=np.uint8)\n    else:\n        self.is_categorical_ = np.asarray(self.is_categorical, dtype=np.uint8)\n\n    n_features = X.shape[1]\n    known_categories = self.known_categories\n    if known_categories is None:\n        known_categories = [None] * n_features\n\n    # validate is_categorical and known_categories parameters\n    for f_idx in range(n_features):\n        is_categorical = self.is_categorical_[f_idx]\n        known_cats = known_categories[f_idx]\n        if is_categorical and known_cats is None:\n            raise ValueError(\n                f\"Known categories for feature {f_idx} must be provided.\"\n            )\n        if not is_categorical and known_cats is not None:\n            raise ValueError(\n                f\"Feature {f_idx} isn't marked as a categorical feature, \"\n                \"but categories were passed.\"\n            )\n\n    self.missing_values_bin_idx_ = self.n_bins - 1\n\n    self.bin_thresholds_ = [None] * n_features\n    n_bins_non_missing = [None] * n_features\n\n    non_cat_thresholds = Parallel(n_jobs=self.n_threads, backend=\"threading\")(\n        delayed(_find_binning_thresholds)(X[:, f_idx], max_bins)\n        for f_idx in range(n_features)\n        if not self.is_categorical_[f_idx]\n    )\n\n    non_cat_idx = 0\n    for f_idx in range(n_features):\n        if self.is_categorical_[f_idx]:\n            # Since categories are assumed to be encoded in\n            # [0, n_cats] and since n_cats <= max_bins,\n            # the thresholds *are* the unique categorical values. This will\n            # lead to the correct mapping in transform()\n            thresholds = known_categories[f_idx]\n            n_bins_non_missing[f_idx] = thresholds.shape[0]\n            self.bin_thresholds_[f_idx] = thresholds\n        else:\n            self.bin_thresholds_[f_idx] = non_cat_thresholds[non_cat_idx]\n            n_bins_non_missing[f_idx] = self.bin_thresholds_[f_idx].shape[0] + 1\n            non_cat_idx += 1\n\n    self.n_bins_non_missing_ = np.array(n_bins_non_missing, dtype=np.uint32)\n    return self",
    "scikit-learn.sklearn.ensemble._stacking.fit": "def fit(self, X, y, **fit_params):\n    \"\"\"Fit the estimators.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Training vectors, where `n_samples` is the number of samples and\n        `n_features` is the number of features.\n\n    y : array-like of shape (n_samples,)\n        Target values. Note that `y` will be internally encoded in\n        numerically increasing order or lexicographic order. If the order\n        matter (e.g. for ordinal regression), one should numerically encode\n        the target `y` before calling :term:`fit`.\n\n    **fit_params : dict\n        Parameters to pass to the underlying estimators.\n\n        .. versionadded:: 1.6\n\n            Only available if `enable_metadata_routing=True`, which can be\n            set by using ``sklearn.set_config(enable_metadata_routing=True)``.\n            See :ref:`Metadata Routing User Guide <metadata_routing>` for\n            more details.\n\n    Returns\n    -------\n    self : object\n        Returns a fitted instance of estimator.\n    \"\"\"\n    _raise_for_params(fit_params, self, \"fit\", allow=[\"sample_weight\"])\n\n    check_classification_targets(y)\n    if type_of_target(y) == \"multilabel-indicator\":\n        self._label_encoder = [LabelEncoder().fit(yk) for yk in y.T]\n        self.classes_ = [le.classes_ for le in self._label_encoder]\n        y_encoded = np.array(\n            [\n                self._label_encoder[target_idx].transform(target)\n                for target_idx, target in enumerate(y.T)\n            ]\n        ).T\n    else:\n        self._label_encoder = LabelEncoder().fit(y)\n        self.classes_ = self._label_encoder.classes_\n        y_encoded = self._label_encoder.transform(y)\n\n    return super().fit(X, y_encoded, **fit_params)",
    "scikit-learn.sklearn.ensemble._stacking.get_metadata_routing": "def get_metadata_routing(self):\n    \"\"\"Get metadata routing of this object.\n\n    Please check :ref:`User Guide <metadata_routing>` on how the routing\n    mechanism works.\n\n    .. versionadded:: 1.6\n\n    Returns\n    -------\n    routing : MetadataRouter\n        A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n        routing information.\n    \"\"\"\n    router = MetadataRouter(owner=self)\n\n    # `self.estimators` is a list of (name, est) tuples\n    for name, estimator in self.estimators:\n        router.add(\n            **{name: estimator},\n            method_mapping=MethodMapping().add(callee=\"fit\", caller=\"fit\"),\n        )\n\n    try:\n        final_estimator_ = self.final_estimator_\n    except AttributeError:\n        final_estimator_ = self.final_estimator\n\n    router.add(\n        final_estimator_=final_estimator_,\n        method_mapping=MethodMapping().add(caller=\"predict\", callee=\"predict\"),\n    )\n\n    return router",
    "scikit-learn.sklearn.feature_extraction.text.transform": "def transform(self, X, copy=True):\n    \"\"\"Transform a count matrix to a tf or tf-idf representation.\n\n    Parameters\n    ----------\n    X : sparse matrix of (n_samples, n_features)\n        A matrix of term/token counts.\n\n    copy : bool, default=True\n        Whether to copy X and operate on the copy or perform in-place\n        operations. `copy=False` will only be effective with CSR sparse matrix.\n\n    Returns\n    -------\n    vectors : sparse matrix of shape (n_samples, n_features)\n        Tf-idf-weighted document-term matrix.\n    \"\"\"\n    check_is_fitted(self)\n    X = validate_data(\n        self,\n        X,\n        accept_sparse=\"csr\",\n        dtype=[np.float64, np.float32],\n        copy=copy,\n        reset=False,\n    )\n    if not sp.issparse(X):\n        X = sp.csr_matrix(X, dtype=X.dtype)\n\n    if self.sublinear_tf:\n        np.log(X.data, X.data)\n        X.data += 1.0\n\n    if hasattr(self, \"idf_\"):\n        # the columns of X (CSR matrix) can be accessed with `X.indices `and\n        # multiplied with the corresponding `idf` value\n        X.data *= self.idf_[X.indices]\n\n    if self.norm is not None:\n        X = normalize(X, norm=self.norm, copy=False)\n\n    return X",
    "scikit-learn.sklearn.preprocessing._data.fit": "def fit(self, X, y=None, sample_weight=None):\n    \"\"\"Compute the mean and std to be used for later scaling.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The data used to compute the mean and standard deviation\n        used for later scaling along the features axis.\n\n    y : None\n        Ignored.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Individual weights for each sample.\n\n        .. versionadded:: 0.24\n           parameter *sample_weight* support to StandardScaler.\n\n    Returns\n    -------\n    self : object\n        Fitted scaler.\n    \"\"\"\n    # Reset internal state before fitting\n    self._reset()\n    return self.partial_fit(X, y, sample_weight)",
    "scikit-learn.sklearn.utils._metadata_requests.get_metadata_routing": "def get_metadata_routing(self):\n    \"\"\"Get metadata routing of this object.\n\n    Please check :ref:`User Guide <metadata_routing>` on how the routing\n    mechanism works.\n\n    Returns\n    -------\n    routing : MetadataRequest\n        A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n        routing information.\n    \"\"\"\n    return self._get_metadata_request()",
    "scikit-learn.sklearn.utils._metadata_requests._routing_enabled": "def _routing_enabled():\n    \"\"\"Return whether metadata routing is enabled.\n\n    .. versionadded:: 1.3\n\n    Returns\n    -------\n    enabled : bool\n        Whether metadata routing is enabled. If the config is not set, it\n        defaults to False.\n    \"\"\"\n    return get_config().get(\"enable_metadata_routing\", False)",
    "scikit-learn.sklearn.utils._metadata_requests.consumes": "def consumes(self, method, params):\n    \"\"\"Return params consumed as metadata in a :term:`router` or its sub-estimators.\n\n    This method returns the subset of `params` that are consumed by the\n    `method`. A `param` is considered consumed if it is used in the specified\n    method of the :term:`router` itself or any of its sub-estimators (or their\n    sub-estimators).\n\n    .. versionadded:: 1.4\n\n    Parameters\n    ----------\n    method : str\n        The name of the method for which to determine consumed parameters.\n\n    params : iterable of str\n        An iterable of parameter names to test for consumption.\n\n    Returns\n    -------\n    consumed_params : set of str\n        A subset of parameters from `params` which are consumed by this method.\n    \"\"\"\n    consumed_params = set()\n    if self._self_request:\n        consumed_params.update(\n            self._self_request.consumes(method=method, params=params)\n        )\n\n    for _, route_mapping in self._route_mappings.items():\n        for caller, callee in route_mapping.mapping:\n            if caller == method:\n                consumed_params.update(\n                    route_mapping.router.consumes(method=callee, params=params)\n                )\n\n    return consumed_params",
    "scikit-learn.sklearn.utils._set_output.wrapped": "@wraps(f)\ndef wrapped(self, X, *args, **kwargs):\n    data_to_wrap = f(self, X, *args, **kwargs)\n    if isinstance(data_to_wrap, tuple):\n        # only wrap the first output for cross decomposition\n        return_tuple = (\n            _wrap_data_with_container(method, data_to_wrap[0], X, self),\n            *data_to_wrap[1:],\n        )\n        # Support for namedtuples `_make` is a documented API for namedtuples:\n        # https://docs.python.org/3/library/collections.html#collections.somenamedtuple._make\n        if hasattr(type(data_to_wrap), \"_make\"):\n            return type(data_to_wrap)._make(return_tuple)\n        return return_tuple\n\n    return _wrap_data_with_container(method, data_to_wrap, X, self)"
}