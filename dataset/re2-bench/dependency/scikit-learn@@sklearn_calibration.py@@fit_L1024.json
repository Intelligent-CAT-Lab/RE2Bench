{
    "scikit-learn.sklearn.calibration._sigmoid_calibration": "def _sigmoid_calibration(\n    predictions, y, sample_weight=None, max_abs_prediction_threshold=30\n):\n    \"\"\"Probability Calibration with sigmoid method (Platt 2000)\n\n    Parameters\n    ----------\n    predictions : ndarray of shape (n_samples,)\n        The decision function or predict proba for the samples.\n\n    y : ndarray of shape (n_samples,)\n        The targets.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights. If None, then samples are equally weighted.\n\n    Returns\n    -------\n    a : float\n        The slope.\n\n    b : float\n        The intercept.\n\n    References\n    ----------\n    Platt, \"Probabilistic Outputs for Support Vector Machines\"\n    \"\"\"\n    predictions = column_or_1d(predictions)\n    y = column_or_1d(y)\n\n    F = predictions  # F follows Platt's notations\n\n    scale_constant = 1.0\n    max_prediction = np.max(np.abs(F))\n\n    # If the predictions have large values we scale them in order to bring\n    # them within a suitable range. This has no effect on the final\n    # (prediction) result because linear models like Logisitic Regression\n    # without a penalty are invariant to multiplying the features by a\n    # constant.\n    if max_prediction >= max_abs_prediction_threshold:\n        scale_constant = max_prediction\n        # We rescale the features in a copy: inplace rescaling could confuse\n        # the caller and make the code harder to reason about.\n        F = F / scale_constant\n\n    # Bayesian priors (see Platt end of section 2.2):\n    # It corresponds to the number of samples, taking into account the\n    # `sample_weight`.\n    mask_negative_samples = y <= 0\n    if sample_weight is not None:\n        prior0 = (sample_weight[mask_negative_samples]).sum()\n        prior1 = (sample_weight[~mask_negative_samples]).sum()\n    else:\n        prior0 = float(np.sum(mask_negative_samples))\n        prior1 = y.shape[0] - prior0\n    T = np.zeros_like(y, dtype=predictions.dtype)\n    T[y > 0] = (prior1 + 1.0) / (prior1 + 2.0)\n    T[y <= 0] = 1.0 / (prior0 + 2.0)\n\n    bin_loss = HalfBinomialLoss()\n\n    def loss_grad(AB):\n        # .astype below is needed to ensure y_true and raw_prediction have the\n        # same dtype. With result = np.float64(0) * np.array([1, 2], dtype=np.float32)\n        # - in Numpy 2, result.dtype is float64\n        # - in Numpy<2, result.dtype is float32\n        raw_prediction = -(AB[0] * F + AB[1]).astype(dtype=predictions.dtype)\n        l, g = bin_loss.loss_gradient(\n            y_true=T,\n            raw_prediction=raw_prediction,\n            sample_weight=sample_weight,\n        )\n        loss = l.sum()\n        # TODO: Remove casting to np.float64 when minimum supported SciPy is 1.11.2\n        # With SciPy >= 1.11.2, the LBFGS implementation will cast to float64\n        # https://github.com/scipy/scipy/pull/18825.\n        # Here we cast to float64 to support SciPy < 1.11.2\n        grad = np.asarray([-g @ F, -g.sum()], dtype=np.float64)\n        return loss, grad\n\n    AB0 = np.array([0.0, log((prior0 + 1.0) / (prior1 + 1.0))])\n\n    opt_result = minimize(\n        loss_grad,\n        AB0,\n        method=\"L-BFGS-B\",\n        jac=True,\n        options={\n            \"gtol\": 1e-6,\n            \"ftol\": 64 * np.finfo(float).eps,\n        },\n    )\n    AB_ = opt_result.x\n\n    # The tuned multiplicative parameter is converted back to the original\n    # input feature scale. The offset parameter does not need rescaling since\n    # we did not rescale the outcome variable.\n    return AB_[0] / scale_constant, AB_[1]",
    "scikit-learn.sklearn.utils.validation.column_or_1d": "def column_or_1d(y, *, dtype=None, input_name=\"y\", warn=False, device=None):\n    \"\"\"Ravel column or 1d numpy array, else raises an error.\n\n    Parameters\n    ----------\n    y : array-like\n       Input data.\n\n    dtype : data-type, default=None\n        Data type for `y`.\n\n        .. versionadded:: 1.2\n\n    input_name : str, default=\"y\"\n        The data name used to construct the error message.\n\n        .. versionadded:: 1.8\n\n    warn : bool, default=False\n       To control display of warnings.\n\n    device : device, default=None\n        `device` object.\n        See the :ref:`Array API User Guide <array_api>` for more details.\n\n        .. versionadded:: 1.6\n\n    Returns\n    -------\n    y : ndarray\n       Output data.\n\n    Raises\n    ------\n    ValueError\n        If `y` is not a 1D array or a 2D array with a single row or column.\n\n    Examples\n    --------\n    >>> from sklearn.utils.validation import column_or_1d\n    >>> column_or_1d([1, 1])\n    array([1, 1])\n    \"\"\"\n    xp, _ = get_namespace(y)\n    y = check_array(\n        y,\n        ensure_2d=False,\n        dtype=dtype,\n        input_name=input_name,\n        ensure_all_finite=False,\n        ensure_min_samples=0,\n    )\n\n    shape = y.shape\n    if len(shape) == 1:\n        return _asarray_with_order(\n            xp.reshape(y, (-1,)), order=\"C\", xp=xp, device=device\n        )\n    if len(shape) == 2 and shape[1] == 1:\n        if warn:\n            warnings.warn(\n                (\n                    \"A column-vector y was passed when a 1d array was\"\n                    \" expected. Please change the shape of y to \"\n                    \"(n_samples, ), for example using ravel().\"\n                ),\n                DataConversionWarning,\n                stacklevel=2,\n            )\n        return _asarray_with_order(\n            xp.reshape(y, (-1,)), order=\"C\", xp=xp, device=device\n        )\n\n    raise ValueError(\n        \"y should be a 1d array, got an array of shape {} instead.\".format(shape)\n    )",
    "scikit-learn.sklearn.utils.validation.indexable": "def indexable(*iterables):\n    \"\"\"Make arrays indexable for cross-validation.\n\n    Checks consistent length, passes through None, and ensures that everything\n    can be indexed by converting sparse matrices to csr and converting\n    non-iterable objects to arrays.\n\n    Parameters\n    ----------\n    *iterables : {lists, dataframes, ndarrays, sparse matrices}\n        List of objects to ensure sliceability.\n\n    Returns\n    -------\n    result : list of {ndarray, sparse matrix, dataframe} or None\n        Returns a list containing indexable arrays (i.e. NumPy array,\n        sparse matrix, or dataframe) or `None`.\n\n    Examples\n    --------\n    >>> from sklearn.utils import indexable\n    >>> from scipy.sparse import csr_matrix\n    >>> import numpy as np\n    >>> iterables = [\n    ...     [1, 2, 3], np.array([2, 3, 4]), None, csr_matrix([[5], [6], [7]])\n    ... ]\n    >>> indexable(*iterables)\n    [[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\n    \"\"\"\n\n    result = [_make_indexable(X) for X in iterables]\n    check_consistent_length(*result)\n    return result"
}