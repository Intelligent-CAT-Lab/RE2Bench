{
    "scikit-learn.sklearn.externals.array_api_compat._internal.wrapped_f": "@wraps(f)\ndef wrapped_f(*args: object, **kwargs: object) -> object:\n    return f(*args, xp=xp, **kwargs)",
    "scikit-learn.sklearn.externals.array_api_compat.numpy._aliases.asarray": "def asarray(\n    obj: Array | complex | NestedSequence[complex] | SupportsBufferProtocol,\n    /,\n    *,\n    dtype: DType | None = None,\n    device: Device | None = None,\n    copy: _Copy | None = None,\n    **kwargs: Any,\n) -> Array:\n    \"\"\"\n    Array API compatibility wrapper for asarray().\n\n    See the corresponding documentation in the array library and/or the array API\n    specification for more details.\n    \"\"\"\n    _helpers._check_device(np, device)\n\n    if copy is None:\n        copy = np._CopyMode.IF_NEEDED\n    elif copy is False:\n        copy = np._CopyMode.NEVER\n    elif copy is True:\n        copy = np._CopyMode.ALWAYS\n\n    return np.array(obj, copy=copy, dtype=dtype, **kwargs)  # pyright: ignore",
    "scikit-learn.sklearn.preprocessing._polynomial._combinations": "@staticmethod\ndef _combinations(\n    n_features, min_degree, max_degree, interaction_only, include_bias\n):\n    comb = combinations if interaction_only else combinations_w_r\n    start = max(1, min_degree)\n    iter = chain.from_iterable(\n        comb(range(n_features), i) for i in range(start, max_degree + 1)\n    )\n    if include_bias:\n        iter = chain(comb(range(n_features), 0), iter)\n    return iter",
    "scikit-learn.sklearn.preprocessing._polynomial.<genexpr>": "cumulative_size = sum(mat.shape[1] for mat in to_stack)\n",
    "scikit-learn.sklearn.preprocessing._polynomial._create_expansion": "def _create_expansion(X, interaction_only, deg, n_features, cumulative_size=0):\n    \"\"\"Helper function for creating and appending sparse expansion matrices\"\"\"\n\n    total_nnz = _calc_total_nnz(X.indptr, interaction_only, deg)\n    expanded_col = _calc_expanded_nnz(n_features, interaction_only, deg)\n\n    if expanded_col == 0:\n        return None\n    # This only checks whether each block needs 64bit integers upon\n    # expansion. We prefer to keep int32 indexing where we can,\n    # since currently SciPy's CSR construction downcasts when possible,\n    # so we prefer to avoid an unnecessary cast. The dtype may still\n    # change in the concatenation process if needed.\n    # See: https://github.com/scipy/scipy/issues/16569\n    max_indices = expanded_col - 1\n    max_indptr = total_nnz\n    max_int32 = np.iinfo(np.int32).max\n    needs_int64 = max(max_indices, max_indptr) > max_int32\n    index_dtype = np.int64 if needs_int64 else np.int32\n\n    # Result of the expansion, modified in place by the\n    # `_csr_polynomial_expansion` routine.\n    expanded_data = np.empty(shape=total_nnz, dtype=X.data.dtype)\n    expanded_indices = np.empty(shape=total_nnz, dtype=index_dtype)\n    expanded_indptr = np.empty(shape=X.indptr.shape[0], dtype=index_dtype)\n    _csr_polynomial_expansion(\n        X.data,\n        X.indices,\n        X.indptr,\n        X.shape[1],\n        expanded_data,\n        expanded_indices,\n        expanded_indptr,\n        interaction_only,\n        deg,\n    )\n    return sparse.csr_matrix(\n        (expanded_data, expanded_indices, expanded_indptr),\n        shape=(X.indptr.shape[0] - 1, expanded_col),\n        dtype=X.dtype,\n    )",
    "scikit-learn.sklearn.utils._array_api._is_numpy_namespace": "def _is_numpy_namespace(xp):\n    \"\"\"Return True if xp is backed by NumPy.\"\"\"\n    return xp.__name__ in _NUMPY_NAMESPACE_NAMES",
    "scikit-learn.sklearn.utils._array_api.supported_float_dtypes": "def supported_float_dtypes(xp, device=None):\n    \"\"\"Supported floating point types for the namespace.\n\n    Parameters\n    ----------\n    xp : module\n        Array namespace to inspect.\n\n    device : str or device instance from xp, default=None\n        Device to use for dtype selection. If ``None``, then a default device\n        is assumed.\n\n    Returns\n    -------\n    supported_dtypes : tuple\n        Tuple of real floating data types supported by the provided array namespace,\n        ordered from the highest precision to lowest.\n\n    See Also\n    --------\n    max_precision_float_dtype : Maximum float dtype for a namespace/device pair.\n\n    Notes\n    -----\n    `float16` is not officially part of the Array API spec at the\n    time of writing but scikit-learn estimators and functions can choose\n    to accept it when xp.float16 is defined.\n\n    Additionally, some devices available within a namespace may not support\n    all floating-point types that the namespace provides.\n\n    https://data-apis.org/array-api/latest/API_specification/data_types.html\n    \"\"\"\n    dtypes_dict = xp.__array_namespace_info__().dtypes(\n        kind=\"real floating\", device=device\n    )\n    valid_float_dtypes = []\n    for dtype_key in (\"float64\", \"float32\"):\n        if dtype_key in dtypes_dict:\n            valid_float_dtypes.append(dtypes_dict[dtype_key])\n\n    if hasattr(xp, \"float16\"):\n        valid_float_dtypes.append(xp.float16)\n\n    return tuple(valid_float_dtypes)",
    "scikit-learn.sklearn.utils._array_api.get_namespace_and_device": "def get_namespace_and_device(\n    *array_list, remove_none=True, remove_types=(str,), xp=None\n):\n    \"\"\"Combination into one single function of `get_namespace` and `device`.\n\n    Parameters\n    ----------\n    *array_list : array objects\n        Array objects.\n    remove_none : bool, default=True\n        Whether to ignore None objects passed in arrays.\n    remove_types : tuple or list, default=(str,)\n        Types to ignore in the arrays.\n    xp : module, default=None\n        Precomputed array namespace module. When passed, typically from a caller\n        that has already performed inspection of its own inputs, skips array\n        namespace inspection.\n\n    Returns\n    -------\n    namespace : module\n        Namespace shared by array objects. If any of the `arrays` are not arrays,\n        the namespace defaults to NumPy.\n    is_array_api_compliant : bool\n        True if the arrays are containers that implement the Array API spec.\n        Always False when array_api_dispatch=False.\n    device : device\n        `device` object (see the \"Device Support\" section of the array API spec).\n    \"\"\"\n    skip_remove_kwargs = dict(remove_none=False, remove_types=[])\n\n    array_list = _remove_non_arrays(\n        *array_list,\n        remove_none=remove_none,\n        remove_types=remove_types,\n    )\n    arrays_device = device(*array_list, **skip_remove_kwargs)\n\n    if xp is None:\n        xp, is_array_api = get_namespace(*array_list, **skip_remove_kwargs)\n    else:\n        xp, is_array_api = xp, True\n\n    if is_array_api:\n        return xp, is_array_api, arrays_device\n    else:\n        return xp, False, arrays_device",
    "scikit-learn.sklearn.utils._set_output.wrapped": "@wraps(f)\ndef wrapped(self, X, *args, **kwargs):\n    data_to_wrap = f(self, X, *args, **kwargs)\n    if isinstance(data_to_wrap, tuple):\n        # only wrap the first output for cross decomposition\n        return_tuple = (\n            _wrap_data_with_container(method, data_to_wrap[0], X, self),\n            *data_to_wrap[1:],\n        )\n        # Support for namedtuples `_make` is a documented API for namedtuples:\n        # https://docs.python.org/3/library/collections.html#collections.somenamedtuple._make\n        if hasattr(type(data_to_wrap), \"_make\"):\n            return type(data_to_wrap)._make(return_tuple)\n        return return_tuple\n\n    return _wrap_data_with_container(method, data_to_wrap, X, self)",
    "scikit-learn.sklearn.utils.validation.check_is_fitted": "def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):\n    \"\"\"Perform is_fitted validation for estimator.\n\n    Checks if the estimator is fitted by verifying the presence of\n    fitted attributes (ending with a trailing underscore) and otherwise\n    raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to\n    specify if the estimator is fitted or not. See\n    :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\n    for an example on how to use the API.\n\n    If no `attributes` are passed, this function will pass if an estimator is stateless.\n    An estimator can indicate it's stateless by setting the `requires_fit` tag. See\n    :ref:`estimator_tags` for more information. Note that the `requires_fit` tag\n    is ignored if `attributes` are passed.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Raises\n    ------\n    TypeError\n        If the estimator is a class or not an estimator instance\n\n    NotFittedError\n        If the attributes are not found.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.utils.validation import check_is_fitted\n    >>> from sklearn.exceptions import NotFittedError\n    >>> lr = LogisticRegression()\n    >>> try:\n    ...     check_is_fitted(lr)\n    ... except NotFittedError as exc:\n    ...     print(f\"Model is not fitted yet.\")\n    Model is not fitted yet.\n    >>> lr.fit([[1, 2], [1, 3]], [1, 0])\n    LogisticRegression()\n    >>> check_is_fitted(lr)\n    \"\"\"\n    if isclass(estimator):\n        raise TypeError(\"{} is a class, not an instance.\".format(estimator))\n    if msg is None:\n        msg = (\n            \"This %(name)s instance is not fitted yet. Call 'fit' with \"\n            \"appropriate arguments before using this estimator.\"\n        )\n\n    if not hasattr(estimator, \"fit\"):\n        raise TypeError(\"%s is not an estimator instance.\" % (estimator))\n\n    tags = get_tags(estimator)\n\n    if not tags.requires_fit and attributes is None:\n        return\n\n    if not _is_fitted(estimator, attributes, all_or_any):\n        raise NotFittedError(msg % {\"name\": type(estimator).__name__})",
    "scikit-learn.sklearn.utils.validation.validate_data": "def validate_data(\n    _estimator,\n    /,\n    X=\"no_validation\",\n    y=\"no_validation\",\n    reset=True,\n    validate_separately=False,\n    skip_check_array=False,\n    **check_params,\n):\n    \"\"\"Validate input data and set or check feature names and counts of the input.\n\n    This helper function should be used in an estimator that requires input\n    validation. This mutates the estimator and sets the `n_features_in_` and\n    `feature_names_in_` attributes if `reset=True`.\n\n    .. versionadded:: 1.6\n\n    Parameters\n    ----------\n    _estimator : estimator instance\n        The estimator to validate the input for.\n\n    X : {array-like, sparse matrix, dataframe} of shape \\\n            (n_samples, n_features), default='no validation'\n        The input samples.\n        If `'no_validation'`, no validation is performed on `X`. This is\n        useful for meta-estimator which can delegate input validation to\n        their underlying estimator(s). In that case `y` must be passed and\n        the only accepted `check_params` are `multi_output` and\n        `y_numeric`.\n\n    y : array-like of shape (n_samples,), default='no_validation'\n        The targets.\n\n        - If `None`, :func:`~sklearn.utils.check_array` is called on `X`. If\n          the estimator's `requires_y` tag is True, then an error will be raised.\n        - If `'no_validation'`, :func:`~sklearn.utils.check_array` is called\n          on `X` and the estimator's `requires_y` tag is ignored. This is a default\n          placeholder and is never meant to be explicitly set. In that case `X` must be\n          passed.\n        - Otherwise, only `y` with `_check_y` or both `X` and `y` are checked with\n          either :func:`~sklearn.utils.check_array` or\n          :func:`~sklearn.utils.check_X_y` depending on `validate_separately`.\n\n    reset : bool, default=True\n        Whether to reset the `n_features_in_` attribute.\n        If False, the input will be checked for consistency with data\n        provided when reset was last True.\n\n        .. note::\n\n           It is recommended to call `reset=True` in `fit` and in the first\n           call to `partial_fit`. All other methods that validate `X`\n           should set `reset=False`.\n\n    validate_separately : False or tuple of dicts, default=False\n        Only used if `y` is not `None`.\n        If `False`, call :func:`~sklearn.utils.check_X_y`. Else, it must be a tuple of\n        kwargs to be used for calling :func:`~sklearn.utils.check_array` on `X` and `y`\n        respectively.\n\n        `estimator=self` is automatically added to these dicts to generate\n        more informative error message in case of invalid input data.\n\n    skip_check_array : bool, default=False\n        If `True`, `X` and `y` are unchanged and only `feature_names_in_` and\n        `n_features_in_` are checked. Otherwise, :func:`~sklearn.utils.check_array`\n        is called on `X` and `y`.\n\n    **check_params : kwargs\n        Parameters passed to :func:`~sklearn.utils.check_array` or\n        :func:`~sklearn.utils.check_X_y`. Ignored if validate_separately\n        is not False.\n\n        `estimator=self` is automatically added to these params to generate\n        more informative error message in case of invalid input data.\n\n    Returns\n    -------\n    out : {ndarray, sparse matrix} or tuple of these\n        The validated input. A tuple is returned if both `X` and `y` are\n        validated.\n    \"\"\"\n    _check_feature_names(_estimator, X, reset=reset)\n    tags = get_tags(_estimator)\n    if y is None and tags.target_tags.required:\n        raise ValueError(\n            f\"This {_estimator.__class__.__name__} estimator \"\n            \"requires y to be passed, but the target y is None.\"\n        )\n\n    no_val_X = isinstance(X, str) and X == \"no_validation\"\n    no_val_y = y is None or (isinstance(y, str) and y == \"no_validation\")\n\n    if no_val_X and no_val_y:\n        raise ValueError(\"Validation should be done on X, y or both.\")\n\n    default_check_params = {\"estimator\": _estimator}\n    check_params = {**default_check_params, **check_params}\n\n    if skip_check_array:\n        if not no_val_X and no_val_y:\n            out = X\n        elif no_val_X and not no_val_y:\n            out = y\n        else:\n            out = X, y\n    elif not no_val_X and no_val_y:\n        out = check_array(X, input_name=\"X\", **check_params)\n    elif no_val_X and not no_val_y:\n        out = _check_y(y, **check_params)\n    else:\n        if validate_separately:\n            # We need this because some estimators validate X and y\n            # separately, and in general, separately calling check_array()\n            # on X and y isn't equivalent to just calling check_X_y()\n            # :(\n            check_X_params, check_y_params = validate_separately\n            if \"estimator\" not in check_X_params:\n                check_X_params = {**default_check_params, **check_X_params}\n            X = check_array(X, input_name=\"X\", **check_X_params)\n            if \"estimator\" not in check_y_params:\n                check_y_params = {**default_check_params, **check_y_params}\n            y = check_array(y, input_name=\"y\", **check_y_params)\n        else:\n            X, y = check_X_y(X, y, **check_params)\n        out = X, y\n\n    if not no_val_X and check_params.get(\"ensure_2d\", True):\n        _check_n_features(_estimator, X, reset=reset)\n\n    return out"
}