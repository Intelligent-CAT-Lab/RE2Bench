{
    "scikit-learn.sklearn.base.clone": "def clone(estimator, *, safe=True):\n    \"\"\"Construct a new unfitted estimator with the same parameters.\n\n    Clone does a deep copy of the model in an estimator\n    without actually copying attached data. It returns a new estimator\n    with the same parameters that has not been fitted on any data.\n\n    .. versionchanged:: 1.3\n        Delegates to `estimator.__sklearn_clone__` if the method exists.\n\n    Parameters\n    ----------\n    estimator : {list, tuple, set} of estimator instance or a single \\\n            estimator instance\n        The estimator or group of estimators to be cloned.\n    safe : bool, default=True\n        If safe is False, clone will fall back to a deep copy on objects\n        that are not estimators. Ignored if `estimator.__sklearn_clone__`\n        exists.\n\n    Returns\n    -------\n    estimator : object\n        The deep copy of the input, an estimator if input is an estimator.\n\n    Notes\n    -----\n    If the estimator's `random_state` parameter is an integer (or if the\n    estimator doesn't have a `random_state` parameter), an *exact clone* is\n    returned: the clone and the original estimator will give the exact same\n    results. Otherwise, *statistical clone* is returned: the clone might\n    return different results from the original estimator. More details can be\n    found in :ref:`randomness`.\n\n    Examples\n    --------\n    >>> from sklearn.base import clone\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> X = [[-1, 0], [0, 1], [0, -1], [1, 0]]\n    >>> y = [0, 0, 1, 1]\n    >>> classifier = LogisticRegression().fit(X, y)\n    >>> cloned_classifier = clone(classifier)\n    >>> hasattr(classifier, \"classes_\")\n    True\n    >>> hasattr(cloned_classifier, \"classes_\")\n    False\n    >>> classifier is cloned_classifier\n    False\n    \"\"\"\n    if hasattr(estimator, \"__sklearn_clone__\") and not inspect.isclass(estimator):\n        return estimator.__sklearn_clone__()\n    return _clone_parametrized(estimator, safe=safe)",
    "scikit-learn.sklearn.gaussian_process._gpc.log_marginal_likelihood": "def log_marginal_likelihood(\n    self, theta=None, eval_gradient=False, clone_kernel=True\n):\n    \"\"\"Returns log-marginal likelihood of theta for training data.\n\n    Parameters\n    ----------\n    theta : array-like of shape (n_kernel_params,), default=None\n        Kernel hyperparameters for which the log-marginal likelihood is\n        evaluated. If None, the precomputed log_marginal_likelihood\n        of ``self.kernel_.theta`` is returned.\n\n    eval_gradient : bool, default=False\n        If True, the gradient of the log-marginal likelihood with respect\n        to the kernel hyperparameters at position theta is returned\n        additionally. If True, theta must not be None.\n\n    clone_kernel : bool, default=True\n        If True, the kernel attribute is copied. If False, the kernel\n        attribute is modified, but may result in a performance improvement.\n\n    Returns\n    -------\n    log_likelihood : float\n        Log-marginal likelihood of theta for training data.\n\n    log_likelihood_gradient : ndarray of shape (n_kernel_params,), \\\n            optional\n        Gradient of the log-marginal likelihood with respect to the kernel\n        hyperparameters at position theta.\n        Only returned when `eval_gradient` is True.\n    \"\"\"\n    if theta is None:\n        if eval_gradient:\n            raise ValueError(\"Gradient can only be evaluated for theta!=None\")\n        return self.log_marginal_likelihood_value_\n\n    if clone_kernel:\n        kernel = self.kernel_.clone_with_theta(theta)\n    else:\n        kernel = self.kernel_\n        kernel.theta = theta\n\n    if eval_gradient:\n        K, K_gradient = kernel(self.X_train_, eval_gradient=True)\n    else:\n        K = kernel(self.X_train_)\n\n    # Compute log-marginal-likelihood Z and also store some temporaries\n    # which can be reused for computing Z's gradient\n    Z, (pi, W_sr, L, b, a) = self._posterior_mode(K, return_temporaries=True)\n\n    if not eval_gradient:\n        return Z\n\n    # Compute gradient based on Algorithm 5.1 of GPML\n    d_Z = np.empty(theta.shape[0])\n    # XXX: Get rid of the np.diag() in the next line\n    R = W_sr[:, np.newaxis] * cho_solve((L, True), np.diag(W_sr))  # Line 7\n    C = solve(L, W_sr[:, np.newaxis] * K)  # Line 8\n    # Line 9: (use einsum to compute np.diag(C.T.dot(C))))\n    s_2 = (\n        -0.5\n        * (np.diag(K) - np.einsum(\"ij, ij -> j\", C, C))\n        * (pi * (1 - pi) * (1 - 2 * pi))\n    )  # third derivative\n\n    for j in range(d_Z.shape[0]):\n        C = K_gradient[:, :, j]  # Line 11\n        # Line 12: (R.T.ravel().dot(C.ravel()) = np.trace(R.dot(C)))\n        s_1 = 0.5 * a.T.dot(C).dot(a) - 0.5 * R.T.ravel().dot(C.ravel())\n\n        b = C.dot(self.y_train_ - pi)  # Line 13\n        s_3 = b - K.dot(R.dot(b))  # Line 14\n\n        d_Z[j] = s_1 + s_2.T.dot(s_3)  # Line 15\n\n    return Z, d_Z",
    "scikit-learn.sklearn.gaussian_process._gpc._posterior_mode": "def _posterior_mode(self, K, return_temporaries=False):\n    \"\"\"Mode-finding for binary Laplace GPC and fixed kernel.\n\n    This approximates the posterior of the latent function values for given\n    inputs and target observations with a Gaussian approximation and uses\n    Newton's iteration to find the mode of this approximation.\n    \"\"\"\n    # Based on Algorithm 3.1 of GPML\n\n    # If warm_start are enabled, we reuse the last solution for the\n    # posterior mode as initialization; otherwise, we initialize with 0\n    if (\n        self.warm_start\n        and hasattr(self, \"f_cached\")\n        and self.f_cached.shape == self.y_train_.shape\n    ):\n        f = self.f_cached\n    else:\n        f = np.zeros_like(self.y_train_, dtype=np.float64)\n\n    # Use Newton's iteration method to find mode of Laplace approximation\n    log_marginal_likelihood = -np.inf\n    for _ in range(self.max_iter_predict):\n        # Line 4\n        pi = expit(f)\n        W = pi * (1 - pi)\n        # Line 5\n        W_sr = np.sqrt(W)\n        W_sr_K = W_sr[:, np.newaxis] * K\n        B = np.eye(W.shape[0]) + W_sr_K * W_sr\n        L = cholesky(B, lower=True)\n        # Line 6\n        b = W * f + (self.y_train_ - pi)\n        # Line 7\n        a = b - W_sr * cho_solve((L, True), W_sr_K.dot(b))\n        # Line 8\n        f = K.dot(a)\n\n        # Line 10: Compute log marginal likelihood in loop and use as\n        #          convergence criterion\n        lml = (\n            -0.5 * a.T.dot(f)\n            - np.log1p(np.exp(-(self.y_train_ * 2 - 1) * f)).sum()\n            - np.log(np.diag(L)).sum()\n        )\n        # Check if we have converged (log marginal likelihood does\n        # not decrease)\n        # XXX: more complex convergence criterion\n        if lml - log_marginal_likelihood < 1e-10:\n            break\n        log_marginal_likelihood = lml\n\n    self.f_cached = f  # Remember solution for later warm-starts\n    if return_temporaries:\n        return log_marginal_likelihood, (pi, W_sr, L, b, a)\n    else:\n        return log_marginal_likelihood",
    "scikit-learn.sklearn.gaussian_process._gpc._constrained_optimization": "def _constrained_optimization(self, obj_func, initial_theta, bounds):\n    if self.optimizer == \"fmin_l_bfgs_b\":\n        opt_res = scipy.optimize.minimize(\n            obj_func, initial_theta, method=\"L-BFGS-B\", jac=True, bounds=bounds\n        )\n        _check_optimize_result(\"lbfgs\", opt_res)\n        theta_opt, func_min = opt_res.x, opt_res.fun\n    elif callable(self.optimizer):\n        theta_opt, func_min = self.optimizer(obj_func, initial_theta, bounds=bounds)\n    else:\n        raise ValueError(\"Unknown optimizer %s.\" % self.optimizer)\n\n    return theta_opt, func_min",
    "scikit-learn.sklearn.gaussian_process.kernels.__init__": "def __init__(self, length_scale=1.0, length_scale_bounds=(1e-5, 1e5)):\n    self.length_scale = length_scale\n    self.length_scale_bounds = length_scale_bounds",
    "scikit-learn.sklearn.gaussian_process.kernels.__call__": "def __call__(self, X, Y=None, eval_gradient=False):\n    \"\"\"Return the kernel k(X, Y) and optionally its gradient.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples_X, n_features) or list of object\n        Left argument of the returned kernel k(X, Y)\n\n    Y : array-like of shape (n_samples_Y, n_features) or list of object,\\\n        default=None\n        Right argument of the returned kernel k(X, Y). If None, k(X, X)\n        is evaluated instead.\n\n    eval_gradient : bool, default=False\n        Determines whether the gradient with respect to the log of\n        the kernel hyperparameter is computed.\n\n    Returns\n    -------\n    K : ndarray of shape (n_samples_X, n_samples_Y)\n        Kernel k(X, Y)\n\n    K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims), \\\n            optional\n        The gradient of the kernel k(X, X) with respect to the log of the\n        hyperparameter of the kernel. Only returned when `eval_gradient`\n        is True.\n    \"\"\"\n    if eval_gradient:\n        K1, K1_gradient = self.k1(X, Y, eval_gradient=True)\n        K2, K2_gradient = self.k2(X, Y, eval_gradient=True)\n        return K1 * K2, np.dstack(\n            (K1_gradient * K2[:, :, np.newaxis], K2_gradient * K1[:, :, np.newaxis])\n        )\n    else:\n        return self.k1(X, Y) * self.k2(X, Y)",
    "scikit-learn.sklearn.gaussian_process.kernels.n_dims": "@property\ndef n_dims(self):\n    \"\"\"Returns the number of non-fixed hyperparameters of the kernel.\"\"\"\n    return self.theta.shape[0]",
    "scikit-learn.sklearn.gaussian_process.kernels.theta": "@theta.setter\ndef theta(self, theta):\n    \"\"\"Sets the (flattened, log-transformed) non-fixed hyperparameters.\n\n    Parameters\n    ----------\n    theta : ndarray of shape (n_dims,)\n        The non-fixed, log-transformed hyperparameters of the kernel\n    \"\"\"\n    k1_dims = self.k1.n_dims\n    self.k1.theta = theta[:k1_dims]\n    self.k2.theta = theta[k1_dims:]",
    "scikit-learn.sklearn.gaussian_process.kernels.bounds": "@property\ndef bounds(self):\n    \"\"\"Returns the log-transformed bounds on the theta.\n\n    Returns\n    -------\n    bounds : ndarray of shape (n_dims, 2)\n        The log-transformed bounds on the kernel's hyperparameters theta\n    \"\"\"\n    if self.k1.bounds.size == 0:\n        return self.k2.bounds\n    if self.k2.bounds.size == 0:\n        return self.k1.bounds\n    return np.vstack((self.k1.bounds, self.k2.bounds))",
    "scikit-learn.sklearn.gaussian_process.kernels.__mul__": "def __mul__(self, b):\n    if not isinstance(b, Kernel):\n        return Product(self, ConstantKernel(b))\n    return Product(self, b)",
    "scikit-learn.sklearn.gaussian_process.kernels._check_bounds_params": "def _check_bounds_params(self):\n    \"\"\"Called after fitting to warn if bounds may have been too tight.\"\"\"\n    list_close = np.isclose(self.bounds, np.atleast_2d(self.theta).T)\n    idx = 0\n    for hyp in self.hyperparameters:\n        if hyp.fixed:\n            continue\n        for dim in range(hyp.n_elements):\n            if list_close[idx, 0]:\n                warnings.warn(\n                    \"The optimal value found for \"\n                    \"dimension %s of parameter %s is \"\n                    \"close to the specified lower \"\n                    \"bound %s. Decreasing the bound and\"\n                    \" calling fit again may find a \"\n                    \"better value.\" % (dim, hyp.name, hyp.bounds[dim][0]),\n                    ConvergenceWarning,\n                )\n            elif list_close[idx, 1]:\n                warnings.warn(\n                    \"The optimal value found for \"\n                    \"dimension %s of parameter %s is \"\n                    \"close to the specified upper \"\n                    \"bound %s. Increasing the bound and\"\n                    \" calling fit again may find a \"\n                    \"better value.\" % (dim, hyp.name, hyp.bounds[dim][1]),\n                    ConvergenceWarning,\n                )\n            idx += 1",
    "scikit-learn.sklearn.preprocessing._label.fit_transform": "def fit_transform(self, y):\n    \"\"\"Fit label encoder and return encoded labels.\n\n    Parameters\n    ----------\n    y : array-like of shape (n_samples,)\n        Target values.\n\n    Returns\n    -------\n    y : array-like of shape (n_samples,)\n        Encoded labels.\n    \"\"\"\n    y = column_or_1d(y, warn=True)\n    self.classes_, y = _unique(y, return_inverse=True)\n    return y",
    "scikit-learn.sklearn.utils.validation.check_random_state": "def check_random_state(seed):\n    \"\"\"Turn seed into an np.random.RandomState instance.\n\n    Parameters\n    ----------\n    seed : None, int or instance of RandomState\n        If seed is None, return the RandomState singleton used by np.random.\n        If seed is an int, return a new RandomState instance seeded with seed.\n        If seed is already a RandomState instance, return it.\n        Otherwise raise ValueError.\n\n    Returns\n    -------\n    :class:`numpy:numpy.random.RandomState`\n        The random state object based on `seed` parameter.\n\n    Examples\n    --------\n    >>> from sklearn.utils.validation import check_random_state\n    >>> check_random_state(42)\n    RandomState(MT19937) at 0x...\n    \"\"\"\n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, numbers.Integral):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState):\n        return seed\n    raise ValueError(\n        \"%r cannot be used to seed a numpy.random.RandomState instance\" % seed\n    )"
}